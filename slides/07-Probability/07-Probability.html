<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Probability &amp; Inference (Part 1?)</title>
    <meta charset="utf-8" />
    <link href="07-Probability_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="07-Probability_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Probability &amp; Inference (Part 1?)
## POLS 7012: Introduction to Political Methodology

---


&lt;style&gt;

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

&lt;/style&gt;




## Outline

A lot of probability/statistics courses are pretty abstract; the great thing about learning `R` first is that we can introduce this in a really hands-on way!

We'll start by learning about probability distribution functions, and learn how to draw random values in `R`.

We'll see how this procedure underlies **all** statistical inference.

Then, I think, Central Limit Theorem. A little backwards compared to the usual approach, but trust me, let's front-load the good stuff.

Random Variables and their properties: Expectation, Variance, Law of Large Numbers

Multivariable PDFs: Covariance, Joint Probability, Conditional Probability, Bayes Rule

As a bonus, you'll get to practice creating your own functions...very useful when you want to customize / mix-and-match the built-in functions we've learned.

---

# Warm Up

- Create an `R` project where you can keep all your code from this session.

- Create `week7.R` in your project folder. Load `tidyverse`.

--

- Create a object `x` and assign it a vector with the elements 1, 2, 3, and 4.

--


```r
x &lt;- c(1,2,3,4)

# this works too
x &lt;- 1:4
```

--

- Find the sum of your vector `x`. Find the length of your vector `x`

--


```r
sum(x)
```

```
[1] 10
```

```r
length(x)
```

```
[1] 4
```

---

# Warm Up

- What does this expression mean?

`$$\int_{-\infty}^{+\infty} f(x)dx = 1$$`
--

The area under the entire curve `\(f(x)\)`, from positive to negative infinity, equals `\(1\)`. 

--

&lt;br&gt;

Keep a copy of these slides open so that you can copy-paste code into your script when needed!

---

## Learning Objectives

By the end of this module, you will be able to...

- Write your own functions and generate random values in `R`

- Explain several foundational concepts in probability theory
  - Probability Distribution Functions (PDFs)
  - Random Variables
  - Expectation
  - Variance
  - Central Limit Theorem
  - Law of Large Numbers
  - Bayes Rule

- Make inferences about populations from sample data

---

class: center, middle

## The Trick Coin Problem

---

## The Trick Coin Problem

Suppose someone flips a coin 100 times and it lands on heads 65 times.

&lt;img src="img/flipping-coin-animated-gif-1.gif" width="600" style="display: block; margin: auto;" /&gt;

--

**The Challenge:** Is it a trick coin? Or did they just get lucky? How would we know? What is the *probability* of getting that many heads with a fair coin?

---

## The Trick Coin Problem

**Our Approach**: Take a fair coin, flip it 10 million times, and see how often we get 65 heads in 100 flips.

--

&lt;img src="img/schitts-creek.gif" width="70%" style="display: block; margin: auto;" /&gt;

On second thought, that would be exhausting, so let's make the computer do it instead.

---

class: center, middle

## Probability Distribution Functions (PDFs)

---

## Probability Distribution Functions (PDFs)

A special kind of function that describes the likelihood of random events.

--

### Property 1

`$$P(x) \geq 0$$`

Nothing has a less-than-zero chance of happening.

--

### Property 2

`$$\sum_{x} P(x) = 1$$` (discrete)

`$$\int_{-\infty}^{+\infty} P(x)dx = 1$$` (continuous)

The probability of **something** happening equals 1.

???

- (Probability not about coin flips and card games; probability is a function that characterizes our uncertainty over outcomes)

Functions take inputs and produce outputs. PDFs output the probability of an event.

(As Wasserman says, rigor and clarity not synonymous; if you want rigor, check out the books.)

---

## Example PDF: Flipping a Coin

`$$\Omega = \{\text{heads}, \text{tails}\}$$`

`$$P(\text{heads}) = 0.5$$`

`$$P(\text{tails}) = 0.5$$`

--

&lt;br&gt;

For you logophiles out there, this is also known as the **Bernoulli** distribution.

 - Two outcomes (0 and 1)
 - `\(P(1) = p\)` and `\(P(0) = 1-p\)`

---

class: center, middle

## Generating Random Values

---

## Generating Random Values in `R`

The `sample` function draws values at random (with equal probability) from a vector:


```r
coin_outcomes &lt;- c('Heads', 'Tails')

sample(coin_outcomes, size = 1)
```

```
[1] "Heads"
```

--


```r
sample(coin_outcomes, size = 30, replace = TRUE)
```

```
 [1] "Heads" "Heads" "Heads" "Tails" "Tails" "Tails" "Tails" "Heads" "Tails"
[10] "Heads" "Tails" "Heads" "Tails" "Heads" "Heads" "Tails" "Tails" "Tails"
[19] "Tails" "Heads" "Heads" "Heads" "Heads" "Heads" "Tails" "Heads" "Heads"
[28] "Heads" "Heads" "Tails"
```

---

## Generating Random Values in `R`

### Exercise 1

Flip the coin 100 times, and assign the result to an object called `flips`.

--


```r
flips &lt;- sample(coin_outcomes, size = 100, replace = TRUE)
```

--

### Exercise 2

Draw 5 letters at random from the alphabet. (Note: the vector `LETTERS` is built-in to R.)

--


```r
sample(LETTERS, size = 5, replace = TRUE)
```

```
[1] "K" "Y" "Z" "P" "H"
```

---

class: center, middle

## Creating Your Own Functions

---

## Creating Your Own Functions

- An **enormous** strength of working with `R`

- Get comfortable with creating your own functions, and you can make `R` do whatever you want.

---

## Creating Your Own Functions

Here's a (silly) function that takes two inputs and adds 1 to their sum:


```r
add_one &lt;- function(x, y = 1){
  x + y + 1
}
```

--

- `add_one` is the name
- `&lt;- function()` assigns a function to that name
- `x, y = 1` are the inputs (aka arguments); `y` has a default value of 1.
- The code within the brackets executes whenever you call the function, and the last line is the function's output.

---

## Creating Your Own Functions

Here's a (silly) function that takes two inputs and adds 1 to their sum:


```r
add_one &lt;- function(x, y = 1){
  x + y + 1
}
```

What happens if you call `add_one(x = 1, y = 2)`?

--


```r
add_one(x = 1, y = 2)
```

```
[1] 4
```

--

What about `add_one(1)`?

--


```r
add_one(1)
```

```
[1] 3
```




---

## Creating Your Own Functions

Here's a function that will flip a coin `\(N\)` times and return the result.


```r
flip_coin &lt;- function(N = 100){
  # specify the sample space of outcomes
  coin_outcomes &lt;- c('Heads', 'Tails')
  
  # return a vector of N random coin flips
  sample(coin_outcomes,
         size = N,
         replace = TRUE)
}

flip_coin(N = 8)
```

```
[1] "Heads" "Heads" "Tails" "Heads" "Heads" "Tails" "Heads" "Heads"
```

---

## Creating Your Own Functions

Here's a function that will flip a coin `\(N\)` times and return the number that landed on heads. (**Note**: it calls the `flip_coin()` function we just made!)


```r
count_heads &lt;- function(N){
  
  # flip the coin N times, assign to 'flips' vector
  flips &lt;- flip_coin(N)
  
  # return the number of heads divided by number of flips
  sum(flips == 'Heads')
}
```

--


```r
count_heads(N = 100)
```

```
[1] 43
```

???

Functions are like Las Vegas.

---

## Creating Your Own Functions

### Exercise 1

Create a function called `sample_n_letters()`, which samples `\(N\)` letters at random from the alphabet.

--


```r
sample_n_letters &lt;- function(num_letters = 5){
  sample(LETTERS, size = num_letters, replace = TRUE)
}

sample_n_letters(num_letters = 10)
```

```
 [1] "D" "T" "K" "Q" "I" "Q" "M" "Q" "T" "A"
```

---

## Creating Your Own Functions

### Exercise 2

Create a function called `fraction_Q()`, which samples `\(N\)` letters from the alphabet at random and returns the fraction that are Q's.

--


```r
fraction_Q &lt;- function(num_letters = 5){
  
  # draw n letters
  sample_letters &lt;- sample_n_letters(num_letters)
  
  # return the number of Q's
  sum(sample_letters == 'Q') / num_letters
}

fraction_Q(100)
```

```
[1] 0.04
```

---

class: center, middle

## Back to the Trick Coin Problem

---

## The Trick Coin Problem

We can solve this now. We have the technology.

--


```r
count_heads(N = 100)
```

```
[1] 48
```

--

Let's repeat that 100,000 times and see how often you get 65 heads.

--


```r
# Flip a coin 100 times and count the number of heads. 
# Repeat that 100,000 times and save the results
results &lt;- replicate(100000, count_heads(N = 100))
```

???

TODO: Questioning lifecycle; okay here's the approach:
create functions to draw the variable, compute the summary statistic, and replicate the heck out of it. Histogram that distribution.

---

## The Trick Coin Problem


```r
tibble(results) %&gt;% 
  ggplot() +
  geom_histogram(aes(x=results), color = 'black', 
                 binwidth = 1) +
  geom_vline(xintercept = 65, color = 'red') +
  labs(x = 'Number of Heads in 100 Fair Coin Flips',
       y = 'Count')
```

&lt;img src="07-Probability_files/figure-html/hist coin flips-1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

## The Trick Coin Problem


```r
tibble(results) %&gt;% 
  ggplot() +
  geom_histogram(aes(x=results), color = 'black', 
                 binwidth = 1) +
  geom_vline(xintercept = 65, color = 'red') +
  labs(x = 'Number of Heads in 100 Fair Coin Flips',
       y = 'Count')
```

&lt;img src="07-Probability_files/figure-html/hist coin flips 2-1.png" width="40%" style="display: block; margin: auto;" /&gt;

By the way, that's called the **binomial** distribution. Take `\(n\)` Bernoulli random variables and add them together. 

???

Out of 100,000 tries we only got a result like 65 a handful of times. It's a low-frequency event.

Next time I see that guy, I'm gonna say "hey let me see that coin". I can't prove it's fake, but, y'know, it's suspicious.

---

## The Trick Coin Problem

A **p-value** is the probability that we observe a result at least as extreme as our test statistic.

--


```r
# the frequency of draws &gt;= 65 as a fraction of all draws
sum(results &gt;= 65) / length(results)
```

```
[1] 0.00168
```

--

The probability of getting 65 or more heads from a fair coin is roughly 0.1%.

---

class: center, middle

## So What?

---

## The Polling Problem

**Exercise:** You've taken a random sample of 1,000 voters. 526 say they plan to vote for the Democrat and 474 say they plan to vote for the Republican.

If 50% of all voters planned to vote for the Republican, what is the probability you would observe a poll so favorable to Democrats?

---

## The Polling Problem


```r
# create 100,000 random polls of 1,000 respondents each
results &lt;- replicate(100000, count_heads(N = 1000))

# what's the probability of getting at least 526 Democratic voters?
sum(results &gt;= 526) / length(results)
```

```
[1] 0.05431
```

&lt;img src="07-Probability_files/figure-html/polling histogram-1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

## The Polling Problem


```r
# create 100,000 random polls of 1,000 respondents each
results &lt;- replicate(100000, count_heads(N = 1000))

# what's the probability of getting at least 526 Democratic voters?
sum(results &gt;= 526) / length(results)
```

```
[1] 0.05394
```

&lt;img src="07-Probability_files/figure-html/polling histogram 2-1.png" width="45%" style="display: block; margin: auto;" /&gt;

That's called the **sampling distribution**. If you understand that, then you understand frequentist statistical inference.

???

Because it's the one weird trick we use over and over and over and over again.

---

## Statistical Inference in Three Steps

--

1. Define your Null Hypothesis `\((H_0)\)`

2. Construct the **sampling distribution** of the test statistic under `\(H_0\)`

3. Compare the test statistic from your observed data with the sampling distribution.

--

Every (frequentist) hypothesis test is a variation on this! The only thing that changes is the PDF. 

---

class: center, middle

## Expected Value and Variance

---

## Expected Value

The **expected value** is the average value of a random variable `\(X\)`.

- Specifically, a *weighted average* of all possible outcomes times their probability.

Discrete: 
`$$E(X) = \sum_x x P(x)$$`

Continuous:
`$$E(X) = \int_{-\infty}^{+\infty} x P(x)dx$$`

---

## Expected Value

Suppose `\(X\)` is distributed as follows:

`\(P(X = 0) = 1-p\)` and `\(P(X = 1) = p\)`

What is `\(E(X)\)`?

--

`$$E(X) = 0 \times (1-p) + 1 \times p = p$$`

---

## Properties of Expected Values

`$$E(aX) = aE(X)$$`
&lt;TODO: Is this useful?&gt;

---

## Variance

The **variance** of a random variable `\(X\)` is the average squared distance from the expected value. In math:

`$$\text{Var}(X) = \sigma^2_X = E[(X-E(X))^2]$$`
--

The **standard deviation** `\((\sigma)\)` of a random variable `\(X\)` is the square root of its variance.

`$$\sigma_X = \sqrt{\text{Var(X)}}$$`

---

class: center, middle

## The Normal Distribution

---

&lt;br&gt;

&lt;img src="img/normal-distribution.png" width="600" style="display: block; margin: auto;" /&gt;

---

class: center, middle

## Where Does The Normal Distribution Come From?

---

## Import CCES


```r
CCES &lt;- read_rds('data/CCES_2018.RDS') %&gt;% 
  mutate(age = 2018 - birthyr)

ggplot(data = CCES) + 
  geom_histogram(aes(x=age), color = 'black', binwidth = 2) +
  labs(x = 'Age', y = 'Count')
```

&lt;img src="07-Probability_files/figure-html/load CCES-1.png" width="40%" style="display: block; margin: auto;" /&gt;

```r
mean(CCES$age)
```

```
[1] 48.08623
```

---

## Take a Sample


```r
# draw a sample of ages and take the mean
mean_age_of_sample &lt;- function(data, n){
  data %&gt;%
    pull(age) %&gt;%
    sample(size = n) %&gt;%
    mean
}

mean_age_of_sample(data = CCES, n = 100)
```

```
[1] 46.82
```

```r
mean_age_of_sample(data = CCES, n = 100)
```

```
[1] 49.74
```

--

The mean from any one sample will always be off a bit.

---

## The Sampling Distribution


```r
draws &lt;- replicate(10000, mean_age_of_sample(data = CCES, n = 100))

tibble(draws) %&gt;% 
  ggplot() + 
  geom_histogram(mapping = aes(x=draws), color = 'black') +
  labs(x = 'Mean Age From Sample', y = 'Count')
```

&lt;img src="07-Probability_files/figure-html/repeat mean sample ages-1.png" width="50%" style="display: block; margin: auto;" /&gt;

--

It's the normal distribution!

---

class: center, middle

## The Central Limit Theorem

The mean of a large number of independent random variables will be (approximately) normally distributed.

---

## Central Limit Theorem

&lt;iframe width="860" height="455" src="https://upload.wikimedia.org/wikipedia/commons/d/dc/Galton_box.webm" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

???

Imagine that each ball is a sample: when it goes left that means we randomly selected a person younger than the average, when it goes right that means we randomly selected a person older than the average. Most samples end up pretty close to the true mean, but sometimes you get really unlucky and draw a bunch of old people or a bunch of young people.

---

class: center, middle

## Standard Errors

---

## Standard Errors

The **standard error** is the standard deviation of the sampling distribution. 

--

&gt; What is the standard deviation of the sampling distribution you just created?

--


```r
sd(draws)
```

```
[1] 1.777336
```

--

&gt; What if you increased the sample size to 400?

--


```r
draws &lt;- replicate(10000, mean_age_of_sample(data = CCES, n = 400))

sd(draws)
```

```
[1] 0.8889038
```


---

## Standard Errors

Quadrupling the sample size cut the standard error in half.

--

That's because standard error equals:

`$$SE = \frac{\sigma}{\sqrt{N}}$$`
--

&lt;br&gt;

What happens to the standard error as `\(N \to \infty\)`?

---


class: center, middle

## The Law of Large Numbers

If you repeatedly draw a random variable a **large number of times**, the sample mean will converge on the expected value.

`$$\bar{X}_n \overset{P}{\to} E(X)$$`

???

Quite easy to understand Law of Large Numbers once you understand CLT.

---

## Let's Prove It


```r
num_draws &lt;- 10000 # draw 10,000 Benoulli random variables
p &lt;- 0.4 # p = 0.4

draws &lt;- sample(c(0,1), num_draws, replace = TRUE, prob = c(1-p, p))

# create a dataframe and plot sample mean as n increases
fig &lt;- tibble(n = 1:num_draws,
       draws) %&gt;% 
  mutate(sample_mean = cumsum(draws) / n) %&gt;% 
  ggplot() + 
  geom_line(mapping = aes(x = n, y = sample_mean)) + 
  geom_hline(yintercept = p, color = 'red', linetype = 'dashed') +
  labs(x = 'n', y = 'Sample Mean')
```

---

## Let's Prove It

&lt;img src="07-Probability_files/figure-html/lln fig-1.png" width="600" style="display: block; margin: auto;" /&gt;


---

class: center, middle

## Confidence Intervals

---

## Confidence Intervals

Man, what a wacky idea.

The 95% confidence interval:

If you were to repeat this procedure a large number of times, the true value would lie within the confidence interval 95% of the time. 

Alternatively, it is the set of values for which we can't reject the null hypothesis with `\(p &lt; 0.05\)`.

---

## Let's Do A Hypothesis Test


```r
CCES %&gt;% 
  filter(ownhome &lt; 3,
         !is.na(CC18_325b)) %&gt;% 
  mutate(ownhome = case_when(ownhome == 1 ~ 'Own',
                             ownhome == 2 ~ 'Rent'),
         support_reducing_MID = case_when(CC18_325b == 1 ~ 1,
                                          CC18_325b == 2 ~ 0)) %&gt;%
  group_by(ownhome) %&gt;% 
  summarize(pct_supporting_reduced_MID = mean(support_reducing_MID),
            num = n())
```

```
# A tibble: 2 x 3
  ownhome pct_supporting_reduced_MID   num
  &lt;chr&gt;                        &lt;dbl&gt; &lt;int&gt;
1 Own                          0.651 36348
2 Rent                         0.635 20244
```

```r
# respondents making over $100,000
CCES %&gt;% 
  filter(faminc_new %in% 1:16,
         !is.na(CC18_325b)) %&gt;% 
  mutate(rich = if_else(faminc_new &gt;= 10, 1, 0),
         support_reducing_MID = case_when(CC18_325b == 1 ~ 1,
                                          CC18_325b == 2 ~ 0)) %&gt;%
  group_by(rich) %&gt;% 
  summarize(pct_supporting_reduced_MID = mean(support_reducing_MID),
            num = n())
```

```
# A tibble: 2 x 3
   rich pct_supporting_reduced_MID   num
  &lt;dbl&gt;                      &lt;dbl&gt; &lt;int&gt;
1     0                      0.661 43045
2     1                      0.593 10582
```

---

## Hypothesis Test

`\(H_0\)`: There is no difference between the average age of Republicans and Democrats.


```r
# sample 100 Republicans ages
rep_age &lt;- CCES %&gt;% 
  mutate(age = 2018 - birthyr) %&gt;%  
  filter(pid3 == 2) %&gt;% 
  pull(age) %&gt;% 
  sample(100)

# sample 100 Democrats ages
dem_age &lt;- CCES %&gt;% 
  mutate(age = 2018 - birthyr) %&gt;%  
  filter(pid3 == 1) %&gt;% 
  pull(age) %&gt;% 
  sample(100)

mean(rep_age)
```

```
[1] 50.96
```

```r
mean(dem_age)
```

```
[1] 49.58
```

The Republicans seem to be older on average, but is that just sampling error?

--

How would you test it?

---

## Function: Compute Test Statistic


```r
difference_in_means &lt;- function(population, n1 = 100, n2 = 100){
  
  # get the mean age of a random sample of size n1
  mean_age_dem &lt;- population %&gt;%
    pull(age) %&gt;%
    sample(size = n1) %&gt;%
    mean
  
  # get the mean age of a random sample of size n2
  mean_age_rep &lt;- population %&gt;%
    pull(age) %&gt;%
    sample(size = n2) %&gt;%
    mean
  
  # return the difference
  mean_age_rep - mean_age_dem
}

difference_in_means(CCES, n1 = 100, n2 = 100)
```

```
[1] 0.98
```

---

## Get Sampling Distribution


```r
observed &lt;- mean(rep_age) - mean(dem_age)
draws &lt;- replicate(10000, difference_in_means(CCES, n1 = 100, n2 = 100))

# sampling distribution
tibble(draws) %&gt;% 
  ggplot() + 
  geom_histogram(aes(x=draws), color = 'black') +
  labs(x = 'Difference in Means', y = 'Count') +
  theme_bw() + 
  geom_vline(xintercept = observed, linetype = 'dashed', color = 'red')
```

&lt;img src="07-Probability_files/figure-html/hypothesis test: ages-1.png" width="600" style="display: block; margin: auto;" /&gt;

```r
# p-value
sum(abs(draws) &gt; observed) / length(draws)
```

```
[1] 0.5872
```

---

## T Test

Hey good news; you can do it with one line.


```r
t.test(rep_age, dem_age)
```

```

	Welch Two Sample t-test

data:  rep_age and dem_age
t = 0.52572, df = 197.56, p-value = 0.5997
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -3.79658  6.55658
sample estimates:
mean of x mean of y 
    50.96     49.58 
```


---

## Another


```r
n1 &lt;- 500
n2 &lt;- 500

CCES &lt;- CCES %&gt;% 
  mutate(rich = if_else(faminc_new &gt;= 10, 1, 0),
         support_reducing_MID = case_when(CC18_325b == 1 ~ 1,
                                          CC18_325b == 2 ~ 0))

rich_support &lt;- CCES %&gt;% 
  filter(rich == 1, !is.na(support_reducing_MID)) %&gt;% 
  pull(support_reducing_MID) %&gt;% 
  sample(n1)
  
non_rich_support &lt;- CCES %&gt;% 
  filter(rich == 0, !is.na(support_reducing_MID)) %&gt;% 
  pull(support_reducing_MID) %&gt;% 
  sample(n2)

mean(rich_support)
```

```
[1] 0.588
```

```r
mean(non_rich_support)
```

```
[1] 0.644
```
---

# Function


```r
difference_in_proportions &lt;- function(population, n1, n2){
  
  # proportion who support reducing MID from a random sample of size n1
  rich_proportion &lt;- population %&gt;% 
    pull(support_reducing_MID) %&gt;% 
    sample(n1) %&gt;% 
    mean
  
  # proportion who support reducing MID from a random sample of size n1
  non_rich_proportion &lt;- population %&gt;% 
    pull(support_reducing_MID) %&gt;% 
    sample(n2) %&gt;% 
    mean
  
  # return the difference in proportions
  rich_proportion - non_rich_proportion
  
}
```

---

## Monte Carlo


```r
observed &lt;- mean(rich_support) - mean(non_rich_support)
CCES &lt;- CCES %&gt;% filter(!is.na(support_reducing_MID))
draws &lt;- replicate(20000, difference_in_proportions(CCES, n1 = 500, n2 = 500))

sum(abs(draws) &gt; abs(observed)) / length(draws)
```

```
[1] 0.0564
```

```r
# sampling distribution
tibble(draws) %&gt;% 
  ggplot() + 
  geom_histogram(aes(x=draws), color = 'black') +
  labs(x = 'Difference in Proportions', y = 'Count') +
  theme_bw() + 
  geom_vline(xintercept = observed, linetype = 'dashed', color = 'red')
```

&lt;img src="07-Probability_files/figure-html/proportion monte carlo-1.png" width="600" style="display: block; margin: auto;" /&gt;

---

## Proportion Test


```r
prop.test(x = c(sum(rich_support), sum(non_rich_support)),
          n = c(n1,n2),
          correct = FALSE) 
```

```

	2-sample test for equality of proportions without continuity
	correction

data:  c(sum(rich_support), sum(non_rich_support)) out of c(n1, n2)
X-squared = 3.3144, df = 1, p-value = 0.06868
alternative hypothesis: two.sided
95 percent confidence interval:
 -0.116188448  0.004188448
sample estimates:
prop 1 prop 2 
 0.588  0.644 
```

```r
# last argument is "Yates' continuity correction" don't worry about it. not with this big of a sample.
```

---


class: center, middle

# Glossary of Terms

???

I threw a lot of new vocabulary at you

---

# Glossary of Terms

---

### Probability Distribution Function (PDF)

--

A function that describes the likelihood of random events. Must be greater than or equal to zero and sum to 1.

--

### Expected Value

--

The average value of a PDF. `\(\sum xf(x)\)` or `\(\int xf(x)dx\)`

--

### Law of Large Numbers

--

As your sample size approaches infinity, the sample mean converges on the expected value. `\(\bar{X}_n \to E(X)\)`

---

# Glossary of Terms

--

### Variance

--

The average squared distance of a random variable from its expected value. Denoted `\(\sigma^2\)`.

--

### Standard Deviation

--

The square root of variance. Denoted `\(\sigma\)`.

--

### Sampling Distribution

--

The distribution of a test statistic from repeated resampling. (E.g. if you were to repeatedly rerun a random poll, your sampling distribution would be binomial centered around the true mean.)

--

### Standard Error

--

The standard deviation of the sampling distribution. `\(SE = \frac{\sigma}{\sqrt{N}}\)` for the mean of a sample.

--

### Central Limit Theorem

--

The sampling distribution of the mean of a large number `\((n&gt;30)\)` of independent random variables will be normally distributed around the true mean.

---

# Glossary of Terms

--

### Bernoulli Distribution

`\(P(X=1) = p\)` and `\(P(X=0) = 1-p\)`

--

### Binomial Distribution

The sum of `\(n\)` Bernoulli random variables

--

### Normal Distribution

--

A continuous, symmetric probability distribution function; 95% of the distribution falls within `\([\mu - 2\sigma, \mu + 2\sigma]\)`.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
