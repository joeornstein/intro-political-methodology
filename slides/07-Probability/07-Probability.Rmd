---
title: "Probability & Inference (Part 1?)"
# author: "Joseph T. Ornstein"
# date: "June 12, 2020"
output:
  xaringan::moon_reader:
    nature:
      highlightStyle: github
      countIncrementalSlides: false
subtitle: 'POLS 7012: Introduction to Political Methodology'
# institute: University of Georgia
---

<style>

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

</style>

```{r Setup, include=FALSE}
library(tidyverse)
theme_set(theme_minimal(base_size = 16))
knitr::opts_chunk$set(comment=NA, fig.width=7, fig.height=5, 
                      fig.align = 'center', out.width = 600,
                      message=FALSE, warning=FALSE, echo=TRUE)
set.seed(42)
```


## Outline

A lot of probability/statistics courses are pretty abstract; the great thing about learning `R` first is that we can introduce this in a really hands-on way!

We'll start by learning about probability distribution functions, and learn how to draw random values in `R`.

We'll see how this procedure underlies **all** statistical inference.

Then, I think, Central Limit Theorem. A little backwards compared to the usual approach, but trust me, let's front-load the good stuff.

Random Variables and their properties: Expectation, Variance, Law of Large Numbers

Multivariable PDFs: Covariance, Joint Probability, Conditional Probability, Bayes Rule

As a bonus, you'll get to practice creating your own functions...very useful when you want to customize / mix-and-match the built-in functions we've learned.

---

# Warm Up

- Create an `R` project where you can keep all your code from this session.

- Create `week7.R` in your project folder. Load `tidyverse`.

--

- Create a object `x` and assign it a vector with the elements 1, 2, 3, and 4.

--

```{r vector x}
x <- c(1,2,3,4)

# this works too
x <- 1:4
```

--

- Find the sum of your vector `x`. Find the length of your vector `x`

--

```{r x functions}
sum(x)

length(x)
```

---

# Warm Up

- What does this expression mean?

$$\int_{-\infty}^{+\infty} f(x)dx = 1$$
--

The area under the entire curve $f(x)$, from positive to negative infinity, equals $1$. 

--

<br>

Keep a copy of these slides open so that you can copy-paste code into your script when needed!

---

## Learning Objectives

By the end of this module, you will be able to...

- Write your own functions and generate random values in `R`

- Explain several foundational concepts in probability theory
  - Probability Distribution Functions (PDFs)
  - Random Variables
  - Expectation
  - Variance
  - Central Limit Theorem
  - Law of Large Numbers
  - Bayes Rule

- Make inferences about populations from sample data

---

class: center, middle

## The Trick Coin Problem

---

## The Trick Coin Problem

Suppose someone flips a coin 100 times and it lands on heads 65 times.

```{r coin gif, echo = FALSE}
knitr::include_graphics('img/flipping-coin-animated-gif-1.gif')
```

--

**The Challenge:** Is it a trick coin? Or did they just get lucky? How would we know? What is the *probability* of getting that many heads with a fair coin?

---

## The Trick Coin Problem

**Our Approach**: Take a fair coin, flip it 10 million times, and see how often we get 65 heads in 100 flips.

--

```{r schitts creek gif, echo = FALSE, out.width = '70%'}
knitr::include_graphics('img/schitts-creek.gif')
```

On second thought, that would be exhausting, so let's make the computer do it instead.

---

class: center, middle

## Probability Distribution Functions (PDFs)

---

## Probability Distribution Functions (PDFs)

A special kind of function that describes the likelihood of random events.

--

### Property 1

$$P(x) \geq 0$$

Nothing has a less-than-zero chance of happening.

--

### Property 2

$$\sum_{x \in \Omega} P(x) = 1$$

The probability of **something** happening equals 1.

???

- (Probability not about coin flips and card games; probability is a function that characterizes our uncertainty over outcomes)

Functions take inputs and produce outputs. PDFs output the probability of an event.

(As Wasserman says, rigor and clarity not synonymous; if you want rigor, check out the books.)

---

## Example PDF: Flipping a Coin

$$\Omega = \{\text{heads}, \text{tails}\}$$

$$P(\text{heads}) = 0.5$$

$$P(\text{tails}) = 0.5$$

--

<br>

For you logophiles out there, this is also known as the **Bernoulli** distribution.

 - Two outcomes (0 and 1)
 - $P(1) = p$ and $P(0) = 1-p$

---

class: center, middle

## Generating Random Values

---

## Generating Random Values in `R`

The `sample` function draws values at random (with equal probability) from a vector:

```{r sample}
coin_outcomes <- c('Heads', 'Tails')

sample(coin_outcomes, size = 1)
```

--

```{r sample 2}
sample(coin_outcomes, size = 30, replace = TRUE)
```

---

## Generating Random Values in `R`

### Exercise 1

Flip the coin 100 times, and assign the result to an object called `flips`.

--

```{r flip exercise}
flips <- sample(coin_outcomes, size = 100, replace = TRUE)
```

--

### Exercise 2

Draw 5 letters at random from the alphabet. (Note: the vector `LETTERS` is built-in to R.)

--

```{r sample exercise}
sample(LETTERS, size = 5, replace = TRUE)
```

---

class: center, middle

## Creating Your Own Functions

---

## Creating Your Own Functions

- An **enormous** strength of working with `R`

- Get comfortable with creating your own functions, and you can make `R` do whatever you want.

---

## Creating Your Own Functions

Here's a (silly) function that takes two inputs and adds 1 to their sum:

```{r funtion syntax}
add_one <- function(x, y = 1){
  x + y + 1
}
```

--

- `add_one` is the name
- `<- function()` assigns a function to that name
- `x, y = 1` are the inputs (aka arguments); `y` has a default value of 1.
- The code within the brackets executes whenever you call the function, and the last line is the function's output.

---

## Creating Your Own Functions

Here's a (silly) function that takes two inputs and adds 1 to their sum:

```{r funtion syntax 2}
add_one <- function(x, y = 1){
  x + y + 1
}
```

What happens if you call `add_one(x = 1, y = 2)`?

--

```{r add one example 1}
add_one(x = 1, y = 2)
```

--

What about `add_one(1)`?

--

```{r add one example 2}
add_one(1)
```




---

## Creating Your Own Functions

Here's a function that will flip a coin $N$ times and return the result.

```{r flip coin}
flip_coin <- function(N = 100){
  # specify the sample space of outcomes
  coin_outcomes <- c('Heads', 'Tails')
  
  # return a vector of N random coin flips
  sample(coin_outcomes,
         size = N,
         replace = TRUE)
}

flip_coin(N = 8)
```

---

## Creating Your Own Functions

Here's a function that will flip a coin $N$ times and return the number that landed on heads. (**Note**: it calls the `flip_coin()` function we just made!)

```{r count_heads()}
count_heads <- function(N){
  
  # flip the coin N times, assign to 'flips' vector
  flips <- flip_coin(N)
  
  # return the number of heads divided by number of flips
  sum(flips == 'Heads')
}
```

--

```{r call count_heads}
count_heads(N = 100)
```

???

Functions are like Las Vegas.

---

## Creating Your Own Functions

### Exercise 1

Create a function called `sample_n_letters()`, which samples $N$ letters at random from the alphabet.

--

```{r create sample letters function}
sample_n_letters <- function(num_letters = 5){
  sample(LETTERS, size = num_letters, replace = TRUE)
}

sample_n_letters(num_letters = 10)
```

---

## Creating Your Own Functions

### Exercise 2

Create a function called `fraction_Q()`, which samples $N$ letters from the alphabet at random and returns the fraction that are Q's.

--

```{r create function 2}
fraction_Q <- function(num_letters = 5){
  
  # draw n letters
  sample_letters <- sample_n_letters(num_letters)
  
  # return the number of Q's
  sum(sample_letters == 'Q') / num_letters
}

fraction_Q(100)
```

---

class: center, middle

## Back to the Trick Coin Problem

---

## The Trick Coin Problem

We can solve this now. We have the technology.

--

```{r call count_heads once}
count_heads(N = 100)
```

--

Let's repeat that 100,000 times and see how often you get 65 heads.

--

```{r replicate}
# Flip a coin 100 times and count the number of heads. 
# Repeat that 100,000 times and save the results
results <- replicate(100000, count_heads(N = 100))
```

???

TODO: Questioning lifecycle; okay here's the approach:
create functions to draw the variable, compute the summary statistic, and replicate the heck out of it. Histogram that distribution.

---

## The Trick Coin Problem

```{r hist coin flips, out.width = '65%'}
tibble(results) %>% 
  ggplot() +
  geom_histogram(aes(x=results), color = 'black', 
                 binwidth = 1) +
  geom_vline(xintercept = 65, color = 'red') +
  labs(x = 'Number of Heads in 100 Fair Coin Flips',
       y = 'Count')
```

---

## The Trick Coin Problem

```{r hist coin flips 2, out.width = '40%'}
tibble(results) %>% 
  ggplot() +
  geom_histogram(aes(x=results), color = 'black', 
                 binwidth = 1) +
  geom_vline(xintercept = 65, color = 'red') +
  labs(x = 'Number of Heads in 100 Fair Coin Flips',
       y = 'Count')
```

By the way, that's called the **binomial** distribution. Take $n$ Bernoulli random variables and add them together. 

???

Out of 100,000 tries we only got a result like 65 a handful of times. It's a low-frequency event.

Next time I see that guy, I'm gonna say "hey let me see that coin". I can't prove it's fake, but, y'know, it's suspicious.

---

## The Trick Coin Problem

A **p-value** is the probability that we observe a result at least as extreme as our test statistic.

--

```{r compute p-value}
# the frequency of draws >= 65 as a fraction of all draws
sum(results >= 65) / length(results)
```

--

The probability of getting 65 or more heads from a fair coin is roughly 0.1%.

---

class: center, middle

## So What?

---

## The Polling Problem

**Exercise:** You've taken a random sample of 1,000 voters. 526 say they plan to vote for the Democrat and 474 say they plan to vote for the Republican.

If 50% of all voters planned to vote for the Republican, what is the probability you would observe a poll so favorable to Democrats?

---

## The Polling Problem

```{r polling problem}
# create 100,000 random polls of 1,000 respondents each
results <- replicate(100000, count_heads(N = 1000))

# what's the probability of getting at least 526 Democratic voters?
sum(results >= 526) / length(results)
```

```{r polling histogram, echo = FALSE, out.width = '65%'}
tibble(results) %>% 
  ggplot() +
  geom_histogram(aes(x=results), color = 'black', binwidth = 5) +
  geom_vline(xintercept = 526, color = 'red') +
  labs(x = 'Support for Democrat in a Random Poll',
       y = 'Number of Polls')
```

---

## Statistical Inference

That's called the sampling distribution. And that's the trick we use over and over and over and over again in empirical social science.

That weird trick is called **frequentist statistical inference**.

--

### Three Steps

1. Define your Null Hypothesis $(H_0)$

2. Construct the **sampling distribution** of the test statistic under $H_0$

3. Compare the test statistic from your observed data with the sampling distribution.

--

Every (frequentist) hypothesis test is a variation on this! The only thing that changes is the PDF. 

---

class: center, middle

## Expected Value and Variance

---

## Expected Value

The **expected value** is the average value of a random variable $X$.

- Specifically, a *weighted average* of all possible outcomes times their probability.

Discrete: 
$$E(X) = \sum_x x f(x)$$

Continuous:
$$E(X) = \int_{-\infty}^{+\infty} x f(x)dx$$

---

## Expected Value

Suppose $X$ is distributed as follows:

$P(X = 0) = 1-p$ and $P(X = 1) = p$

What is $E(X)$?

--

$$E(X) = 0(1-p) + 1p = p$$

---


class: center, middle

## The Normal Distribution

---

## The Normal Distribution

---

## Structure

expected values, 95% within 2 standard deviations, plot, etc.

---

## Where Does The Normal Distribution Come From?

---

## Import CCES

```{r load CCES, out.width = '50%'}
CCES <- read_rds('data/CCES_2018.RDS') %>% 
  mutate(age = 2018 - birthyr)

ggplot(data = CCES) + 
  geom_histogram(aes(x=age), color = 'black', binwidth = 2) +
  labs(x = 'Age', y = 'Count')

mean(CCES$age)
```

---

## Take a Sample

```{r sample age}
# draw a sample of ages and take the mean
mean_age_of_sample <- function(data, n){
  data %>%
    pull(age) %>%
    sample(size = n) %>%
    mean
}

mean_age_of_sample(data = CCES, n = 100)

mean_age_of_sample(data = CCES, n = 100)
```

--

The mean from any one sample will always be off a bit.

---

## The Sampling Distribution

```{r repeat mean sample ages, out.width='60%'}
draws <- replicate(10000, mean_age_of_sample(data = CCES, n = 100))

tibble(draws) %>% 
  ggplot() + 
  geom_histogram(mapping = aes(x=draws), color = 'black') +
  labs(x = 'Mean Age From Sample', y = 'Count')
```

--

It's the normal distribution!

---

class: center, middle

## The Central Limit Theorem

The mean of a large number of independent random variables will be (approximately) normally distributed.

---

## Central Limit Theorem

<iframe width="860" height="455" src="https://upload.wikimedia.org/wikipedia/commons/d/dc/Galton_box.webm" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

???

Imagine that each ball is a sample: when it goes left that means we randomly selected a person younger than the average, when it goes right that means we randomly selected a person older than the average. Most samples end up pretty close to the true mean, but sometimes you get really unlucky and draw a bunch of old people or a bunch of young people.

---

class: center, middle

## Standard Errors

---

## Standard Errors

The **standard error** is the standard deviation of the sampling distribution. 

--

> What is the standard deviation of the sampling distribution you just created?

--

```{r standard error}
sd(draws)
```

--

> What if you increased the sample size to 400?

--

```{r quadruple sample size}
draws <- replicate(10000, mean_age_of_sample(data = CCES, n = 400))

sd(draws)
```


---

## Standard Errors

Quadrupling the sample size cut the standard error in half.

--

That's because standard error equals:

$$SE = \frac{\sigma}{\sqrt{N}}$$
--

<br>

What happens to the standard error as $N \to \infty$?

---


class: center, middle

## The Law of Large Numbers

If you repeatedly draw a random variable a **large number of times**, the sample mean will converge on the expected value.

$$\bar{X}_n \overset{P}{\to} E(X)$$

???

Quite easy to understand Law of Large Numbers once you understand CLT.

---

## Let's Prove It

```{r law of large numbers}
num_draws <- 10000 # draw 10,000 Benoulli random variables
p <- 0.4 # p = 0.4

draws <- sample(c(0,1), num_draws, replace = TRUE, prob = c(1-p, p))

# create a dataframe and plot sample mean as n increases
fig <- tibble(n = 1:num_draws,
       draws) %>% 
  mutate(sample_mean = cumsum(draws) / n) %>% 
  ggplot() + 
  geom_line(mapping = aes(x = n, y = sample_mean)) + 
  geom_hline(yintercept = p, color = 'red', linetype = 'dashed') +
  labs(x = 'n', y = 'Sample Mean')
```

---

## Let's Prove It

```{r lln fig, echo = FALSE}
fig
```


---

class: center, middle

## Confidence Intervals

---

## Confidence Intervals

Man, what a wacky idea.

The 95% confidence interval:

If you were to repeat this procedure a large number of times, the true value would lie within the confidence interval 95% of the time. 

Alternatively, it is the set of values for which we can't reject the null hypothesis with $p < 0.05$.

---

## Let's Do A Hypothesis Test

```{r t test}
CCES %>% 
  filter(ownhome < 3,
         !is.na(CC18_325b)) %>% 
  mutate(ownhome = case_when(ownhome == 1 ~ 'Own',
                             ownhome == 2 ~ 'Rent'),
         support_reducing_MID = case_when(CC18_325b == 1 ~ 1,
                                          CC18_325b == 2 ~ 0)) %>%
  group_by(ownhome) %>% 
  summarize(pct_supporting_reduced_MID = mean(support_reducing_MID),
            num = n())

# respondents making over $100,000
CCES %>% 
  filter(faminc_new %in% 1:16,
         !is.na(CC18_325b)) %>% 
  mutate(rich = if_else(faminc_new >= 10, 1, 0),
         support_reducing_MID = case_when(CC18_325b == 1 ~ 1,
                                          CC18_325b == 2 ~ 0)) %>%
  group_by(rich) %>% 
  summarize(pct_supporting_reduced_MID = mean(support_reducing_MID),
            num = n())
```