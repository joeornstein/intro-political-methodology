<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Correlation (Part 1 of 2)</title>
    <meta charset="utf-8" />
    <link href="09-Correlation_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="09-Correlation_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Correlation (Part 1 of 2)

---


&lt;style&gt;

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

&lt;/style&gt;




&lt;!-- ## Recap --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Week 1: Introduction to `R` --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Week 2: Visualizing Data --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Week 3: Reproducible Research --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Weeks 4-5: Data Wrangling --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Week 6: Calculus --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Weeks 7-8: Probability and Inference --&gt;

&lt;!-- --- --&gt;

## Preview

--

#### October 21 &amp; 28: Correlation

- Covariance and Linear Regression
- Matrix Algebra

--

#### November 4: Prediction

- Models and Machine Learning
- Overfitting, Cross-Validation, Regularization, and Ensembles

--

#### November 11 &amp; 18: Causation

- Analyzing Experiments
- Observational Causal Inference

--

#### November 25: Thanksgiving

--

#### December 2 &amp; 9: Bonus Weeks!

- *Possible Topics*: Big Data, Text-As-Data, Networks, Spatial/Geographic Data, Advanced `R`, Advanced Visualizations (Interactives/Animations)

---

## Correlation

By the end of this module you will be able to...

--

1. Compute covariance and correlation coefficients between two variables.

--

2. Estimate the slope of a line of best fit, plus confidence intervals and p-values.

--

3. Fit multivariable linear models using matrix algebra.


???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)

---

class: center, middle

## Covariance and Correlation

---

## Covariance

Recall that the **variance** of a random variable is its expected squared distance from the mean:

`$$\text{Var}(X) = E[(X-E(X))^2]$$`

--

The **covariance** extends that definition of variance to two random variables `\(X\)` and `\(Y\)`:

`$$\text{Cov}(X,Y) = E[(X-E(X))(Y-E(Y))]$$`
--

Note that:

`$$\text{Cov}(X,X) = \text{Var}(X)$$`

---

## Covariance

Covariance captures the degree of association between two variables. Does `\(X\)` tend to be high when `\(Y\)` is high?


```r
flower_plot &lt;- ggplot(data = iris) + 
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length)) +
  labs(x = 'Sepal Length', y = 'Petal Length', title = 'Flower Measurements') +
  geom_vline(xintercept = mean(iris$Sepal.Length), linetype = 'dashed') +
  geom_hline(yintercept = mean(iris$Petal.Length), linetype = 'dashed')

flower_plot
```

&lt;img src="09-Correlation_files/figure-html/covariance-1.png" width="50%" style="display: block; margin: auto;" /&gt;

???

Notice that whenever `\(X\)` is greater than its mean, `\(Y\)` tends to be greater than its mean, and vice versa. 

When two variables `\(X\)` and `\(Y\)` **covary** with one another, `\(X\)` tends to be high whenever `\(Y\)` is high, and `\(X\)` tends to be low whenever `\(Y\)` is low.

---

## Covariance

Because petal length tends to be larger than average whenever sepal length is larger than average (and vice versa) when you take the mean of all the the `\((X-\bar{X})(Y-\bar{Y})\)`, you get a positive number.


```r
cov(iris$Sepal.Length, iris$Petal.Length)
```

```
[1] 1.274315
```

When covariance is positive, `\(X\)` and `\(Y\)` tend to move together. When covariance is negative, `\(X\)` and `\(Y\)` tend to move in opposite directions.

---

## Correlation Coefficients

The problem with covariance is that it's not easily interpretable. What does a covariance of 1.2743154 mean? How strong is that relationship?

--

The **correlation** coefficient solves that problem by standardizing the covariance.

`$$\text{Cor}(X,Y) = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$$`
--

This yields a value between `\(-1\)` (perfectly anti-correlated) and `\(+1\)` (perfectly correlated).


```r
cor(iris$Sepal.Length, iris$Petal.Length)
```

```
[1] 0.8717538
```

???

Kind of like how standard deviation standardizes the variance by taking the square root.

---

## Let's Play "Guess The Correlation"

&lt;img src="09-Correlation_files/figure-html/game 1-1.png" width="600" style="display: block; margin: auto;" /&gt;

--

Actual Correlation: 0.3042285

---

## Let's Play "Guess The Correlation"

&lt;img src="09-Correlation_files/figure-html/game 2-1.png" width="600" style="display: block; margin: auto;" /&gt;

--

Actual Correlation: -0.7702996

---

## Let's Play "Guess The Correlation"

&lt;img src="09-Correlation_files/figure-html/game 3-1.png" width="600" style="display: block; margin: auto;" /&gt;

--

Actual Correlation: 0.5387565

---

## Let's Play "Guess The Correlation"

For more fun, try [http://guessthecorrelation.com/](http://guessthecorrelation.com/)

![](img/guess-the-correlation.png)

---

class: center, middle

## Linear Regression

---

## Linear Regression



Correlation coefficients are nice, but limited. Both of these have the same correlation coefficient (0.696 and 0.696).

.pull-left[
&lt;img src="09-Correlation_files/figure-html/cor1-1.png" width="600" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="09-Correlation_files/figure-html/cor2-1.png" width="600" style="display: block; margin: auto;" /&gt;
]

--

We want to find the **slope** of the relationship. 

  - When we increase `\(X\)` by 1, how much does `\(Y\)` increase or decrease, on average?
  
--

  - We would also like to perform hypothesis tests and compute confidence intervals.

---

## Linear Regression

The two-variable linear model looks like this:

`$$Y = a + bX + \varepsilon$$`

--

**Terms:**

- `\(Y\)` is a **vector** of outcomes

- `\(X\)` is a **vector** we're using to predict the outcome

- `\(a\)` is the y-intercept 

- `\(b\)` is the slope of the relationship between `\(X\)` and `\(Y\)`, and

- `\(\varepsilon\)` is **vector** of random error

  - The difference between the true value of `\(Y\)` and the predicted value `\(a + bX\)`.

---

## Linear Regression

`$$Y = a + bX + \varepsilon$$`

#### Example:

.pull-left[
`\(X = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}\)`
]

.pull-right[
`\(Y = \begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}\)`
]

&lt;br&gt;

`\(a = 2\)`, `\(b = 2\)`

--

`$$\underbrace{\begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}}_Y = \underbrace{2 \times \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}}_a + \underbrace{2 \times \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}}_{bX} + \underbrace{\begin{bmatrix} 0 \\ -2 \\ 0 \end{bmatrix}}_\varepsilon$$`

---

### Linear Regression Finds the "Line of Best Fit"


```r
flower_plot +
  geom_smooth(aes(x=Sepal.Length, y=Petal.Length),
              method = 'lm', se = FALSE)
```

&lt;img src="09-Correlation_files/figure-html/line of best fit-1.png" width="600" style="display: block; margin: auto;" /&gt;

---

## How do we estimate the line of best fit?

To see how, let's first generate some random data.


```r
n &lt;- 300 # sample size
a &lt;- 2 # y-intercept
b &lt;- 3 # slope

# X and epsilon are normally distributed
# Y = a + bX + epsilon
data &lt;- tibble(X = rnorm(n, 1, 1), 
               epsilon = rnorm(n, 0, 2),
               Y = a + b*X + epsilon)

cor(data$X, data$Y)
```

```
[1] 0.850003
```

---

## How do we estimate the line of best fit?


```r
ggplot(data) +
  geom_point(mapping = aes(x=X,y=Y))
```

&lt;img src="09-Correlation_files/figure-html/plot bivariate regression DGP-1.png" width="600" style="display: block; margin: auto;" /&gt;

---

## How do we estimate the line of best fit?

To make things easier, we will ignore the y-intercept for now.


```r
demeaned_data &lt;- data %&gt;% 
  mutate(Y = Y - mean(Y), X = X - mean(X))
```

--

The data looks the same; we've just shifted it down and to the left.

.pull-left[

```r
ggplot(data) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Original Data')
```

&lt;img src="09-Correlation_files/figure-html/non-demeaned plot-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]

.pull-right[

```r
ggplot(demeaned_data) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Demeaned Data')
```

&lt;img src="09-Correlation_files/figure-html/demeaned plot-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]

???

You'll thank me when we start doing the calculus.

---

## How do we estimate the line of best fit?

--

The "best" is the one that minimizes error. Specifically, we're going to find the line that minimizes the **sum of squared errors**.

`$$Y = bX + \varepsilon$$`
`$$\varepsilon = Y - bX$$`
--

Let's create a function called `\(f(b)\)` equal to the sum of squared errors:

`$$f(b) = \sum \varepsilon_i^2 = \sum(Y_i-bX_i)^2$$`
--

Distribute:

`$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$`

---

## Three Steps to Minimize a Function?

--

#### Step 1: Take the derivative

`$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$`

`$$\frac{\partial f}{\partial b} = -2\sum X_iY_i + 2b \sum X_i^2$$`

--

#### Step 2: Set Equal to Zero

`$$-2\sum X_iY_i + 2b \sum X_i^2 = 0$$`
--

#### Step 3: Solve for `\(b\)`

`$$2\sum X_iY_i = 2b\sum X_i^2$$`
`$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$`
---

## Slope Estimate

`$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$`


```r
slope_estimate &lt;- 
  sum(demeaned_data$X * demeaned_data$Y) /
  sum(demeaned_data$X^2)

slope_estimate
```

```
[1] 3.152109
```

---

### Slope Estimate


```r
ggplot(demeaned_data) + 
  geom_point(mapping = aes(x=X,y=Y)) +
  geom_abline(intercept = 0, slope = slope_estimate)
```

&lt;img src="09-Correlation_files/figure-html/plot estimated slope-1.png" width="600" style="display: block; margin: auto;" /&gt;

---

## Some Terminology

The slope parameter `\(b\)` is called the **estimand**. It is the thing we are trying to estimate.

`\(\frac{\sum X_iY_i}{\sum X_i^2}\)` is the **estimator**. It is the equation we use to produce our estimate.

3.152 is our **estimate**.

  - Typically, we denote estimates with little hats, like this: `\(\hat{b} =\)` 3.152.

---

## Interesting Footnote

Notice that our estimator `\(\frac{\sum X_iY_i}{\sum X_i^2}\)` is equal to `\(\frac{\sum (X_i-0)(Y_i-0)}{\sum (X_i-0)^2}\)`, which is equal to `\(\frac{\sum (X_i-\bar{X})(Y_i-\bar{Y})}{\sum (X_i-\bar{X})^2}\)` because `\(\bar{X}=0\)` and `\(\bar{Y}=0\)`.

So another way of writing the estimator is `\(\hat{b} = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}\)`.

--


```r
slope_estimate
```

```
[1] 3.152109
```

```r
cov(demeaned_data$X, demeaned_data$Y) / var(demeaned_data$X)
```

```
[1] 3.152109
```

---

## Inference

We have a **point estimate** of the slope. What if we want confidence intervals and p-values? 

--

### Three Steps

--

1. Specify the Null Hypothesis `\((b = 0)\)`

--

2. Generate Sampling Distribution of `\(\hat{b}\)` assuming `\(b = 0\)`

--

3. Compare observed value of `\(\hat{b}\)` to the sampling distribution

---

### Where do we get the sampling distribution?

In a linear regression, the randomness comes from the error term `\(\varepsilon\)`. Imagine that we repeatedly draw a new `\(\varepsilon\)` vector with each sample.

--


```r
null_slope_estimate &lt;- function(X){
  
  b &lt;- 0 # null hypothesis: b = 0 
  n &lt;- length(X)
  
  # Create random dataset assuming null hypothesis
  epsilon &lt;- rnorm(n, 0, 2)
  Y &lt;- b*X + epsilon
  
  # Return the slope estimate
  sum(X * Y) / sum(X^2)
}

null_slope_estimate(X = demeaned_data$X)
```

```
[1] -0.07552862
```
---

## Generate the Sampling Distribution

&lt;TODO: Bump this to 20000&gt;


```r
sampling_distribution &lt;- replicate(200, null_slope_estimate(X = demeaned_data$X))

tibble(sampling_distribution) %&gt;% 
  ggplot() +
  geom_histogram(mapping = aes(x=sampling_distribution),
                 color = 'black', binwidth = 0.02) +
  labs(x='Slope Estimate (Sampling Distribution)') # a normally distributed sampling distribution again!
```

&lt;img src="09-Correlation_files/figure-html/sampling distribution-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

## P-values


```r
sum(sampling_distribution &gt; slope_estimate) # p-value effectively zero
```

```
[1] 0
```

## Confidence Intervals


```r
standard_error &lt;- sd(sampling_distribution)
standard_error
```

```
[1] 0.1210715
```

```r
confidence_interval &lt;- c(slope_estimate - 1.96 * standard_error,
                         slope_estimate + 1.96 * standard_error)
confidence_interval
```

```
[1] 2.914809 3.389410
```

---

## The One-Line Built-In `R` Function

The `lm()` function estimates the linear model parameters (slope + y-intercept) and computes confidence intervals and p-values.


```r
linear_model_fit &lt;- lm(formula = Y ~ X, 
                       data = demeaned_data) 

coef(linear_model_fit) # get the coefficient estimates
```

```
  (Intercept)             X 
-3.076740e-16  3.152109e+00 
```

```r
confint(linear_model_fit) # get the confidence intervals
```

```
                 2.5 %    97.5 %
(Intercept) -0.2196849 0.2196849
X            2.9294117 3.3748072
```

---

## The One-Line Built-In `R` Function


```r
summary(linear_model_fit)
```

```

Call:
lm(formula = Y ~ X, data = demeaned_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.1058 -1.2291 -0.0505  1.4577  5.4956 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -3.077e-16  1.116e-01    0.00        1    
X            3.152e+00  1.132e-01   27.86   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.934 on 298 degrees of freedom
Multiple R-squared:  0.7225,	Adjusted R-squared:  0.7216 
F-statistic: 775.9 on 1 and 298 DF,  p-value: &lt; 2.2e-16
```

???

Reports a bunch of other statistics and diagnostics that you will learn about in 7014. I gotta leave something for Mollie to teach you.

---

class: center, middle

## Multivariate Linear Regression

---

## Multivariate Linear Regression


```r
data &lt;- tibble(c = 1,
            x1 = rnorm(100, 0, 1),
            x2 = rnorm(100, 0, 1),
            y = 2*x1 + 3*x2 + rnorm(100, 0, 1))

linear_model &lt;- lm(y ~ x1 + x2, data = data)

summary(linear_model)
```

```

Call:
lm(formula = y ~ x1 + x2, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.2456 -0.7339  0.0145  0.7302  2.1772 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   0.1265     0.1060   1.194    0.236    
x1            1.9897     0.1102  18.058   &lt;2e-16 ***
x2            3.0345     0.1127  26.922   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.033 on 97 degrees of freedom
Multiple R-squared:  0.9133,	Adjusted R-squared:  0.9115 
F-statistic: 511.1 on 2 and 97 DF,  p-value: &lt; 2.2e-16
```

```r
# X matrix
X &lt;- data %&gt;% 
  select(c, x1, x2) %&gt;% 
  as.matrix

# y vector
y &lt;- data %&gt;% 
  pull(y)

# beta hat = (X'X)^-1 X'y
beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y

beta_hat
```

```
        [,1]
c  0.1264848
x1 1.9897382
x2 3.0344777
```

```r
linear_model # that's where the coefficients come from; the parameter estimates that minimize the sum of squared errors
```

```

Call:
lm(formula = y ~ x1 + x2, data = data)

Coefficients:
(Intercept)           x1           x2  
     0.1265       1.9897       3.0345  
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
