<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Correlation (Part 1 of 2)</title>
    <meta charset="utf-8" />
    <link href="09-Correlation_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="09-Correlation_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Correlation (Part 1 of 2)

---


&lt;style&gt;

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

&lt;/style&gt;




&lt;!-- ## Recap --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Week 1: Introduction to `R` --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Week 2: Visualizing Data --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Week 3: Reproducible Research --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Weeks 4-5: Data Wrangling --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Week 6: Calculus --&gt;

&lt;!-- -- --&gt;

&lt;!-- #### Weeks 7-8: Probability and Inference --&gt;

&lt;!-- --- --&gt;

## Preview

--

#### October 21 &amp; 28: Correlation

- Covariance and Linear Regression
- Matrix Algebra

--

#### November 4: Prediction

- Fitting Models and Machine Learning
- Cross-Validation, Regularization, and Ensembles

--

#### November 11 &amp; 18: Causation

- Experimental Data
- Observational Causal Inference

--

#### November 25: Thanksgiving

--

#### December 2 &amp; 9: Bonus Weeks!

- *Possible Topics*: Big Data, Text-As-Data, Networks, Spatial/Geographic Data, Advanced `R`, Advanced Visualizations (Interactives/Animations)

---

## Correlation

By the end of this module you will be able to...

--

1. Compute covariance and correlation coefficients.

--

2. Estimate the slope of a line of best fit, plus confidence intervals and p-values.

--

3. Fit multivariable linear models using matrix algebra.


???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)

---

class: center, middle

## Covariance and Correlation

---

## Covariance

Recall that the **variance** of a random variable is its expected squared distance from the mean:

`$$\text{Var}(X) = E[(X-E(X))^2]$$`

--

The **covariance** extends that definition of variance to two random variables `\(X\)` and `\(Y\)`:

`$$\text{Cov}(X,Y) = E[(X-E(X))(Y-E(Y))]$$`
--

Covariance captures the degree of association between two variables. Does `\(X\)` tend to be high when `\(Y\)` is high?

--

Note that:

`$$\text{Cov}(X,X) = \text{Var}(X)$$`

---

## Covariance


```r
flower_plot &lt;- ggplot(data = iris) + 
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length)) +
  labs(x = 'Sepal Length', y = 'Petal Length', title = 'Flower Measurements')
flower_plot
```

&lt;img src="09-Correlation_files/figure-html/covariance-1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

## Covariance


```r
flower_plot &lt;- flower_plot +
  geom_vline(xintercept = mean(iris$Sepal.Length), linetype = 'dashed') +
  geom_hline(yintercept = mean(iris$Petal.Length), linetype = 'dashed')
flower_plot
```

&lt;img src="09-Correlation_files/figure-html/flower plot 2-1.png" width="65%" style="display: block; margin: auto;" /&gt;

???

Notice that whenever `\(X\)` is greater than its mean, `\(Y\)` tends to be greater than its mean, and vice versa. 

When two variables `\(X\)` and `\(Y\)` **covary** with one another, `\(X\)` tends to be high whenever `\(Y\)` is high, and `\(X\)` tends to be low whenever `\(Y\)` is low.

---

## Covariance

Because petal length tends to be larger than average whenever sepal length is larger than average (and vice versa) when you take the mean of all the the `\((X-\bar{X})(Y-\bar{Y})\)`, you get a positive number.


```r
cov(iris$Sepal.Length, iris$Petal.Length)
```

```
[1] 1.274315
```

When covariance is positive, `\(X\)` and `\(Y\)` tend to move together. When covariance is negative, `\(X\)` and `\(Y\)` tend to move in opposite directions.

---

## Correlation Coefficients

The problem with covariance is that it's not easily interpretable. What does a covariance of 1.2743154 mean? How strong is that relationship?

--

The **correlation** coefficient solves that problem by standardizing the covariance.

`$$\text{Cor}(X,Y) = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$$`
--

This yields a value between `\(-1\)` (perfectly anti-correlated) and `\(+1\)` (perfectly correlated).


```r
cor(iris$Sepal.Length, iris$Petal.Length)
```

```
[1] 0.8717538
```

```r
cov(iris$Sepal.Length, iris$Petal.Length) / sd(iris$Sepal.Length) / sd(iris$Petal.Length)
```

```
[1] 0.8717538
```

???

Kind of like how standard deviation standardizes the variance by taking the square root.

---

## Let's Play "Guess The Correlation"

&lt;img src="09-Correlation_files/figure-html/game 1-1.png" width="600" style="display: block; margin: auto;" /&gt;

--

Actual Correlation: 0.3042285

---

## Let's Play "Guess The Correlation"

&lt;img src="09-Correlation_files/figure-html/game 2-1.png" width="600" style="display: block; margin: auto;" /&gt;

--

Actual Correlation: -0.7702996

---

## Let's Play "Guess The Correlation"

&lt;img src="09-Correlation_files/figure-html/game 3-1.png" width="600" style="display: block; margin: auto;" /&gt;

--

Actual Correlation: 0.5387565

---

## Let's Play "Guess The Correlation"

For more fun, try [http://guessthecorrelation.com/](http://guessthecorrelation.com/)

![](img/guess-the-correlation.png)

---

class: center, middle

# Linear Regression

---

## Linear Regression



Correlation coefficients are nice, but limited. Both pairs of variables below have the same correlation coefficient (0.696 and 0.696).

.pull-left[
&lt;img src="09-Correlation_files/figure-html/cor1-1.png" width="600" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="09-Correlation_files/figure-html/cor2-1.png" width="600" style="display: block; margin: auto;" /&gt;
]

--

We want to find the **slope** of the relationship (the "line of best fit").

  - When we increase `\(X\)` by 1, how much does `\(Y\)` increase or decrease, on average?
  
???

  - We would also like to perform hypothesis tests and compute confidence intervals.

---

## Linear Regression

Correlation coefficients are nice, but limited. Both pairs of variables below have the same correlation coefficient (0.696 and 0.696).

.pull-left[
&lt;img src="09-Correlation_files/figure-html/cor3-1.png" width="600" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="09-Correlation_files/figure-html/cor4-1.png" width="600" style="display: block; margin: auto;" /&gt;
]

We want to find the **slope** of the relationship (the "line of best fit").

  - When we increase `\(X\)` by 1, how much does `\(Y\)` increase or decrease, on average?

---

## Linear Regression

The two-variable linear model looks like this:

`$$Y = a + bX + \varepsilon$$`

--

**Terms:**

- `\(Y\)` is a **vector** of outcomes

- `\(X\)` is a **vector** we're using to predict the outcome

- `\(a\)` is the y-intercept 

- `\(b\)` is the slope of the relationship between `\(X\)` and `\(Y\)`, and

- `\(\varepsilon\)` is **vector** of random error

  - The difference between the true value of `\(Y\)` and the predicted value `\(a + bX\)`.

---

## Linear Regression

`$$Y = a + bX + \varepsilon$$`

#### Example:

.pull-left[
`\(X = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}\)`
]

.pull-right[
`\(Y = \begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}\)`
]

&lt;br&gt;

`\(a = 2\)`, `\(b = 2\)`

--

`$$\underbrace{\begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}}_Y = \underbrace{2 \times \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}}_a + \underbrace{2 \times \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}}_{bX} + \underbrace{\begin{bmatrix} 0 \\ -2 \\ 0 \end{bmatrix}}_\varepsilon$$`

---

## How do we estimate the line of best fit?

--

To see how, let's first generate some random data.


```r
n &lt;- 300 # sample size
a &lt;- 2 # y-intercept
b &lt;- 3 # slope

# X and epsilon are normally distributed
# Y = a + bX + epsilon
data &lt;- tibble(X = rnorm(n, 1, 1), 
               epsilon = rnorm(n, 0, 2),
               Y = a + b*X + epsilon)

cor(data$X, data$Y)
```

```
[1] 0.850003
```

---

## How do we estimate the line of best fit?


```r
ggplot(data) +
  geom_point(mapping = aes(x=X,y=Y))
```

&lt;img src="09-Correlation_files/figure-html/plot bivariate regression DGP-1.png" width="600" style="display: block; margin: auto;" /&gt;

---

## How do we estimate the line of best fit?

To make things easier, we will ignore the y-intercept for now.


```r
demeaned_data &lt;- data %&gt;% 
  mutate(Y = Y - mean(Y), X = X - mean(X))
```

--

.pull-left[

```r
ggplot(data) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Original Data')
```

&lt;img src="09-Correlation_files/figure-html/non-demeaned plot-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]

.pull-right[

```r
ggplot(demeaned_data) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Demeaned Data')
```

&lt;img src="09-Correlation_files/figure-html/demeaned plot-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]

???

The data looks the same; we've just shifted it down and to the left. You'll thank me when we start doing the calculus.

---

## How do we estimate the line of best fit?

--

The "best" line is the one that minimizes error. Specifically, we're going to find the line that minimizes the **sum of squared errors**.

`$$Y = bX + \varepsilon$$`
`$$\varepsilon = Y - bX$$`
--

Let's create a function called `\(f(b)\)` equal to the sum of squared errors:

`$$f(b) = \sum \varepsilon_i^2 = \sum(Y_i-bX_i)^2$$`
--

Distribute:

`$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$`

---

## Three Steps to Minimize a Function?

--

#### Step 1: Take the derivative

`$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$`

`$$\frac{\partial f}{\partial b} = -2\sum X_iY_i + 2b \sum X_i^2$$`

--

#### Step 2: Set Equal to Zero

`$$-2\sum X_iY_i + 2b \sum X_i^2 = 0$$`
--

#### Step 3: Solve for `\(b\)`

`$$2\sum X_iY_i = 2b\sum X_i^2$$`
`$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$`
---

## Slope Estimate

`$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$`


```r
slope_estimate &lt;- 
  sum(demeaned_data$X * demeaned_data$Y) /
  sum(demeaned_data$X^2)

slope_estimate
```

```
[1] 3.152109
```

---

### Slope Estimate


```r
ggplot(demeaned_data) + 
  geom_point(mapping = aes(x=X,y=Y)) +
  geom_abline(intercept = 0, slope = slope_estimate)
```

&lt;img src="09-Correlation_files/figure-html/plot estimated slope-1.png" width="600" style="display: block; margin: auto;" /&gt;

---

## Some Terminology

The slope parameter `\(b\)` is called the **estimand**. It is the thing we are trying to estimate.

`\(\frac{\sum X_iY_i}{\sum X_i^2}\)` is the **estimator**. It is the equation we use to produce our estimate.

3.152 is our **estimate**.

  - Typically, we denote estimates with little hats, like this: `\(\hat{b} =\)` 3.152.

---

## Interesting Footnote

Notice that our estimator `\(\frac{\sum X_iY_i}{\sum X_i^2}\)` is equal to `\(\frac{\sum (X_i-0)(Y_i-0)}{\sum (X_i-0)^2}\)`, which is equal to `\(\frac{\sum (X_i-\bar{X})(Y_i-\bar{Y})}{\sum (X_i-\bar{X})^2}\)` because `\(\bar{X}=0\)` and `\(\bar{Y}=0\)`.

So another way of writing the estimator is `\(\hat{b} = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}\)`.

--


```r
slope_estimate
```

```
[1] 3.152109
```

```r
cov(demeaned_data$X, demeaned_data$Y) / var(demeaned_data$X)
```

```
[1] 3.152109
```

---

## Inference

We have a **point estimate** of the slope ( `\(\hat{b} =\)` 3.152 ). What if we want an **interval estimate** (confidence intervals) and p-values? 

--

### Three Steps To Statistical Inference

--

1. Specify the Null Hypothesis `\((b = 0)\)`

--

2. Generate Sampling Distribution of `\(\hat{b}\)` assuming `\(b = 0\)`

--

3. Compare observed value of `\(\hat{b}\)` to the sampling distribution

---

### Where do we get the sampling distribution?

In a linear regression, the randomness comes from the error term `\(\varepsilon\)`. Imagine that we repeatedly draw a new `\(\varepsilon\)` vector with each sample.

--


```r
null_slope_estimate &lt;- function(X){
  
  b &lt;- 0 # null hypothesis: b = 0 
  n &lt;- length(X)
  
  # Create random dataset assuming null hypothesis
  epsilon &lt;- rnorm(n, 0, 2)
  Y &lt;- b*X + epsilon
  
  # Return the slope estimate
  sum(X * Y) / sum(X^2)
}

null_slope_estimate(X = demeaned_data$X)
```

```
[1] -0.07552862
```
---

## Generate the Sampling Distribution


```r
sampling_distribution &lt;- replicate(20000, null_slope_estimate(X = demeaned_data$X))

tibble(sampling_distribution) %&gt;% 
  ggplot() +
  geom_histogram(mapping = aes(x=sampling_distribution),
                 color = 'black', binwidth = 0.02) +
  labs(x='Slope Estimate (Sampling Distribution)') # a normally distributed sampling distribution again!
```

&lt;img src="09-Correlation_files/figure-html/sampling distribution-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

## P-values


```r
sum(sampling_distribution &gt; slope_estimate) # p-value effectively zero
```

```
[1] 0
```

## Confidence Intervals


```r
standard_error &lt;- sd(sampling_distribution)
standard_error
```

```
[1] 0.1166938
```

```r
confidence_interval &lt;- c(slope_estimate - 1.96 * standard_error,
                         slope_estimate + 1.96 * standard_error)
confidence_interval
```

```
[1] 2.923390 3.380829
```

---

## The One-Line Built-In `R` Function

The `lm()` function estimates the linear model parameters (slope + y-intercept) and computes confidence intervals and p-values.


```r
linear_model_fit &lt;- lm(formula = Y ~ X, 
                       data = demeaned_data) 

coef(linear_model_fit) # get the coefficient estimates
```

```
  (Intercept)             X 
-3.076740e-16  3.152109e+00 
```

```r
confint(linear_model_fit) # get the confidence intervals
```

```
                 2.5 %    97.5 %
(Intercept) -0.2196849 0.2196849
X            2.9294117 3.3748072
```

---

## The One-Line Built-In `R` Function


```r
summary(linear_model_fit)
```

```

Call:
lm(formula = Y ~ X, data = demeaned_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.1058 -1.2291 -0.0505  1.4577  5.4956 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -3.077e-16  1.116e-01    0.00        1    
X            3.152e+00  1.132e-01   27.86   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.934 on 298 degrees of freedom
Multiple R-squared:  0.7225,	Adjusted R-squared:  0.7216 
F-statistic: 775.9 on 1 and 298 DF,  p-value: &lt; 2.2e-16
```

???

Reports a bunch of other statistics and diagnostics that you will learn about in POLS 7014. I gotta leave something for Mollie to teach you.

---

## Exercise

1. Check out the documentation for the `mtcars` dataset by typing `?mtcars`.

2. What is the mean fuel efficiency of cars (`mpg`), grouped by the number of cylinders (`cyl`)?

3. Do cars with a manual transmission have significantly higher/lower horsepower than those with an automatic transmission?

4. What is the correlation between horsepower (`hp`) and fuel efficiency (`mpg`)? Visualize the relationship.

5. Fit a linear model with horsepower as the predictor variable `\((X)\)` and fuel efficiency as the outcome variable `\((Y)\)`. What is the slope of the relationship? What is the 95% confidence interval on that slope estimate?

---

## Exercise (Solution)

What is the mean fuel efficiency of cars (`mpg`), grouped by the number of cylinders (`cyl`)?


```r
mtcars %&gt;% 
  group_by(cyl) %&gt;% 
  summarize(mean_mpg = mean(mpg),
            num = n())
```

```
# A tibble: 3 x 3
    cyl mean_mpg   num
  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;
1     4     26.7    11
2     6     19.7     7
3     8     15.1    14
```

---

## Exercise (Solution)

Do cars with a manual transmission have significantly higher/lower horsepower than those with an automatic transmission?


```r
mtcars %&gt;% 
  t.test(hp ~ am, data = .)
```

```

	Welch Two Sample t-test

data:  hp by am
t = 1.2662, df = 18.715, p-value = 0.221
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -21.87858  88.71259
sample estimates:
mean in group 0 mean in group 1 
       160.2632        126.8462 
```

---

## Exercise (Solution)

What is the correlation between horsepower (`hp`) and fuel efficiency (`mpg`)?


```r
ggplot(data = mtcars) +
  geom_point(mapping = aes(x=hp, y=mpg)) +
  labs(x = 'Horsepower', y = 'Fuel Efficiency (Miles Per Gallon)',
       title = paste0('Correlation = ', cor(mtcars$hp, mtcars$mpg) %&gt;% round(3)))
```

&lt;img src="09-Correlation_files/figure-html/exercise 3-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

## Exercise (Solution)

Fit a linear model with horsepower as the predictor variable `\((X)\)` and fuel efficiency as the outcome variable `\((Y)\)`. What is the slope of the relationship? What is the 95% confidence interval on that slope estimate?


```r
cars_linear_model &lt;- lm(mpg ~ hp, data = mtcars)

coef(cars_linear_model)
```

```
(Intercept)          hp 
30.09886054 -0.06822828 
```

```r
confint(cars_linear_model)
```

```
                  2.5 %     97.5 %
(Intercept) 26.76194879 33.4357723
hp          -0.08889465 -0.0475619
```

---


class: center, middle

# Multivariable Linear Regression

---

## Multivariable Linear Regression

Suppose we want to include **multiple** explanatory variables. 

--

`$$\text{mpg} = \alpha + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon$$`
Cars with higher horsepower probably have lower fuel efficiency *and* heavier cars probably have lower fuel efficiency. We'd like to estimate the slope of both relationships simultaneously!

--

**Vector Representation**:

`$$\underbrace{\begin{bmatrix} 21.0 \\ 21.0 \\ 22.8 \\ \vdots \\ 21.4 \end{bmatrix}}_\text{mpg} = \underbrace{\alpha \times \begin{bmatrix} 1 \\ 1 \\1 \\ \vdots \\ 1 \end{bmatrix}}_\alpha + \underbrace{\beta_1 \times \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}}_{\beta_1 \text{hp}} + \underbrace{\beta_2 \times \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}}_{\beta_2 \text{wt}} + \underbrace{\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \vdots \\ \varepsilon_n \end{bmatrix}}_\varepsilon$$`
---

## Multivariable Linear Regression

The challenge is to simultaneously estimate `\(\alpha\)`, `\(\beta_1\)`, and `\(\beta_2\)`.

`$$\text{mpg} = \alpha + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon$$`
--

We've come as far as we can with scalar algebra. It's time you learned **matrix algebra**.

---

class: center, middle

# Matrix Algebra

---

## Matrix Algebra

Recall from Week 1 that a **matrix** is a bunch of vectors squished together.

--

.pull-left[
`\(\text{hp} = \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}\)`
]

.pull-right[
`\(\text{wt} = \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}\)`
]

--

&lt;br&gt;

`$$X = \begin{bmatrix} 110 &amp; 2.62 \\ 110 &amp; 2.875 \\ 93 &amp; 2.32 \\ \vdots &amp; \vdots \\ 109 &amp; 2.78 \end{bmatrix}$$`
???

We've been calling this a **dataframe**.

---

## Matrix Algebra

**Adding** and **subtracting** matrices is straightforward. Just add and subtract elementwise. 

.pull-left[
`\(A = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 3 \\ 4 &amp; 4 \end{bmatrix}\)`
]

.pull-right[
`\(B = \begin{bmatrix} 2 &amp; 1 \\ 4 &amp; 4 \\ 8 &amp; 5 \end{bmatrix}\)`
]

--

`$$A + B = \begin{bmatrix} 3 &amp; 3 \\ 6 &amp; 7 \\ 12 &amp; 9 \end{bmatrix}$$`
--

**Multiplying** and **dividing** is where it gets tricky.
  - You can only multiply *some* matrices together (they must be **conformable**)
  - And matrix division isn't really a thing. Instead, we multiply by the matrix's **inverse**.


---

## Matrix Multiplication

--

First, let me introduce the **dot product** of two vectors.

If `\(A = [3,1,2]\)` and `\(B = [1,2,3]\)`, then:

`$$A \cdot B = 3 \times 1 + 1 \times 2 + 2 \times 3 = 11$$`
--


```r
A &lt;- c(3,1,2)
B &lt;- c(1,2,3)

# dot product
sum(A*B)
```

```
[1] 11
```

---

## Matrix Multiplication

When you multiply two matrices, you take a series of dot products.

.pull-left[
`\(A = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 3 \end{bmatrix}\)`
]

.pull-right[
`\(B = \begin{bmatrix} 2 &amp; 1 \\ 4 &amp; 4 \end{bmatrix}\)`
]

--

&lt;br&gt;

`$$AB = \begin{bmatrix} 1 \times 2 + 2 \times 4 &amp; 1 \times 1 + 2 \times 4 \\ 2 \times 2 + 3 \times 4 &amp; 2 \times 1 + 3 \times 4 \end{bmatrix} = \begin{bmatrix} 10 &amp; 9 \\ 16 &amp; 14 \end{bmatrix}$$`

--

&lt;br&gt;

This is all very strange and confusing to get used to at first, but we'll soon see that it makes representing our multivariable linear regression problem a whole lot easier.

---

## Matrix Multiplication

You can multiply matrices in `R` with the `%*%` command.


```r
A &lt;- cbind(c(1,2), c(2,3))
A
```

```
     [,1] [,2]
[1,]    1    2
[2,]    2    3
```

```r
B &lt;- cbind(c(2,4), c(1,4))
B
```

```
     [,1] [,2]
[1,]    2    1
[2,]    4    4
```

--


```r
A %*% B
```

```
     [,1] [,2]
[1,]   10    9
[2,]   16   14
```

---

## To Be Continued...



???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
