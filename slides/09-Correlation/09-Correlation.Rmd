---
title: "Correlation (Part 1 of 2)"
# author: "Joseph T. Ornstein"
# date: "June 12, 2020"
output:
  xaringan::moon_reader:
    nature:
      highlightStyle: github
      countIncrementalSlides: false
# subtitle: 'POLS 7012: Introduction to Political Methodology'
# institute: University of Georgia
---

<style>

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

</style>

```{r Setup, include=FALSE}
library(tidyverse)
theme_set(theme_minimal(base_size = 16))
knitr::opts_chunk$set(comment=NA, fig.width=7, fig.height=5, 
                      fig.align = 'center', out.width = 600,
                      message=FALSE, warning=FALSE, echo=TRUE)
set.seed(42)
```


<!-- ## Recap -->

<!-- -- -->

<!-- #### Week 1: Introduction to `R` -->

<!-- -- -->

<!-- #### Week 2: Visualizing Data -->

<!-- -- -->

<!-- #### Week 3: Reproducible Research -->

<!-- -- -->

<!-- #### Weeks 4-5: Data Wrangling -->

<!-- -- -->

<!-- #### Week 6: Calculus -->

<!-- -- -->

<!-- #### Weeks 7-8: Probability and Inference -->

<!-- --- -->

## Preview

--

#### October 21 & 28: Correlation

- Covariance and Linear Regression
- Matrix Algebra

--

#### November 4: Prediction

- Fitting Models and Machine Learning
- Cross-Validation, Regularization, and Ensembles

--

#### November 11 & 18: Causation

- Experimental Data
- Observational Causal Inference

--

#### November 25: Thanksgiving

--

#### December 2 & 9: Bonus Weeks!

- *Possible Topics*: Big Data, Text-As-Data, Networks, Spatial/Geographic Data, Advanced `R`, Advanced Visualizations (Interactives/Animations)

---

## Correlation

By the end of this module you will be able to...

--

1. Compute covariance and correlation coefficients.

--

2. Estimate the slope of a line of best fit, plus confidence intervals and p-values.

--

3. Fit multivariable linear models using matrix algebra.


???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)

---

class: center, middle

## Covariance and Correlation

---

## Covariance

Recall that the **variance** of a random variable is its expected squared distance from the mean:

$$\text{Var}(X) = E[(X-E(X))^2]$$

--

The **covariance** extends that definition of variance to two random variables $X$ and $Y$:

$$\text{Cov}(X,Y) = E[(X-E(X))(Y-E(Y))]$$
--

Covariance captures the degree of association between two variables. Does $X$ tend to be high when $Y$ is high?

--

Note that:

$$\text{Cov}(X,X) = \text{Var}(X)$$

---

## Covariance

```{r covariance, out.width='65%'}
flower_plot <- ggplot(data = iris) + 
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length)) +
  labs(x = 'Sepal Length', y = 'Petal Length', title = 'Flower Measurements')
flower_plot
```

---

## Covariance

```{r flower plot 2, out.width='65%'}
flower_plot <- flower_plot +
  geom_vline(xintercept = mean(iris$Sepal.Length), linetype = 'dashed') +
  geom_hline(yintercept = mean(iris$Petal.Length), linetype = 'dashed')
flower_plot
```

???

Notice that whenever $X$ is greater than its mean, $Y$ tends to be greater than its mean, and vice versa. 

When two variables $X$ and $Y$ **covary** with one another, $X$ tends to be high whenever $Y$ is high, and $X$ tends to be low whenever $Y$ is low.

---

## Covariance

Because petal length tends to be larger than average whenever sepal length is larger than average (and vice versa) when you take the mean of all the the $(X-\bar{X})(Y-\bar{Y})$, you get a positive number.

```{r covariance computation}
cov(iris$Sepal.Length, iris$Petal.Length)
```

When covariance is positive, $X$ and $Y$ tend to move together. When covariance is negative, $X$ and $Y$ tend to move in opposite directions.

---

## Correlation Coefficients

The problem with covariance is that it's not easily interpretable. What does a covariance of `r cov(iris$Sepal.Length, iris$Petal.Length)` mean? How strong is that relationship?

--

The **correlation** coefficient solves that problem by standardizing the covariance.

$$\text{Cor}(X,Y) = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$$
--

This yields a value between $-1$ (perfectly anti-correlated) and $+1$ (perfectly correlated).

```{r correlation coefficient}
cor(iris$Sepal.Length, iris$Petal.Length)

cov(iris$Sepal.Length, iris$Petal.Length) / sd(iris$Sepal.Length) / sd(iris$Petal.Length)
```

???

Kind of like how standard deviation standardizes the variance by taking the square root.

---

## Let's Play "Guess The Correlation"

```{r game 1, echo = FALSE}
n <- 100
X <- rnorm(n,0,1)
Y <- X + rnorm(n,0,4)

ggplot(tibble(X,Y)) + 
  geom_point(aes(x=X,y=Y))
```

--

Actual Correlation: `r cor(X,Y)`

---

## Let's Play "Guess The Correlation"

```{r game 2, echo = FALSE}
n <- 100
X <- rnorm(n,0,1)
Y <- -X + rnorm(n,0,1)

ggplot(tibble(X,Y)) + 
  geom_point(aes(x=X,y=Y))
```

--

Actual Correlation: `r cor(X,Y)`

---

## Let's Play "Guess The Correlation"

```{r game 3, echo = FALSE}
n <- 100
X <- rnorm(n,0,1)
Y <- X + rnorm(n,0,2)

ggplot(tibble(X,Y)) + 
  geom_point(aes(x=X,y=Y))
```

--

Actual Correlation: `r cor(X,Y)`

---

## Let's Play "Guess The Correlation"

For more fun, try [http://guessthecorrelation.com/](http://guessthecorrelation.com/)

![](img/guess-the-correlation.png)

---

class: center, middle

# Linear Regression

---

## Linear Regression

```{r set up plots, echo = FALSE}
n <- 100
data <- tibble(X = rnorm(n,0,1),
               epsilon = rnorm(n,0,2),
               Y1 = 0.75*X + rnorm(n,0,0.75),
               Y2 = 3*X + rnorm(n,0,2.55))
```

Correlation coefficients are nice, but limited. Both pairs of variables below have the same correlation coefficient (`r cor(data$X, data$Y1) %>% round(3)` and `r cor(data$X, data$Y2) %>% round(3)`).

.pull-left[
```{r cor1, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y1)) +
  scale_y_continuous(limits = c(-10,10))
```
]

.pull-right[
```{r cor2, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y2)) +
  scale_y_continuous(limits = c(-10,10))
```
]

--

We want to find the **slope** of the relationship (the "line of best fit").

  - When we increase $X$ by 1, how much does $Y$ increase or decrease, on average?
  
???

  - We would also like to perform hypothesis tests and compute confidence intervals.

---

## Linear Regression

Correlation coefficients are nice, but limited. Both pairs of variables below have the same correlation coefficient (`r cor(data$X, data$Y1) %>% round(3)` and `r cor(data$X, data$Y2) %>% round(3)`).

.pull-left[
```{r cor3, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y1)) +
  scale_y_continuous(limits = c(-10,10)) +
  geom_smooth(aes(x=X,y=Y1), method = 'lm', se = FALSE)
```
]

.pull-right[
```{r cor4, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y2)) +
  scale_y_continuous(limits = c(-10,10)) +
  geom_smooth(aes(x=X,y=Y2), method = 'lm', se = FALSE)
```
]

We want to find the **slope** of the relationship (the "line of best fit").

  - When we increase $X$ by 1, how much does $Y$ increase or decrease, on average?

---

## Linear Regression

The two-variable linear model looks like this:

$$Y = a + bX + \varepsilon$$

--

**Terms:**

- $Y$ is a **vector** of outcomes

- $X$ is a **vector** we're using to predict the outcome

- $a$ is the y-intercept 

- $b$ is the slope of the relationship between $X$ and $Y$, and

- $\varepsilon$ is **vector** of random error

  - The difference between the true value of $Y$ and the predicted value $a + bX$.

---

## Linear Regression

$$Y = a + bX + \varepsilon$$

#### Example:

.pull-left[
$X = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}$
]

.pull-right[
$Y = \begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}$
]

<br>

$a = 2$, $b = 2$

--

$$\underbrace{\begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}}_Y = \underbrace{2 \times \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}}_a + \underbrace{2 \times \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}}_{bX} + \underbrace{\begin{bmatrix} 0 \\ -2 \\ 0 \end{bmatrix}}_\varepsilon$$

---

## The Line of Best Fit

The "line of best fit" is the one that minimizes error (specifically, the sum of squared errors).

```{r iris line of best fit, out.width='65%'}
ggplot(data = iris) +
  geom_point(mapping = aes(x = Petal.Width, y = Petal.Length)) + 
  geom_smooth(mapping = aes(x= Petal.Width, y = Petal.Length),
              method = 'lm', se = FALSE)
```

---

## Estimating The Line of Best Fit

To make things easier, we will ignore the y-intercept for now.

  - Create new variables called $X$ and $Y$, equal to Petal Width and Petal Length minus their means.

```{r demean}
demeaned_iris <- iris %>% 
  mutate(Y = Petal.Length - mean(Petal.Length), X = Petal.Width - mean(Petal.Width))
```

--

**Exercise**: What is the mean of $X$?

```{r mean x and y}
demeaned_iris$X %>% mean %>% round(4)
```

--

You'll thank me when we start doing the calculus.

---

## Estimating The Line of Best Fit

.pull-left[
```{r non-demeaned plot, out.width = '90%'}
ggplot(iris) +
  geom_point(mapping = aes(x=Petal.Width, y=Petal.Length)) +
  labs(x='X',y='Y',title = 'Original Data') +
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0)
```
]

.pull-right[
```{r demeaned plot, out.width='90%'}
ggplot(demeaned_iris) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Demeaned Data') +
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0)
```
]

--

The data looks the same; we've just shifted it down and to the left.

---

## Estimating The Line of Best Fit

--

The "best" line is the one that minimizes error. Specifically, we're going to find the line that minimizes the **sum of squared errors**.

$$Y = bX + \varepsilon$$
$$\varepsilon = Y - bX$$
--

Let's create a function called $f(b)$ equal to the sum of squared errors:

$$f(b) = \sum \varepsilon_i^2 = \sum(Y_i-bX_i)^2$$
--

Distribute:

$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$

---

## Three Steps to Minimize a Function?

--

#### Step 1: Take the derivative

$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$

$$\frac{\partial f}{\partial b} = -2\sum X_iY_i + 2b \sum X_i^2$$

--

#### Step 2: Set Equal to Zero

$$-2\sum X_iY_i + 2b \sum X_i^2 = 0$$
--

#### Step 3: Solve for $b$

$$2\sum X_iY_i = 2b\sum X_i^2$$
$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$
---

## Slope Estimate

$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$

```{r estimate slope}
slope_estimate <- 
  sum(demeaned_iris$X * demeaned_iris$Y) /
  sum(demeaned_iris$X^2)

slope_estimate
```

---

## Slope Estimate

```{r plot estimated slope}
ggplot(demeaned_iris) + 
  geom_point(mapping = aes(x=X,y=Y)) +
  geom_abline(intercept = 0, slope = slope_estimate)
```

---

## Some Terminology

The slope parameter $b$ is called the **estimand**. It is the thing we are trying to estimate.

$\frac{\sum X_iY_i}{\sum X_i^2}$ is the **estimator**. It is the equation we use to produce our estimate.

`r slope_estimate %>% round(3)` is our **estimate**.

  - Typically, we denote estimates with little hats, like this: $\hat{b} =$ `r slope_estimate %>% round(3)`.

---

## Interesting Footnote

Notice that our estimator $\frac{\sum X_iY_i}{\sum X_i^2}$ is equal to $\frac{\sum (X_i-0)(Y_i-0)}{\sum (X_i-0)^2}$, which is equal to $\frac{\sum (X_i-\bar{X})(Y_i-\bar{Y})}{\sum (X_i-\bar{X})^2}$ because $\bar{X}=0$ and $\bar{Y}=0$.

So another way of writing the estimator is $\hat{b} = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}$.

--

```{r another slope estimate}
slope_estimate

cov(demeaned_iris$X, demeaned_iris$Y) / var(demeaned_iris$X)
```



---

## Residuals

The vector of observed errors $(\hat{\varepsilon})$, also known as the **residuals**, is equal to $Y-\hat{b}X$.

```{r compute errors}
# Compute the epsilon vector
epsilon <- demeaned_iris$Y - slope_estimate * demeaned_iris$X
```

--

If we fit the line correctly, then the average error should equal zero.

```{r mean epsilon}
mean(epsilon)
```

---

class: center, middle

## Statistical Inference

---

## Statistical Inference

We now have a **point estimate** of the slope ( $\hat{b} =$ `r slope_estimate %>% round(3)` ). What if we want an **interval estimate** (confidence intervals) and p-values? 

--

### Three Steps

--

1. Specify the Null Hypothesis $(b = 0)$

--

2. Generate Sampling Distribution of $\hat{b}$ assuming $b = 0$

--

3. Compare observed value of $\hat{b}$ to the sampling distribution

---

### Where do we get the sampling distribution?

In a linear regression, the randomness comes from the error term $\varepsilon$. Imagine that we repeatedly draw a new $\varepsilon$ vector with each sample.

--

```{r sampling function}
null_slope_estimate <- function(X, epsilon){
  
  # Randomly sample a vector of epsilons
  epsilon <- sample(epsilon, replace = TRUE)
  
  # null hypothesis: b = 0
  b <- 0  
  
  # create a random dataset assuming the null hypothesis
  Y <- b*X + epsilon
  
  # Return the slope estimate
  sum(X * Y) / sum(X^2)
}

null_slope_estimate(X = demeaned_iris$X, epsilon)
```

---

## Generate the Sampling Distribution

```{r sampling distribution, out.width = '60%'}
sampling_distribution <- replicate(20000, null_slope_estimate(X = demeaned_iris$X, epsilon))

tibble(sampling_distribution) %>% 
  ggplot() +
  geom_histogram(mapping = aes(x=sampling_distribution),
                 color = 'black', binwidth = 0.01) +
  labs(x='Slope Estimate (Sampling Distribution)') # a normally distributed sampling distribution again!
```

---

## P-values

```{r p-value and confidence interval}
sum(sampling_distribution > slope_estimate) # p-value effectively zero
```

## Confidence Intervals

```{r confidence intervals}
standard_error <- sd(sampling_distribution)
standard_error

confidence_interval <- c(slope_estimate - 1.96 * standard_error,
                         slope_estimate + 1.96 * standard_error)
confidence_interval

```

---

## The One-Line Built-In `R` Function

The `lm()` function estimates the linear model parameters (slope + y-intercept) and computes confidence intervals and p-values.

```{r lm}
linear_model_fit <- lm(formula = Y ~ X, 
                       data = demeaned_iris) 

coef(linear_model_fit) # get the coefficient estimates

confint(linear_model_fit) # get the confidence intervals
```

---

## The One-Line Built-In `R` Function

```{r summary(lm)}
summary(linear_model_fit)
```

???

Reports a bunch of other statistics and diagnostics that you will learn about in POLS 7014. I gotta leave something for Mollie to teach you.

---

## Exercise

1. Check out the documentation for the `mtcars` dataset by typing `?mtcars`.

2. What is the mean fuel efficiency of cars (`mpg`), grouped by the number of cylinders (`cyl`)?

3. Do cars with a manual transmission have significantly higher/lower horsepower than those with an automatic transmission?

4. What is the correlation between horsepower (`hp`) and fuel efficiency (`mpg`)? Visualize the relationship.

5. Fit a linear model with horsepower as the predictor variable $(X)$ and fuel efficiency as the outcome variable $(Y)$. What is the slope of the relationship? What is the 95% confidence interval on that slope estimate?

---

## Exercise (Solution)

What is the mean fuel efficiency of cars (`mpg`), grouped by the number of cylinders (`cyl`)?

```{r exercise 1}
mtcars %>% 
  group_by(cyl) %>% 
  summarize(mean_mpg = mean(mpg),
            num = n())
```

---

## Exercise (Solution)

Do cars with a manual transmission have significantly higher/lower horsepower than those with an automatic transmission?

```{r exercise 2}
mtcars %>% 
  t.test(hp ~ am, data = .)
```

---

## Exercise (Solution)

What is the correlation between horsepower (`hp`) and fuel efficiency (`mpg`)?

```{r exercise 3, out.width = '60%'}
ggplot(data = mtcars) +
  geom_point(mapping = aes(x=hp, y=mpg)) +
  labs(x = 'Horsepower', y = 'Fuel Efficiency (Miles Per Gallon)',
       title = paste0('Correlation = ', cor(mtcars$hp, mtcars$mpg) %>% round(3)))
```

---

## Exercise (Solution)

Fit a linear model with horsepower as the predictor variable $(X)$ and fuel efficiency as the outcome variable $(Y)$. What is the slope of the relationship? What is the 95% confidence interval on that slope estimate?

```{r exercise 4}
cars_linear_model <- lm(mpg ~ hp, data = mtcars)

coef(cars_linear_model)

confint(cars_linear_model)
```

---


class: center, middle

# Multivariable Linear Regression

---

## Multivariable Linear Regression

Suppose we want to explain the outcome as a function of **multiple** explanatory variables. 

--

$$\text{mpg} = \alpha + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon$$
Fuel efficiency probably depends on both horsepower **and** weight. More powerful and heavier cars will tend to have lower fuel efficiency. We'd like to estimate the slope of both relationships simultaneously!

--

**Vector Representation**:

$$\underbrace{\begin{bmatrix} 21.0 \\ 21.0 \\ 22.8 \\ \vdots \\ 21.4 \end{bmatrix}}_\text{mpg} = \underbrace{\alpha \times \begin{bmatrix} 1 \\ 1 \\1 \\ \vdots \\ 1 \end{bmatrix}}_\alpha + \underbrace{\beta_1 \times \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}}_{\beta_1 \text{hp}} + \underbrace{\beta_2 \times \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}}_{\beta_2 \text{wt}} + \underbrace{\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \vdots \\ \varepsilon_n \end{bmatrix}}_\varepsilon$$
---

## Multivariable Linear Regression

The challenge is to simultaneously estimate $\alpha$, $\beta_1$, and $\beta_2$.

$$\text{mpg} = \alpha + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon$$
--

We've come as far as we can with scalar algebra. It's time you learned **matrix algebra**.

---

class: center, middle

# Matrix Algebra

---

## Matrix Algebra

Recall from Week 1 that a **matrix** is a bunch of vectors squished together.

--

.pull-left[
$\text{hp} = \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}$
]

.pull-right[
$\text{wt} = \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}$
]

--

<br>

$$X = \begin{bmatrix} 110 & 2.62 \\ 110 & 2.875 \\ 93 & 2.32 \\ \vdots & \vdots \\ 109 & 2.78 \end{bmatrix}$$
???

We've been calling this a **dataframe**.

---

## Matrix Algebra

The **dimension** of a matrix refers to the number of rows and columns. An $m \times n$ matrix has $m$ rows and $n$ columns.

--

```{r matrix dimensions}
dim(mtcars)
```

There are 32 rows and 11 columns in the mtcars data matrix.

---

## Matrix Algebra

**Adding** and **subtracting** matrices is straightforward. Just add and subtract elementwise. 

.pull-left[
$A = \begin{bmatrix} 1 & 2 \\ 2 & 3 \\ 4 & 4 \end{bmatrix}$
]

.pull-right[
$B = \begin{bmatrix} 2 & 1 \\ 4 & 4 \\ 8 & 5 \end{bmatrix}$
]

--

$$A + B = \begin{bmatrix} 3 & 3 \\ 6 & 7 \\ 12 & 9 \end{bmatrix}$$
--

**Multiplying** and **dividing** is where it gets tricky.
  - You can only multiply *some* matrices together (they must be **conformable**)
  - And matrix division isn't really a thing. Instead, we multiply by the matrix's **inverse**.


---

class: center, middle

## Matrix Multiplication

---

## Matrix Multiplication

--

First, let's introduce the **dot product** of two vectors.

$$a \cdot b = \sum a_i b_i$$
--

If $a = [3,1,2]$ and $b = [1,2,3]$, then the dot product of $a$ and $b$ equals:

$$a \cdot b =  3 \times 1 + 1 \times 2 + 2 \times 3 = 11$$
--

In `R`, a dot product can be computed like so:

```{r dot product}
A <- c(3,1,2)
B <- c(1,2,3)

# dot product
sum(A*B)
```

---

## Matrix Multiplication

**Exercise:** Take the dot product of $a$ and $b$.

$a = [1,4,5]$ and $b = [3,2,1]$

--

**Answer:**

$$a \cdot b =  1 \times 3 + 4 \times 2 + 5 \times 1 = 16$$

```{r dot product 2}
A <- c(1,4,5)
B <- c(3,2,1)

# dot product
sum(A*B)
```

---

## Matrix Multiplication

When you multiply two matrices, you take a series of dot products.

.pull-left[
$A = \begin{bmatrix} 1 & 2 \\ 2 & 3 \end{bmatrix}$
]

.pull-right[
$B = \begin{bmatrix} 2 & 1 \\ 4 & 4 \end{bmatrix}$
]

To get the entry in the first row, first column of $AB$, take the dot product of:

- The first row of $A$, and
- The first column of $B$

--

Then do that for every row in $A$ and every column in $B$.

--

$$AB = \begin{bmatrix} 1 \times 2 + 2 \times 4 & 1 \times 1 + 2 \times 4 \\ 2 \times 2 + 3 \times 4 & 2 \times 1 + 3 \times 4 \end{bmatrix} = \begin{bmatrix} 10 & 9 \\ 16 & 14 \end{bmatrix}$$

--

This is all very strange and confusing to get used to if you've never seen it before, but we'll soon see that it makes representing our multivariable linear regression problem a whole lot easier.

---

## Matrix Multiplication

You can multiply matrices in `R` with the `%*%` command.

```{r matrix multiplication}
A <- cbind(c(1,2), c(2,3))
A

B <- cbind(c(2,4), c(1,4))
B
```

--

```{r matrix multiplication 2}
A %*% B
```

---

## Matrix Multiplication

**Exercise:** Try multiplying these two matrices.

.pull-left[
$A = \begin{bmatrix} 4 & 1 \\ 1 & 2 \end{bmatrix}$
]

.pull-right[
$B = \begin{bmatrix} 5 & 5 \\ 2 & 1 \end{bmatrix}$
]

--

**Answer:**

$$AB = \begin{bmatrix} 4 \times 5 + 1 \times 2 & 4 \times 5 + 1 \times 1 \\ 1 \times 5 + 2 \times 2 & 5 \times 1 + 1 \times 2 \end{bmatrix} = \begin{bmatrix} 22 & 21 \\ 9 & 7 \end{bmatrix}$$
```{r matrix multiplication exercise}
A <- cbind(c(4,1), c(1,2))
B <- cbind(c(5,2), c(5,1))
A %*% B
```

---

## Matrix Multiplication

This process -- taking the dot product of rows and columns -- means that you can only multiply two matrices $AB$ if the row vectors of $A$ are the same length as the column vectors in $B$.

--

- In other words, you can only multiply $AB$ if the dimension of $A$ is $m \times k$ and the dimension of $B$ is $k \times n$.
- If this condition holds, then the two matrices are **conformable**.

Multiplying an $m \times k$ matrix with a $k \times n$ matrix yields an $m \times n$ matrix.

--

.pull-left[
```{r conformable matrices}
A <- cbind(c(3,1,2), c(2,2,2))
A
```
]

.pull-right[
```{r conformable matrices 2}
B <- cbind(c(4,5), c(5,4), c(1,1))
B
```
]

**Exercise**: Which can you multiply: $AB$ or $BA$?

---

## Matrix Multiplication

**Answer:** Both!

```{r conformable or non-conformable}
A %*% B

B %*% A
```

--

But now try `A %*% A`. I can't do it here because `R` gets so mad it won't even render my slides.


---

## Matrix Multiplication

To make matrices conformable for multiplication, sometimes you may need to take the **transpose** of a matrix. The transpose just takes the rows and turns them into columns.

.pull-left[
$$A = \begin{bmatrix} 4 & 1 \\ 1 & 2 \\ 3 & 3 \end{bmatrix}$$
]

.pull-right[
$$A' = \begin{bmatrix} 4 & 1 & 3 \\ 1 & 2 & 3 \end{bmatrix}$$
]

```{r transpose}
t(A)

t(A) %*% A
```

---

## Matrix Multiplication

Multiplying a vector by its transpose is the same as taking the dot product with itself:

$$a = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}$$

$$a \cdot a = a'a = 1 \times 1 + 3 \times 3 + 4 \times 4 = 26$$
--

Hey, it's the **sum of squares**! That could be useful for something...

--

```{r transpose multiplication}
a <- c(1,3,4)

sum(a*a)

t(a) %*% a
```

---

class: center, middle

## Matrix Inversion

---

## Matrix Inversion

Before I can teach you how to **divide** matrices, I need to tell you about a very special matrix, called the **identity matrix**.

--

Remember how any number times 1 just equals the original number?

$$a \times 1 = a$$
This is called the **identity property**. It's what makes $1$ a very special number.

--

<br>

The **identity matrix** $(I)$ is basically the $1$ of matrices.

$$AI = A$$

You multiply any matrix by $I$ and you get the same matrix back.


---

## Matrix Inversion

The identity matrix $I_n$ is an $n \times n$ matrix with ones in the diagonal and zeroes everywhere else.

$$I_3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$$

--

**Exercise:** Try multiplying this matrix with the following matrix $A$.

$$A = \begin{bmatrix} 2 & 1 & 5 \\ -2 & 8 & 100 \\ 7 & 42 & -42 \end{bmatrix}$$

---

## Matrix Inversion

**Answer:**

```{r identity matrix}
diag(3) # Create the identity matrix in R with the `diag()` function

A <- rbind(c(2, 1, 5),
           c(-2, 8, 100),
           c(7, 42, -2))

# multiply AI
A %*% diag(3)
```


---

## Matrix Inversion

Hey, remember how dividing $\frac{a}{b}$ is the same as multiplying $a \times \frac{1}{b}$?

--

$\frac{1}{b} = b^{-1}$ is called the **inverse** (or reciprocal) of $b$. 

--

**Exercise:** What do you get when you multiply a number by its inverse?

--

- **Answer:** $a \times \frac{1}{a} = a^1a^{-1} = a^0 = 1$

--

There is an equivalent concept in matrix algebra, called the **matrix inverse**.

$$AA^{-1} = I$$
---

## Matrix Inversion

Good news! It's the 21st century. No one is going to make you solve for matrix inverses by hand.

--

There is literally a function in R called `solve()` which will do it for you.

```{r solve}
solve(A)

A %*% solve(A)
```

---

## Matrix Inversion

Now that we know how to multiply by an inverse, we have what we need to perform matrix algebra.

**Exercise:** Solve this equation for $A$.

$$AB = C$$

--

**Answer:** Multiply both sides by $B^{-1}$

$$ABB^{-1} = CB^{-1}$$

$$AI = CB^{-1}$$

<br>

$$A = CB^{-1}$$
---

## Matrix Inversion

Watch out for conformability! With matrices, it matters whether you multiply on the right or the left.

**Exercise:** Solve for $B$.

$$AB = C$$
--

**Answer:**

$$A^{-1}AB = A^{-1}C$$

$$IB = A^{-1}C$$
<br>

$$B = A^{-1}C$$
--

$$B \neq CA^{-1}$$


---

class: center, middle

## Back to Multivariable Regression


---

## Multivariable Regression

This is the regression problem we wanted to solve:

$$\underbrace{\begin{bmatrix} 21.0 \\ 21.0 \\ 22.8 \\ \vdots \\ 21.4 \end{bmatrix}}_\text{mpg} = \underbrace{\alpha \times \begin{bmatrix} 1 \\ 1 \\1 \\ \vdots \\ 1 \end{bmatrix}}_\alpha + \underbrace{\beta_1 \times \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}}_{\beta_1 \text{hp}} + \underbrace{\beta_2 \times \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}}_{\beta_2 \text{wt}} + \underbrace{\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \vdots \\ \varepsilon_n \end{bmatrix}}_\varepsilon$$

--

Notice that we can restate it as a matrix multiplication problem:

$$\underbrace{\begin{bmatrix} 21.0 \\ 21.0 \\ 22.8 \\ \vdots \\ 21.4 \end{bmatrix}}_\text{mpg} = \underbrace{\begin{bmatrix} 1 & 110 & 2.62 \\ 1 & 110 & 2.875 \\ 1 & 93 & 2.32 \\ \vdots & \vdots & \vdots \\ 1 & 109 & 2.78 \end{bmatrix}}_X \underbrace{\begin{bmatrix} \alpha \\ \beta_1 \\ \beta_2 \end{bmatrix}}_\beta + \underbrace{\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \vdots \\ \varepsilon_n \end{bmatrix}}_\varepsilon = X\beta + \varepsilon$$

???

How do you simultaneously estiamte $\alpha$, $\beta_1$, and $\beta_2$?

---

## Multivariable Regression

$X\beta$ is an $n \times 1$ vector of predicted values, and $\varepsilon$ is an $n \times 1$ vector of errors.

$$Y = X\beta + \varepsilon$$

--

Just like before, we want to minimize the sum of squared errors:

$$\varepsilon \cdot \varepsilon = \varepsilon'\varepsilon = (Y - X\beta)'(Y-X\beta)$$
--

Minimizing this expression follows the same three steps we used with scalar calculus. Just be careful with the multiplication and division. Start by distributing the function:

$$f(X,Y,\beta) = (Y - X\beta)'(Y-X\beta) = Y'Y - 2(X\beta)'Y + (X\beta)'X\beta$$
---

## Estimating The Regression Parameters

**Step 1: Take the derivative with respect to $\beta$**

$$f(X,Y,\beta) = Y'Y - 2(X\beta)'Y + (X\beta)'X\beta$$

$$\frac{\partial f}{\partial \beta} = -2X'Y + 2 X'X \beta$$ 

--

**Step 2: Set the derivative equal to zero**

$$-2X'Y + 2 X'X \beta = 0$$
--

**Step 3: Solve for $\beta$**

$$2 X'X \beta = 2 X'Y$$
$$\hat{\beta} = (X'X)^{-1} X'Y$$ 
???

That's it! The holy grail!

---

## Estimate The Multivariable Regression

```{r estimate multivariable regression}
# create the Y vector
Y <- mtcars$mpg

# create the X matrix
X <- mtcars %>% 
  select(hp, wt) %>% 
  mutate(intercept = 1) %>% 
  as.matrix

head(X)
```

---

## Estimate The Multivariable Regression

The vector of estimates that minimizes the sum of squared errors equals $(X'X)^{-1}(X'Y)$:

```{r estimate multivariable regression part 2}
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y

beta_hat
```

--

```{r multivariable lm}
lm_fit <- lm(mpg ~ hp + wt, data = mtcars)
coef(lm_fit)
```

---

## What's Going On Here?

Previously, we showed that the line of best fit between `hp` and `mpg` had this slope:

```{r previous slope}
bivariate_fit <- lm(mpg ~ hp, data = mtcars)
coef(bivariate_fit)
```

Now the slope is this half that...

```{r multivariate slope}
multivariate_fit <- lm(mpg ~ hp + wt, data = mtcars)
coef(multivariate_fit)
```


---

## What's Going On Here?

```{r plotly, echo = FALSE, out.height='80%'}
library(plotly)

hp <- mtcars$hp
wt <- mtcars$wt
mpg <- mtcars$mpg

scatter3d <- plot_ly(x = hp, y = wt, z = mpg, type = 'scatter3d', mode = 'markers',
                     marker = list(size = 5, color = "black", symbol = 104)) %>% 
  layout(scene = list(
      xaxis = list(title = "Horsepower (hp)"),
      yaxis = list(title = "Weight (wt)"),
      zaxis = list(title = "Fuel Efficiency (mpg)")))

scatter3d
```

???

Notice that hp and wt are correlated with each other, so the bivariate slope captures both the effect of hp and the effect of wt.

---

## What's Going On Here?

<!-- `r multivariate_fit$coef[1] %>% round(2)` + `r multivariate_fit$coef[2] %>% round(2)` $\times$ `hp` + `r multivariate_fit$coef[3] %>% round(2)` $\times$ `wt` -->

```{r plotly with plane, echo = FALSE, out.height='80%'}

x <- 0:round(max(hp))
y <- 0:round(max(wt)+1)

zhat <- function(x,y){
  coef(multivariate_fit)[1] + coef(multivariate_fit)[2] * x + coef(multivariate_fit)[3] * y
}

plane_of_best_fit <- outer(x,y,zhat)
colnames(plane_of_best_fit) <- y
rownames(plane_of_best_fit) <- x


p <- plot_ly(z = plane_of_best_fit, type = "surface") %>% 
  layout(scene = list(
      xaxis = list(title = "Weight (wt)"),
      yaxis = list(title = "Horsepower (hp)"),
      zaxis = list(title = "Fuel Efficiency (mpg)"))) %>% 
  add_trace(x = wt, y = hp, z = mpg, type = 'scatter3d', mode = 'markers',
            marker = list(size = 5, color = "black", symbol = 104))

p
```

???

Instead of a "line of best fit", we're now fitting a "plane of best fit".


---

# Exercise

Estimate a linear regression model from the `iris` dataset using Petal Length as the outcome variable and Sepal Width, Sepal Length, and Petal Width as the explanatory variables.

1. How does the coefficient on Sepal Width change from our previous bivariate regression with Petal Length and Petal Width alone? Why?

2. Conduct a t-test to see if the versicolor petals are significantly longer than setosa petals.

3. Try `lm(Petal.Length ~ Species, data = iris)`. Notice anything familiar?

???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)