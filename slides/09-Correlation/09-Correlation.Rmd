---
title: "Correlation (Part 1 of 2)"
# author: "Joseph T. Ornstein"
# date: "June 12, 2020"
output:
  xaringan::moon_reader:
    nature:
      highlightStyle: github
      countIncrementalSlides: false
# subtitle: 'POLS 7012: Introduction to Political Methodology'
# institute: University of Georgia
---

<style>

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

</style>

```{r Setup, include=FALSE}
library(tidyverse)
theme_set(theme_minimal(base_size = 16))
knitr::opts_chunk$set(comment=NA, fig.width=7, fig.height=5, 
                      fig.align = 'center', out.width = 600,
                      message=FALSE, warning=FALSE, echo=TRUE)
set.seed(42)
```


<!-- ## Recap -->

<!-- -- -->

<!-- #### Week 1: Introduction to `R` -->

<!-- -- -->

<!-- #### Week 2: Visualizing Data -->

<!-- -- -->

<!-- #### Week 3: Reproducible Research -->

<!-- -- -->

<!-- #### Weeks 4-5: Data Wrangling -->

<!-- -- -->

<!-- #### Week 6: Calculus -->

<!-- -- -->

<!-- #### Weeks 7-8: Probability and Inference -->

<!-- --- -->

## Preview

--

#### October 21 & 28: Correlation

- Covariance and Linear Regression
- Matrix Algebra

--

#### November 4: Prediction

- Fitting Models and Machine Learning
- Cross-Validation, Regularization, and Ensembles

--

#### November 11 & 18: Causation

- Experimental Data
- Observational Causal Inference

--

#### November 25: Thanksgiving

--

#### December 2 & 9: Bonus Weeks!

- *Possible Topics*: Big Data, Text-As-Data, Networks, Spatial/Geographic Data, Advanced `R`, Advanced Visualizations (Interactives/Animations)

---

## Correlation

By the end of this module you will be able to...

--

1. Compute covariance and correlation coefficients.

--

2. Estimate the slope of a line of best fit, plus confidence intervals and p-values.

--

3. Fit multivariable linear models using matrix algebra.


???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)

---

class: center, middle

## Covariance and Correlation

---

## Covariance

Recall that the **variance** of a random variable is its expected squared distance from the mean:

$$\text{Var}(X) = E[(X-E(X))^2]$$

--

The **covariance** extends that definition of variance to two random variables $X$ and $Y$:

$$\text{Cov}(X,Y) = E[(X-E(X))(Y-E(Y))]$$
--

Covariance captures the degree of association between two variables. Does $X$ tend to be high when $Y$ is high?

--

Note that:

$$\text{Cov}(X,X) = \text{Var}(X)$$

---

## Covariance

```{r covariance, out.width='65%'}
flower_plot <- ggplot(data = iris) + 
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length)) +
  labs(x = 'Sepal Length', y = 'Petal Length', title = 'Flower Measurements')
flower_plot
```

---

## Covariance

```{r flower plot 2, out.width='65%'}
flower_plot <- flower_plot +
  geom_vline(xintercept = mean(iris$Sepal.Length), linetype = 'dashed') +
  geom_hline(yintercept = mean(iris$Petal.Length), linetype = 'dashed')
flower_plot
```

???

Notice that whenever $X$ is greater than its mean, $Y$ tends to be greater than its mean, and vice versa. 

When two variables $X$ and $Y$ **covary** with one another, $X$ tends to be high whenever $Y$ is high, and $X$ tends to be low whenever $Y$ is low.

---

## Covariance

Because petal length tends to be larger than average whenever sepal length is larger than average (and vice versa) when you take the mean of all the the $(X-\bar{X})(Y-\bar{Y})$, you get a positive number.

```{r covariance computation}
cov(iris$Sepal.Length, iris$Petal.Length)
```

When covariance is positive, $X$ and $Y$ tend to move together. When covariance is negative, $X$ and $Y$ tend to move in opposite directions.

---

## Correlation Coefficients

The problem with covariance is that it's not easily interpretable. What does a covariance of `r cov(iris$Sepal.Length, iris$Petal.Length)` mean? How strong is that relationship?

--

The **correlation** coefficient solves that problem by standardizing the covariance.

$$\text{Cor}(X,Y) = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$$
--

This yields a value between $-1$ (perfectly anti-correlated) and $+1$ (perfectly correlated).

```{r correlation coefficient}
cor(iris$Sepal.Length, iris$Petal.Length)

cov(iris$Sepal.Length, iris$Petal.Length) / sd(iris$Sepal.Length) / sd(iris$Petal.Length)
```

???

Kind of like how standard deviation standardizes the variance by taking the square root.

---

## Let's Play "Guess The Correlation"

```{r game 1, echo = FALSE}
n <- 100
X <- rnorm(n,0,1)
Y <- X + rnorm(n,0,4)

ggplot(tibble(X,Y)) + 
  geom_point(aes(x=X,y=Y))
```

--

Actual Correlation: `r cor(X,Y)`

---

## Let's Play "Guess The Correlation"

```{r game 2, echo = FALSE}
n <- 100
X <- rnorm(n,0,1)
Y <- -X + rnorm(n,0,1)

ggplot(tibble(X,Y)) + 
  geom_point(aes(x=X,y=Y))
```

--

Actual Correlation: `r cor(X,Y)`

---

## Let's Play "Guess The Correlation"

```{r game 3, echo = FALSE}
n <- 100
X <- rnorm(n,0,1)
Y <- X + rnorm(n,0,2)

ggplot(tibble(X,Y)) + 
  geom_point(aes(x=X,y=Y))
```

--

Actual Correlation: `r cor(X,Y)`

---

## Let's Play "Guess The Correlation"

For more fun, try [http://guessthecorrelation.com/](http://guessthecorrelation.com/)

![](img/guess-the-correlation.png)

---

class: center, middle

# Linear Regression

---

## Linear Regression

```{r set up plots, echo = FALSE}
n <- 100
data <- tibble(X = rnorm(n,0,1),
               epsilon = rnorm(n,0,2),
               Y1 = 0.75*X + rnorm(n,0,0.75),
               Y2 = 3*X + rnorm(n,0,2.55))
```

Correlation coefficients are nice, but limited. Both pairs of variables below have the same correlation coefficient (`r cor(data$X, data$Y1) %>% round(3)` and `r cor(data$X, data$Y2) %>% round(3)`).

.pull-left[
```{r cor1, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y1)) +
  scale_y_continuous(limits = c(-10,10))
```
]

.pull-right[
```{r cor2, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y2)) +
  scale_y_continuous(limits = c(-10,10))
```
]

--

We want to find the **slope** of the relationship (the "line of best fit").

  - When we increase $X$ by 1, how much does $Y$ increase or decrease, on average?
  
???

  - We would also like to perform hypothesis tests and compute confidence intervals.

---

## Linear Regression

Correlation coefficients are nice, but limited. Both pairs of variables below have the same correlation coefficient (`r cor(data$X, data$Y1) %>% round(3)` and `r cor(data$X, data$Y2) %>% round(3)`).

.pull-left[
```{r cor3, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y1)) +
  scale_y_continuous(limits = c(-10,10)) +
  geom_smooth(aes(x=X,y=Y1), method = 'lm', se = FALSE)
```
]

.pull-right[
```{r cor4, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y2)) +
  scale_y_continuous(limits = c(-10,10)) +
  geom_smooth(aes(x=X,y=Y2), method = 'lm', se = FALSE)
```
]

We want to find the **slope** of the relationship (the "line of best fit").

  - When we increase $X$ by 1, how much does $Y$ increase or decrease, on average?

---

## Linear Regression

The two-variable linear model looks like this:

$$Y = a + bX + \varepsilon$$

--

**Terms:**

- $Y$ is a **vector** of outcomes

- $X$ is a **vector** we're using to predict the outcome

- $a$ is the y-intercept 

- $b$ is the slope of the relationship between $X$ and $Y$, and

- $\varepsilon$ is **vector** of random error

  - The difference between the true value of $Y$ and the predicted value $a + bX$.

---

## Linear Regression

$$Y = a + bX + \varepsilon$$

#### Example:

.pull-left[
$X = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}$
]

.pull-right[
$Y = \begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}$
]

<br>

$a = 2$, $b = 2$

--

$$\underbrace{\begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}}_Y = \underbrace{2 \times \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}}_a + \underbrace{2 \times \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}}_{bX} + \underbrace{\begin{bmatrix} 0 \\ -2 \\ 0 \end{bmatrix}}_\varepsilon$$

---

## The Line of Best Fit

The "line of best fit" is the one that minimizes error (specifically, the sum of squared errors).

```{r iris line of best fit, out.width='65%'}
ggplot(data = iris) +
  geom_point(mapping = aes(x = Petal.Width, y = Petal.Length)) + 
  geom_smooth(mapping = aes(x= Petal.Width, y = Petal.Length),
              method = 'lm', se = FALSE)
```

---

## Estimating The Line of Best Fit

To make things easier, we will ignore the y-intercept for now.

  - Create new variables called $X$ and $Y$, equal to Petal Width and Petal Length minus their means.

```{r demean}
demeaned_iris <- iris %>% 
  mutate(Y = Petal.Length - mean(Petal.Length), X = Petal.Width - mean(Petal.Width))
```

--

**Exercise**: What is the mean of $X$?

```{r mean x and y}
demeaned_iris$X %>% mean %>% round(4)
```

--

You'll thank me when we start doing the calculus.

---

## Estimating The Line of Best Fit

.pull-left[
```{r non-demeaned plot, out.width = '90%'}
ggplot(iris) +
  geom_point(mapping = aes(x=Petal.Width, y=Petal.Length)) +
  labs(x='X',y='Y',title = 'Original Data') +
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0)
```
]

.pull-right[
```{r demeaned plot, out.width='90%'}
ggplot(demeaned_iris) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Demeaned Data') +
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0)
```
]

--

The data looks the same; we've just shifted it down and to the left.

---

## Estimating The Line of Best Fit

--

The "best" line is the one that minimizes error. Specifically, we're going to find the line that minimizes the **sum of squared errors**.

$$Y = bX + \varepsilon$$
$$\varepsilon = Y - bX$$
--

Let's create a function called $f(b)$ equal to the sum of squared errors:

$$f(b) = \sum \varepsilon_i^2 = \sum(Y_i-bX_i)^2$$
--

Distribute:

$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$

---

## Three Steps to Minimize a Function?

--

#### Step 1: Take the derivative

$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$

$$\frac{\partial f}{\partial b} = -2\sum X_iY_i + 2b \sum X_i^2$$

--

#### Step 2: Set Equal to Zero

$$-2\sum X_iY_i + 2b \sum X_i^2 = 0$$
--

#### Step 3: Solve for $b$

$$2\sum X_iY_i = 2b\sum X_i^2$$
$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$
---

## Slope Estimate

$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$

```{r estimate slope}
slope_estimate <- 
  sum(demeaned_iris$X * demeaned_iris$Y) /
  sum(demeaned_iris$X^2)

slope_estimate
```

---

## Slope Estimate

```{r plot estimated slope}
ggplot(demeaned_iris) + 
  geom_point(mapping = aes(x=X,y=Y)) +
  geom_abline(intercept = 0, slope = slope_estimate)
```

---

## Some Terminology

The slope parameter $b$ is called the **estimand**. It is the thing we are trying to estimate.

$\frac{\sum X_iY_i}{\sum X_i^2}$ is the **estimator**. It is the equation we use to produce our estimate.

`r slope_estimate %>% round(3)` is our **estimate**.

  - Typically, we denote estimates with little hats, like this: $\hat{b} =$ `r slope_estimate %>% round(3)`.

---

## Interesting Footnote

Notice that our estimator $\frac{\sum X_iY_i}{\sum X_i^2}$ is equal to $\frac{\sum (X_i-0)(Y_i-0)}{\sum (X_i-0)^2}$, which is equal to $\frac{\sum (X_i-\bar{X})(Y_i-\bar{Y})}{\sum (X_i-\bar{X})^2}$ because $\bar{X}=0$ and $\bar{Y}=0$.

So another way of writing the estimator is $\hat{b} = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}$.

--

```{r another slope estimate}
slope_estimate

cov(demeaned_iris$X, demeaned_iris$Y) / var(demeaned_iris$X)
```



---

## Residuals

The vector of observed errors $(\hat{\varepsilon})$, also known as the **residuals**, is equal to $Y-\hat{b}X$.

```{r compute errors}
# Compute the epsilon vector
epsilon <- demeaned_iris$Y - slope_estimate * demeaned_iris$X
```

--

If we fit the line correctly, then the average error should equal zero.

```{r mean epsilon}
mean(epsilon)
```

---

class: center, middle

## Statistical Inference

---

## Statistical Inference

We now have a **point estimate** of the slope ( $\hat{b} =$ `r slope_estimate %>% round(3)` ). What if we want an **interval estimate** (confidence intervals) and p-values? 

--

### Three Steps

--

1. Specify the Null Hypothesis $(b = 0)$

--

2. Generate Sampling Distribution of $\hat{b}$ assuming $b = 0$

--

3. Compare observed value of $\hat{b}$ to the sampling distribution

---

### Where do we get the sampling distribution?

In a linear regression, the randomness comes from the error term $\varepsilon$. Imagine that we repeatedly draw a new $\varepsilon$ vector with each sample.

--

```{r sampling function}
null_slope_estimate <- function(X, epsilon){
  
  # Randomly sample a vector of epsilons
  epsilon <- sample(epsilon, replace = TRUE)
  
  # null hypothesis: b = 0
  b <- 0  
  
  # create a random dataset assuming the null hypothesis
  Y <- b*X + epsilon
  
  # Return the slope estimate
  sum(X * Y) / sum(X^2)
}

null_slope_estimate(X = demeaned_iris$X, epsilon)
```

---

## Generate the Sampling Distribution

```{r sampling distribution, out.width = '60%'}
sampling_distribution <- replicate(20000, null_slope_estimate(X = demeaned_iris$X, epsilon))

tibble(sampling_distribution) %>% 
  ggplot() +
  geom_histogram(mapping = aes(x=sampling_distribution),
                 color = 'black', binwidth = 0.01) +
  labs(x='Slope Estimate (Sampling Distribution)') # a normally distributed sampling distribution again!
```

---

## P-values

```{r p-value and confidence interval}
sum(sampling_distribution > slope_estimate) # p-value effectively zero
```

## Confidence Intervals

```{r confidence intervals}
standard_error <- sd(sampling_distribution)
standard_error

confidence_interval <- c(slope_estimate - 1.96 * standard_error,
                         slope_estimate + 1.96 * standard_error)
confidence_interval

```

---

## The One-Line Built-In `R` Function

The `lm()` function estimates the linear model parameters (slope + y-intercept) and computes confidence intervals and p-values.

```{r lm}
linear_model_fit <- lm(formula = Y ~ X, 
                       data = demeaned_iris) 

coef(linear_model_fit) # get the coefficient estimates

confint(linear_model_fit) # get the confidence intervals
```

---

## The One-Line Built-In `R` Function

```{r summary(lm)}
summary(linear_model_fit)
```

???

Reports a bunch of other statistics and diagnostics that you will learn about in POLS 7014. I gotta leave something for Mollie to teach you.

---

## Exercise

1. Check out the documentation for the `mtcars` dataset by typing `?mtcars`.

2. What is the mean fuel efficiency of cars (`mpg`), grouped by the number of cylinders (`cyl`)?

3. Do cars with a manual transmission have significantly higher/lower horsepower than those with an automatic transmission?

4. What is the correlation between horsepower (`hp`) and fuel efficiency (`mpg`)? Visualize the relationship.

5. Fit a linear model with horsepower as the predictor variable $(X)$ and fuel efficiency as the outcome variable $(Y)$. What is the slope of the relationship? What is the 95% confidence interval on that slope estimate?

---

## Exercise (Solution)

What is the mean fuel efficiency of cars (`mpg`), grouped by the number of cylinders (`cyl`)?

```{r exercise 1}
mtcars %>% 
  group_by(cyl) %>% 
  summarize(mean_mpg = mean(mpg),
            num = n())
```

---

## Exercise (Solution)

Do cars with a manual transmission have significantly higher/lower horsepower than those with an automatic transmission?

```{r exercise 2}
mtcars %>% 
  t.test(hp ~ am, data = .)
```

---

## Exercise (Solution)

What is the correlation between horsepower (`hp`) and fuel efficiency (`mpg`)?

```{r exercise 3, out.width = '60%'}
ggplot(data = mtcars) +
  geom_point(mapping = aes(x=hp, y=mpg)) +
  labs(x = 'Horsepower', y = 'Fuel Efficiency (Miles Per Gallon)',
       title = paste0('Correlation = ', cor(mtcars$hp, mtcars$mpg) %>% round(3)))
```

---

## Exercise (Solution)

Fit a linear model with horsepower as the predictor variable $(X)$ and fuel efficiency as the outcome variable $(Y)$. What is the slope of the relationship? What is the 95% confidence interval on that slope estimate?

```{r exercise 4}
cars_linear_model <- lm(mpg ~ hp, data = mtcars)

coef(cars_linear_model)

confint(cars_linear_model)
```

---


class: center, middle

# Multivariable Linear Regression

---

## Multivariable Linear Regression

Suppose we want to explain the outcome as a function of **multiple** explanatory variables. 

--

$$\text{mpg} = \alpha + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon$$
Fuel efficiency probably depends on both horsepower **and** weight. More powerful and heavier cars will tend to have lower fuel efficiency. We'd like to estimate the slope of both relationships simultaneously!

--

**Vector Representation**:

$$\underbrace{\begin{bmatrix} 21.0 \\ 21.0 \\ 22.8 \\ \vdots \\ 21.4 \end{bmatrix}}_\text{mpg} = \underbrace{\alpha \times \begin{bmatrix} 1 \\ 1 \\1 \\ \vdots \\ 1 \end{bmatrix}}_\alpha + \underbrace{\beta_1 \times \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}}_{\beta_1 \text{hp}} + \underbrace{\beta_2 \times \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}}_{\beta_2 \text{wt}} + \underbrace{\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \vdots \\ \varepsilon_n \end{bmatrix}}_\varepsilon$$
---

## Multivariable Linear Regression

The challenge is to simultaneously estimate $\alpha$, $\beta_1$, and $\beta_2$.

$$\text{mpg} = \alpha + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon$$
--

We've come as far as we can with scalar algebra. It's time you learned **matrix algebra**.

---

class: center, middle

# Matrix Algebra

---

## Matrix Algebra

Recall from Week 1 that a **matrix** is a bunch of vectors squished together.

--

.pull-left[
$\text{hp} = \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}$
]

.pull-right[
$\text{wt} = \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}$
]

--

<br>

$$X = \begin{bmatrix} 110 & 2.62 \\ 110 & 2.875 \\ 93 & 2.32 \\ \vdots & \vdots \\ 109 & 2.78 \end{bmatrix}$$
???

We've been calling this a **dataframe**.

---

## Matrix Algebra

The **dimension** of a matrix refers to the number of rows and columns. An $m \times n$ matrix has $m$ rows and $n$ columns.

--

```{r matrix dimensions}
dim(mtcars)
```

There are 32 rows and 11 columns in the mtcars data matrix.

---

## Matrix Algebra

**Adding** and **subtracting** matrices is straightforward. Just add and subtract elementwise. 

.pull-left[
$A = \begin{bmatrix} 1 & 2 \\ 2 & 3 \\ 4 & 4 \end{bmatrix}$
]

.pull-right[
$B = \begin{bmatrix} 2 & 1 \\ 4 & 4 \\ 8 & 5 \end{bmatrix}$
]

--

$$A + B = \begin{bmatrix} 3 & 3 \\ 6 & 7 \\ 12 & 9 \end{bmatrix}$$
--

**Multiplying** and **dividing** is where it gets tricky.
  - You can only multiply *some* matrices together (they must be **conformable**)
  - And matrix division isn't really a thing. Instead, we multiply by the matrix's **inverse**.


---

class: center, middle

## Matrix Multiplication

---

## Matrix Multiplication

--

First, let's introduce the **dot product** of two vectors.

$$a \cdot b = \sum a_i b_i$$
--

If $a = [3,1,2]$ and $b = [1,2,3]$, then the dot product of $a$ and $b$ equals:

$$a \cdot b =  3 \times 1 + 1 \times 2 + 2 \times 3 = 11$$
--

In `R`, a dot product can be computed like so:

```{r dot product}
A <- c(3,1,2)
B <- c(1,2,3)

# dot product
sum(A*B)
```

---

## Matrix Multiplication

**Exercise:** Take the dot product of $a$ and $b$.

$a = [1,4,5]$ and $b = [3,2,1]$

--

**Answer:**

$$a \cdot b =  1 \times 3 + 4 \times 2 + 5 \times 1 = 16$$

```{r dot product 2}
A <- c(1,4,5)
B <- c(3,2,1)

# dot product
sum(A*B)
```

---

## Matrix Multiplication

When you multiply two matrices, you take a series of dot products.

.pull-left[
$A = \begin{bmatrix} 1 & 2 \\ 2 & 3 \end{bmatrix}$
]

.pull-right[
$B = \begin{bmatrix} 2 & 1 \\ 4 & 4 \end{bmatrix}$
]

To get the entry in the first row, first column of $AB$, take the dot product of:

- The first row of $A$, and
- The first column of $B$

--

Then do that for every row in $A$ and every column in $B$.

--

$$AB = \begin{bmatrix} 1 \times 2 + 2 \times 4 & 1 \times 1 + 2 \times 4 \\ 2 \times 2 + 3 \times 4 & 2 \times 1 + 3 \times 4 \end{bmatrix} = \begin{bmatrix} 10 & 9 \\ 16 & 14 \end{bmatrix}$$

--

This is all very strange and confusing to get used to if you've never seen it before, but we'll soon see that it makes representing our multivariable linear regression problem a whole lot easier.

---

## Matrix Multiplication

You can multiply matrices in `R` with the `%*%` command.

```{r matrix multiplication}
A <- cbind(c(1,2), c(2,3))
A

B <- cbind(c(2,4), c(1,4))
B
```

--

```{r matrix multiplication 2}
A %*% B
```

---

## Matrix Multiplication

**Exercise:** Try multiplying these two matrices.

.pull-left[
$A = \begin{bmatrix} 4 & 1 \\ 1 & 2 \end{bmatrix}$
]

.pull-right[
$B = \begin{bmatrix} 5 & 5 \\ 2 & 1 \end{bmatrix}$
]

--

**Answer:**

$$AB = \begin{bmatrix} 4 \times 5 + 1 \times 2 & 4 \times 5 + 1 \times 1 \\ 1 \times 5 + 2 \times 2 & 5 \times 1 + 1 \times 2 \end{bmatrix} = \begin{bmatrix} 22 & 21 \\ 9 & 7 \end{bmatrix}$$
```{r matrix multiplication exercise}
A <- cbind(c(4,1), c(1,2))
B <- cbind(c(5,2), c(5,1))
A %*% B
```

---

## Matrix Multiplication

This process -- taking the dot product of rows and columns -- means that you can only multiply two matrices $AB$ if the row vectors of $A$ are the same length as the column vectors in $B$.

--

- In other words, you can only multiply $AB$ if the dimension of $A$ is $m \times k$ and the dimension of $B$ is $k \times n$.
- If this condition holds, then the two matrices are **conformable**.

Multiplying an $m \times k$ matrix with a $k \times n$ matrix yields an $m \times n$ matrix.

--

.pull-left[
```{r conformable matrices}
A <- cbind(c(3,1,2), c(2,2,2))
A
```
]

.pull-right[
```{r conformable matrices 2}
B <- cbind(c(4,5), c(5,4), c(1,1))
B
```
]

**Exercise**: Which can you multiply: $AB$ or $BA$?

---

## Matrix Multiplication

**Answer:**

```{r conformable or non-conformable}
A %*% B

B %*% A
```

--

But now try `A %*% A`. I can't do it here because `R` gets so mad it won't even render my slides.


---

## Matrix Multiplication

To make matrices conformable for multiplication, sometimes you may need to take the **transpose** of a matrix. The transpose just takes the rows and turns them into columns.

.pull-left[
$$A = \begin{bmatrix} 4 & 1 \\ 1 & 2 \\ 3 & 3 \end{bmatrix}$$
]

.pull-right[
$$A' = \begin{bmatrix} 4 & 1 & 3 \\ 1 & 2 & 3 \end{bmatrix}$$
]

```{r transpose}
t(A)

t(A) %*% A
```

---

### Here's a useful thing to remember in about 10 slides... 

Multiplying a vector by its transpose is the same as taking the dot product with itself:

$$a = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}$$

$$a \cdot a = a'a = 1 \times 1 + 3 \times 3 + 4 \times 4 = 26$$
--

Hey, it's the **sum of squares**! Could be useful for something...

--

```{r transpose multiplication}
a <- c(1,3,4)

sum(a*a)

t(a) %*% a
```

---

class: center, middle

## Matrix Inversion

---

## Matrix Inversion




---

class: center, middle

## Back to Multivariable Regression


---

## Multivariable Regression

This is the regression problem we wanted to solve:

$$\underbrace{\begin{bmatrix} 21.0 \\ 21.0 \\ 22.8 \\ \vdots \\ 21.4 \end{bmatrix}}_\text{mpg} = \underbrace{\alpha \times \begin{bmatrix} 1 \\ 1 \\1 \\ \vdots \\ 1 \end{bmatrix}}_\alpha + \underbrace{\beta_1 \times \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}}_{\beta_1 \text{hp}} + \underbrace{\beta_2 \times \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}}_{\beta_2 \text{wt}} + \underbrace{\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \vdots \\ \varepsilon_n \end{bmatrix}}_\varepsilon$$

--

Notice that we can restate it as a matrix multiplication problem:

$$\underbrace{\begin{bmatrix} 21.0 \\ 21.0 \\ 22.8 \\ \vdots \\ 21.4 \end{bmatrix}}_\text{mpg} = \underbrace{\begin{bmatrix} 1 & 110 & 2.62 \\ 1 & 110 & 2.875 \\ 1 & 93 & 2.32 \\ \vdots & \vdots & \vdots \\ 1 & 109 & 2.78 \end{bmatrix}}_X \underbrace{\begin{bmatrix} \alpha \\ \beta_1 \\ \beta_2 \end{bmatrix}}_\beta + \underbrace{\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \vdots \\ \varepsilon_n \end{bmatrix}}_\varepsilon = X\beta + \varepsilon$$

???

How do you simultaneously estiamte $\alpha$, $\beta_1$, and $\beta_2$?

---

## Multivariable Regression

$X\beta$ is an $n \times 1$ vector of predicted values, and $\varepsilon$ is an $n \times 1$ vector of errors.

$$Y = X\beta + \varepsilon$$

--

Just like before, we want to minimize the sum of squared errors:

$$\varepsilon \cdot \varepsilon = \varepsilon'\varepsilon = (Y - X\beta)'(Y-X\beta)$$
--

Minimizing this expression follows the same three steps we used with scalar calculus. Just be careful with the multiplication and division. Start by distributing the function:

$$f(X,Y,\beta) = (Y - X\beta)'(Y-X\beta) = Y'Y - 2(X\beta)'Y + (X\beta)'X\beta$$
---

## Estimating The Regression Parameters

**Step 1: Take the derivative with respect to $\beta$**

$$f(X,Y,\beta) = Y'Y - 2(X\beta)'Y + (X\beta)'X\beta$$

$$\frac{\partial f}{\partial \beta} = -2X'Y + 2 \beta X'X$$ 

--

**Step 2: Set the derivative equal to zero**

$$-2X'Y - 2 \beta X'X = 0$$
--

**Step 3: Solve for $\beta$**

$$2\beta X'X = 2 X'Y$$
$$\hat{\beta} = (X'X)^{-1} X'Y$$ 
???

That's it! The holy grail!

---

## Estimate The Multivariable Regression

```{r estimate multivariable regression}
# create the Y vector
Y <- mtcars$mpg

# create the X matrix
X <- mtcars %>% 
  select(hp, wt) %>% 
  mutate(intercept = 1) %>% 
  as.matrix

head(X)
```

---

## Estimate The Multivariable Regression

The vector of estimates that minimizes the sum of squared errors equals $(X'X)^{-1}(X'Y)$:

```{r estimate multivariable regression part 2}
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y

beta_hat
```

--

```{r multivariable lm}
lm_fit <- lm(mpg ~ hp + wt, data = mtcars)
coef(lm_fit)
```

---

## What's Going On Here?

Previously, we showed that the line of best fit between `hp` and `mpg` had this slope:

```{r previous slope}
bivariate_fit <- lm(mpg ~ hp, data = mtcars)
coef(bivariate_fit)
```

Now the slope is this half that...

```{r multivariate slope}
multivariate_fit <- lm(mpg ~ hp + wt, data = mtcars)
coef(multivariate_fit)
```


---

## What's Going On Here?

```{r plotly, echo = FALSE, out.height='80%'}
library(plotly)

hp <- mtcars$hp
wt <- mtcars$wt
mpg <- mtcars$mpg

scatter3d <- plot_ly(x = hp, y = wt, z = mpg, type = 'scatter3d', mode = 'markers',
                     marker = list(size = 5, color = "black", symbol = 104)) %>% 
  layout(scene = list(
      xaxis = list(title = "Horsepower (hp)"),
      yaxis = list(title = "Weight (wt)"),
      zaxis = list(title = "Fuel Efficiency (mpg)")))

scatter3d
```

???

Notice that hp and wt are correlated with each other, so the bivariate slope captures both the effect of hp and the effect of wt.

---

## What's Going On Here?

<!-- `r multivariate_fit$coef[1] %>% round(2)` + `r multivariate_fit$coef[2] %>% round(2)` $\times$ `hp` + `r multivariate_fit$coef[3] %>% round(2)` $\times$ `wt` -->

```{r plotly with plane, echo = FALSE, out.height='80%'}

x <- 0:round(max(hp))
y <- 0:round(max(wt)+1)

zhat <- function(x,y){
  coef(multivariate_fit)[1] + coef(multivariate_fit)[2] * x + coef(multivariate_fit)[3] * y
}

plane_of_best_fit <- outer(x,y,zhat)
colnames(plane_of_best_fit) <- y
rownames(plane_of_best_fit) <- x


p <- plot_ly(z = plane_of_best_fit, type = "surface") %>% 
  layout(scene = list(
      xaxis = list(title = "Weight (wt)"),
      yaxis = list(title = "Horsepower (hp)"),
      zaxis = list(title = "Fuel Efficiency (mpg)"))) %>% 
  add_trace(x = wt, y = hp, z = mpg, type = 'scatter3d', mode = 'markers',
            marker = list(size = 5, color = "black", symbol = 104))

p
```

???

Instead of a "line of best fit", we're now fitting a "plane of best fit".


---

# Exercise



???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)