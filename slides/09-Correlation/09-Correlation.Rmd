---
title: "Correlation (Part 1 of 2)"
# author: "Joseph T. Ornstein"
# date: "June 12, 2020"
output:
  xaringan::moon_reader:
    nature:
      highlightStyle: github
      countIncrementalSlides: false
# subtitle: 'POLS 7012: Introduction to Political Methodology'
# institute: University of Georgia
---

<style>

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

</style>

```{r Setup, include=FALSE}
library(tidyverse)
theme_set(theme_minimal(base_size = 16))
knitr::opts_chunk$set(comment=NA, fig.width=7, fig.height=5, 
                      fig.align = 'center', out.width = 600,
                      message=FALSE, warning=FALSE, echo=TRUE)
set.seed(42)
```


<!-- ## Recap -->

<!-- -- -->

<!-- #### Week 1: Introduction to `R` -->

<!-- -- -->

<!-- #### Week 2: Visualizing Data -->

<!-- -- -->

<!-- #### Week 3: Reproducible Research -->

<!-- -- -->

<!-- #### Weeks 4-5: Data Wrangling -->

<!-- -- -->

<!-- #### Week 6: Calculus -->

<!-- -- -->

<!-- #### Weeks 7-8: Probability and Inference -->

<!-- --- -->

## Preview

--

#### October 21 & 28: Correlation

- Covariance and Linear Regression
- Matrix Algebra

--

#### November 4: Prediction

- Models and Machine Learning
- Overfitting, Cross-Validation, Regularization, and Ensembles

--

#### November 11 & 18: Causation

- Analyzing Experiments
- Observational Causal Inference

--

#### November 25: Thanksgiving

--

#### December 2 & 9: Bonus Weeks!

- *Possible Topics*: Big Data, Text-As-Data, Networks, Spatial/Geographic Data, Advanced `R`, Advanced Visualizations (Interactives/Animations)

---

## Correlation

By the end of this module you will be able to...

--

1. Compute covariance and correlation coefficients between two variables.

--

2. Estimate the slope of a line of best fit, plus confidence intervals and p-values.

--

3. Fit multivariable linear models using matrix algebra.


???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)

---

## Bivariate Regression

```{r bivariate regression}

# Create some data where we know the true data-generating process
n <- 300
a <- 2
b <- 3

data <- tibble(X = rnorm(n, 1, 1),
               epsilon = rnorm(n, 0, 2),
               Y = a + b*X + epsilon)

data %>% 
  select(X,Y) %>% 
  cor

ggplot(data) +
  geom_point(mapping = aes(x=X,y=Y))
```
---

### To make things easier, ignore the y-intercept

```{r demean}
demeaned_data <- data %>% 
  mutate(Y = Y - mean(Y),
         X = X - mean(X))
```

--

.pull-left[
```{r non-demeaned plot, out.width = '90%'}
ggplot(data) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Original Data')
```
]

.pull-right[
```{r demeaned plot, out.width='90%'}
ggplot(demeaned_data) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Demeaned Data')
```
]


---

### What's the right slope?

Do calculus; beta hat = cov(x,y) / var(x)

---

### Slope Estimate

```{r estimate slope}
slope_estimate <- 
  sum(demeaned_data$X * demeaned_data$Y) /
  sum(demeaned_data$X^2)

slope_estimate

linear_model_fit <- lm(Y~X, data = demeaned_data) 
linear_model_fit # hello

ggplot(demeaned_data) + 
  geom_point(mapping = aes(x=X,y=Y)) +
  geom_abline(intercept = 0, slope = slope_estimate)
```

---

## Some Terminology

The slope parameter $b$ is called the **estimand**. It is the thing we are trying to estimate.

$\frac{\sum X_iY_i}{\sum X_i^2}$ is the **estimator**. It is the equation we use to produce our estimate.

`r slope_estimate %>% round(3)` is our **estimate**.

  - Typically, we denote estimates with little hats, like this: $\hat{b} =$ `r slope_estimate %>% round(3)`.

---

## Inference

We have a **point estimate** of the slope. What if we want confidence intervals and p-values? 

**Three Steps:**

--

1. Specify the Null Hypothesis $(b = 0)$

--

2. Generate Sampling Distribution of $\hat{b}$ if $b = 0$

--

3. Compare observed value of $\hat{b}$ to the sampling distribution

---

## Where do we get the sampling distribution?

When we do regression, the randomness comes from the noise term $\varepsilon$. Imagine that we repeatedly draw a new $\varepsilon$ vector with each sample.

```{r sampling function}
get_slope_estimate <- function(a = 0, b = 0, X,
                               sd_epsilon = 2){
  
  n <- length(X)
  
  # Create random dataset assuming null hypothesis (b = 0, a = 0)  
  data <- tibble(X = X,
                 epsilon = rnorm(n, 0, sd_epsilon),
                 Y = a + b*X + epsilon)
  
  # Return the slope estimate
  sum(data$X * data$Y) / sum(data$X^2)
}

get_slope_estimate(X = demeaned_data$X)

```
---

## Generate the Sampling Distribution

<TODO: Bump this to 10000>

```{r sampling distribution, out.width = '70%'}
sampling_distribution <- replicate(500, get_slope_estimate(X = data$X))

tibble(sampling_distribution) %>% 
  ggplot() +
  geom_histogram(mapping = aes(x=sampling_distribution),
                 color = 'black') +
  labs(x='Slope Estimate (Sampling Distribution)') # a bell curved sampling distribution again! (t distribution)
```
---

# P-value and Confidence Interval

```{r p-value and confidence interval}
sum(sampling_distribution > slope_estimate) # p-value effectively zero

standard_error <- sd(sampling_distribution)

confidence_interval <- c(slope_estimate - 1.96 * standard_error,
                         slope_estimate + 1.96 * standard_error)

summary(linear_model_fit)
confint(linear_model_fit)
```

---

class: center, middle

## Multivariate Linear Regression

---

## Multivariate Linear Regression

```{r regression}
data <- tibble(c = 1,
            x1 = rnorm(100, 0, 1),
            x2 = rnorm(100, 0, 1),
            y = 2*x1 + 3*x2 + rnorm(100, 0, 1))

linear_model <- lm(y ~ x1 + x2, data = data)

summary(linear_model)

# X matrix
X <- data %>% 
  select(c, x1, x2) %>% 
  as.matrix

# y vector
y <- data %>% 
  pull(y)

# beta hat = (X'X)^-1 X'y
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y

beta_hat
linear_model # that's where the coefficients come from; the parameter estimates that minimize the sum of squared errors
```