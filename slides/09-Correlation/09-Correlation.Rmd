---
title: "Correlation (Part 1 of 2)"
# author: "Joseph T. Ornstein"
# date: "June 12, 2020"
output:
  xaringan::moon_reader:
    nature:
      highlightStyle: github
      countIncrementalSlides: false
# subtitle: 'POLS 7012: Introduction to Political Methodology'
# institute: University of Georgia
---

<style>

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

</style>

```{r Setup, include=FALSE}
library(tidyverse)
theme_set(theme_minimal(base_size = 16))
knitr::opts_chunk$set(comment=NA, fig.width=7, fig.height=5, 
                      fig.align = 'center', out.width = 600,
                      message=FALSE, warning=FALSE, echo=TRUE)
set.seed(42)
```


<!-- ## Recap -->

<!-- -- -->

<!-- #### Week 1: Introduction to `R` -->

<!-- -- -->

<!-- #### Week 2: Visualizing Data -->

<!-- -- -->

<!-- #### Week 3: Reproducible Research -->

<!-- -- -->

<!-- #### Weeks 4-5: Data Wrangling -->

<!-- -- -->

<!-- #### Week 6: Calculus -->

<!-- -- -->

<!-- #### Weeks 7-8: Probability and Inference -->

<!-- --- -->

## Preview

--

#### October 21 & 28: Correlation

- Covariance and Linear Regression
- Matrix Algebra

--

#### November 4: Prediction

- Fitting Models and Machine Learning
- Cross-Validation, Regularization, and Ensembles

--

#### November 11 & 18: Causation

- Experimental Data
- Observational Causal Inference

--

#### November 25: Thanksgiving

--

#### December 2 & 9: Bonus Weeks!

- *Possible Topics*: Big Data, Text-As-Data, Networks, Spatial/Geographic Data, Advanced `R`, Advanced Visualizations (Interactives/Animations)

---

## Correlation

By the end of this module you will be able to...

--

1. Compute covariance and correlation coefficients.

--

2. Estimate the slope of a line of best fit, plus confidence intervals and p-values.

--

3. Fit multivariable linear models using matrix algebra.


???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)

---

class: center, middle

## Covariance and Correlation

---

## Covariance

Recall that the **variance** of a random variable is its expected squared distance from the mean:

$$\text{Var}(X) = E[(X-E(X))^2]$$

--

The **covariance** extends that definition of variance to two random variables $X$ and $Y$:

$$\text{Cov}(X,Y) = E[(X-E(X))(Y-E(Y))]$$
--

Covariance captures the degree of association between two variables. Does $X$ tend to be high when $Y$ is high?

--

Note that:

$$\text{Cov}(X,X) = \text{Var}(X)$$

---

## Covariance

```{r covariance, out.width='65%'}
flower_plot <- ggplot(data = iris) + 
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length)) +
  labs(x = 'Sepal Length', y = 'Petal Length', title = 'Flower Measurements')
flower_plot
```

---

## Covariance

```{r flower plot 2, out.width='65%'}
flower_plot <- flower_plot +
  geom_vline(xintercept = mean(iris$Sepal.Length), linetype = 'dashed') +
  geom_hline(yintercept = mean(iris$Petal.Length), linetype = 'dashed')
flower_plot
```

???

Notice that whenever $X$ is greater than its mean, $Y$ tends to be greater than its mean, and vice versa. 

When two variables $X$ and $Y$ **covary** with one another, $X$ tends to be high whenever $Y$ is high, and $X$ tends to be low whenever $Y$ is low.

---

## Covariance

Because petal length tends to be larger than average whenever sepal length is larger than average (and vice versa) when you take the mean of all the the $(X-\bar{X})(Y-\bar{Y})$, you get a positive number.

```{r covariance computation}
cov(iris$Sepal.Length, iris$Petal.Length)
```

When covariance is positive, $X$ and $Y$ tend to move together. When covariance is negative, $X$ and $Y$ tend to move in opposite directions.

---

## Correlation Coefficients

The problem with covariance is that it's not easily interpretable. What does a covariance of `r cov(iris$Sepal.Length, iris$Petal.Length)` mean? How strong is that relationship?

--

The **correlation** coefficient solves that problem by standardizing the covariance.

$$\text{Cor}(X,Y) = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$$
--

This yields a value between $-1$ (perfectly anti-correlated) and $+1$ (perfectly correlated).

```{r correlation coefficient}
cor(iris$Sepal.Length, iris$Petal.Length)

cov(iris$Sepal.Length, iris$Petal.Length) / sd(iris$Sepal.Length) / sd(iris$Petal.Length)
```

???

Kind of like how standard deviation standardizes the variance by taking the square root.

---

## Let's Play "Guess The Correlation"

```{r game 1, echo = FALSE}
n <- 100
X <- rnorm(n,0,1)
Y <- X + rnorm(n,0,4)

ggplot(tibble(X,Y)) + 
  geom_point(aes(x=X,y=Y))
```

--

Actual Correlation: `r cor(X,Y)`

---

## Let's Play "Guess The Correlation"

```{r game 2, echo = FALSE}
n <- 100
X <- rnorm(n,0,1)
Y <- -X + rnorm(n,0,1)

ggplot(tibble(X,Y)) + 
  geom_point(aes(x=X,y=Y))
```

--

Actual Correlation: `r cor(X,Y)`

---

## Let's Play "Guess The Correlation"

```{r game 3, echo = FALSE}
n <- 100
X <- rnorm(n,0,1)
Y <- X + rnorm(n,0,2)

ggplot(tibble(X,Y)) + 
  geom_point(aes(x=X,y=Y))
```

--

Actual Correlation: `r cor(X,Y)`

---

## Let's Play "Guess The Correlation"

For more fun, try [http://guessthecorrelation.com/](http://guessthecorrelation.com/)

![](img/guess-the-correlation.png)

---

class: center, middle

# Linear Regression

---

## Linear Regression

```{r set up plots, echo = FALSE}
n <- 100
data <- tibble(X = rnorm(n,0,1),
               epsilon = rnorm(n,0,2),
               Y1 = 0.75*X + rnorm(n,0,0.75),
               Y2 = 3*X + rnorm(n,0,2.55))
```

Correlation coefficients are nice, but limited. Both pairs of variables below have the same correlation coefficient (`r cor(data$X, data$Y1) %>% round(3)` and `r cor(data$X, data$Y2) %>% round(3)`).

.pull-left[
```{r cor1, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y1)) +
  scale_y_continuous(limits = c(-10,10))
```
]

.pull-right[
```{r cor2, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y2)) +
  scale_y_continuous(limits = c(-10,10))
```
]

--

We want to find the **slope** of the relationship (the "line of best fit").

  - When we increase $X$ by 1, how much does $Y$ increase or decrease, on average?
  
???

  - We would also like to perform hypothesis tests and compute confidence intervals.

---

## Linear Regression

Correlation coefficients are nice, but limited. Both pairs of variables below have the same correlation coefficient (`r cor(data$X, data$Y1) %>% round(3)` and `r cor(data$X, data$Y2) %>% round(3)`).

.pull-left[
```{r cor3, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y1)) +
  scale_y_continuous(limits = c(-10,10)) +
  geom_smooth(aes(x=X,y=Y1), method = 'lm', se = FALSE)
```
]

.pull-right[
```{r cor4, echo = FALSE}
ggplot(data) +
  geom_point(aes(x=X,y=Y2)) +
  scale_y_continuous(limits = c(-10,10)) +
  geom_smooth(aes(x=X,y=Y2), method = 'lm', se = FALSE)
```
]

We want to find the **slope** of the relationship (the "line of best fit").

  - When we increase $X$ by 1, how much does $Y$ increase or decrease, on average?

---

## Linear Regression

The two-variable linear model looks like this:

$$Y = a + bX + \varepsilon$$

--

**Terms:**

- $Y$ is a **vector** of outcomes

- $X$ is a **vector** we're using to predict the outcome

- $a$ is the y-intercept 

- $b$ is the slope of the relationship between $X$ and $Y$, and

- $\varepsilon$ is **vector** of random error

  - The difference between the true value of $Y$ and the predicted value $a + bX$.

---

## Linear Regression

$$Y = a + bX + \varepsilon$$

#### Example:

.pull-left[
$X = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}$
]

.pull-right[
$Y = \begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}$
]

<br>

$a = 2$, $b = 2$

--

$$\underbrace{\begin{bmatrix} 4 \\ 6 \\ 10 \end{bmatrix}}_Y = \underbrace{2 \times \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}}_a + \underbrace{2 \times \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}}_{bX} + \underbrace{\begin{bmatrix} 0 \\ -2 \\ 0 \end{bmatrix}}_\varepsilon$$

---

## How do we estimate the line of best fit?

--

To see how, let's first generate some random data.

```{r bivariate regression}
n <- 300 # sample size
a <- 2 # y-intercept
b <- 3 # slope

# X and epsilon are normally distributed
# Y = a + bX + epsilon
data <- tibble(X = rnorm(n, 1, 1), 
               epsilon = rnorm(n, 0, 2),
               Y = a + b*X + epsilon)

cor(data$X, data$Y)
```

---

## How do we estimate the line of best fit?

```{r plot bivariate regression DGP}
ggplot(data) +
  geom_point(mapping = aes(x=X,y=Y))
```

---

## How do we estimate the line of best fit?

To make things easier, we will ignore the y-intercept for now.

```{r demean}
demeaned_data <- data %>% 
  mutate(Y = Y - mean(Y), X = X - mean(X))
```

--

.pull-left[
```{r non-demeaned plot, out.width = '90%'}
ggplot(data) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Original Data')
```
]

.pull-right[
```{r demeaned plot, out.width='90%'}
ggplot(demeaned_data) +
  geom_point(mapping = aes(x=X,y=Y)) +
  labs(x='X',y='Y',title = 'Demeaned Data')
```
]

???

The data looks the same; we've just shifted it down and to the left. You'll thank me when we start doing the calculus.

---

## How do we estimate the line of best fit?

--

The "best" line is the one that minimizes error. Specifically, we're going to find the line that minimizes the **sum of squared errors**.

$$Y = bX + \varepsilon$$
$$\varepsilon = Y - bX$$
--

Let's create a function called $f(b)$ equal to the sum of squared errors:

$$f(b) = \sum \varepsilon_i^2 = \sum(Y_i-bX_i)^2$$
--

Distribute:

$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$

---

## Three Steps to Minimize a Function?

--

#### Step 1: Take the derivative

$$f(b) = \sum Y_i^2 - \sum 2bX_iY_i + \sum b^2 X_i^2$$

$$\frac{\partial f}{\partial b} = -2\sum X_iY_i + 2b \sum X_i^2$$

--

#### Step 2: Set Equal to Zero

$$-2\sum X_iY_i + 2b \sum X_i^2 = 0$$
--

#### Step 3: Solve for $b$

$$2\sum X_iY_i = 2b\sum X_i^2$$
$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$
---

## Slope Estimate

$$b = \frac{\sum X_iY_i}{\sum X_i^2}$$

```{r estimate slope}
slope_estimate <- 
  sum(demeaned_data$X * demeaned_data$Y) /
  sum(demeaned_data$X^2)

slope_estimate
```

---

### Slope Estimate

```{r plot estimated slope}
ggplot(demeaned_data) + 
  geom_point(mapping = aes(x=X,y=Y)) +
  geom_abline(intercept = 0, slope = slope_estimate)
```

---

## Some Terminology

The slope parameter $b$ is called the **estimand**. It is the thing we are trying to estimate.

$\frac{\sum X_iY_i}{\sum X_i^2}$ is the **estimator**. It is the equation we use to produce our estimate.

`r slope_estimate %>% round(3)` is our **estimate**.

  - Typically, we denote estimates with little hats, like this: $\hat{b} =$ `r slope_estimate %>% round(3)`.

---

## Interesting Footnote

Notice that our estimator $\frac{\sum X_iY_i}{\sum X_i^2}$ is equal to $\frac{\sum (X_i-0)(Y_i-0)}{\sum (X_i-0)^2}$, which is equal to $\frac{\sum (X_i-\bar{X})(Y_i-\bar{Y})}{\sum (X_i-\bar{X})^2}$ because $\bar{X}=0$ and $\bar{Y}=0$.

So another way of writing the estimator is $\hat{b} = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}$.

--

```{r another slope estimate}
slope_estimate

cov(demeaned_data$X, demeaned_data$Y) / var(demeaned_data$X)
```

---

## Inference

We have a **point estimate** of the slope ( $\hat{b} =$ `r slope_estimate %>% round(3)` ). What if we want an **interval estimate** (confidence intervals) and p-values? 

--

### Three Steps To Statistical Inference

--

1. Specify the Null Hypothesis $(b = 0)$

--

2. Generate Sampling Distribution of $\hat{b}$ assuming $b = 0$

--

3. Compare observed value of $\hat{b}$ to the sampling distribution

---

### Where do we get the sampling distribution?

In a linear regression, the randomness comes from the error term $\varepsilon$. Imagine that we repeatedly draw a new $\varepsilon$ vector with each sample.

--

```{r sampling function}
null_slope_estimate <- function(X){
  
  b <- 0 # null hypothesis: b = 0 
  n <- length(X)
  
  # Create random dataset assuming null hypothesis
  epsilon <- rnorm(n, 0, 2)
  Y <- b*X + epsilon
  
  # Return the slope estimate
  sum(X * Y) / sum(X^2)
}

null_slope_estimate(X = demeaned_data$X)

```
---

## Generate the Sampling Distribution

```{r sampling distribution, out.width = '60%'}
sampling_distribution <- replicate(20000, null_slope_estimate(X = demeaned_data$X))

tibble(sampling_distribution) %>% 
  ggplot() +
  geom_histogram(mapping = aes(x=sampling_distribution),
                 color = 'black', binwidth = 0.02) +
  labs(x='Slope Estimate (Sampling Distribution)') # a normally distributed sampling distribution again!
```

---

## P-values

```{r p-value and confidence interval}
sum(sampling_distribution > slope_estimate) # p-value effectively zero
```

## Confidence Intervals

```{r confidence intervals}
standard_error <- sd(sampling_distribution)
standard_error

confidence_interval <- c(slope_estimate - 1.96 * standard_error,
                         slope_estimate + 1.96 * standard_error)
confidence_interval

```

---

## The One-Line Built-In `R` Function

The `lm()` function estimates the linear model parameters (slope + y-intercept) and computes confidence intervals and p-values.

```{r lm}
linear_model_fit <- lm(formula = Y ~ X, 
                       data = demeaned_data) 

coef(linear_model_fit) # get the coefficient estimates

confint(linear_model_fit) # get the confidence intervals
```

---

## The One-Line Built-In `R` Function

```{r summary(lm)}
summary(linear_model_fit)
```

???

Reports a bunch of other statistics and diagnostics that you will learn about in POLS 7014. I gotta leave something for Mollie to teach you.

---

## Exercise

1. Check out the documentation for the `mtcars` dataset by typing `?mtcars`.

2. What is the mean fuel efficiency of cars (`mpg`), grouped by the number of cylinders (`cyl`)?

3. Do cars with a manual transmission have significantly higher/lower horsepower than those with an automatic transmission?

4. What is the correlation between horsepower (`hp`) and fuel efficiency (`mpg`)? Visualize the relationship.

5. Fit a linear model with horsepower as the predictor variable $(X)$ and fuel efficiency as the outcome variable $(Y)$. What is the slope of the relationship? What is the 95% confidence interval on that slope estimate?

---

## Exercise (Solution)

What is the mean fuel efficiency of cars (`mpg`), grouped by the number of cylinders (`cyl`)?

```{r exercise 1}
mtcars %>% 
  group_by(cyl) %>% 
  summarize(mean_mpg = mean(mpg),
            num = n())
```

---

## Exercise (Solution)

Do cars with a manual transmission have significantly higher/lower horsepower than those with an automatic transmission?

```{r exercise 2}
mtcars %>% 
  t.test(hp ~ am, data = .)
```

---

## Exercise (Solution)

What is the correlation between horsepower (`hp`) and fuel efficiency (`mpg`)?

```{r exercise 3, out.width = '60%'}
ggplot(data = mtcars) +
  geom_point(mapping = aes(x=hp, y=mpg)) +
  labs(x = 'Horsepower', y = 'Fuel Efficiency (Miles Per Gallon)',
       title = paste0('Correlation = ', cor(mtcars$hp, mtcars$mpg) %>% round(3)))
```

---

## Exercise (Solution)

Fit a linear model with horsepower as the predictor variable $(X)$ and fuel efficiency as the outcome variable $(Y)$. What is the slope of the relationship? What is the 95% confidence interval on that slope estimate?

```{r exercise 4}
cars_linear_model <- lm(mpg ~ hp, data = mtcars)

coef(cars_linear_model)

confint(cars_linear_model)
```

---


class: center, middle

# Multivariable Linear Regression

---

## Multivariable Linear Regression

Suppose we want to include **multiple** explanatory variables. 

--

$$\text{mpg} = \alpha + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon$$
Cars with higher horsepower probably have lower fuel efficiency *and* heavier cars probably have lower fuel efficiency. We'd like to estimate the slope of both relationships simultaneously!

--

**Vector Representation**:

$$\underbrace{\begin{bmatrix} 21.0 \\ 21.0 \\ 22.8 \\ \vdots \\ 21.4 \end{bmatrix}}_\text{mpg} = \underbrace{\alpha \times \begin{bmatrix} 1 \\ 1 \\1 \\ \vdots \\ 1 \end{bmatrix}}_\alpha + \underbrace{\beta_1 \times \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}}_{\beta_1 \text{hp}} + \underbrace{\beta_2 \times \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}}_{\beta_2 \text{wt}} + \underbrace{\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \vdots \\ \varepsilon_n \end{bmatrix}}_\varepsilon$$
---

## Multivariable Linear Regression

The challenge is to simultaneously estimate $\alpha$, $\beta_1$, and $\beta_2$.

$$\text{mpg} = \alpha + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon$$
--

We've come as far as we can with scalar algebra. It's time you learned **matrix algebra**.

---

class: center, middle

# Matrix Algebra

---

## Matrix Algebra

Recall from Week 1 that a **matrix** is a bunch of vectors squished together.

--

.pull-left[
$\text{hp} = \begin{bmatrix} 110 \\ 110 \\ 93 \\ \vdots \\ 109 \end{bmatrix}$
]

.pull-right[
$\text{wt} = \begin{bmatrix} 2.62 \\ 2.875 \\ 2.32 \\ \vdots \\ 2.78 \end{bmatrix}$
]

--

<br>

$$X = \begin{bmatrix} 110 & 2.62 \\ 110 & 2.875 \\ 93 & 2.32 \\ \vdots & \vdots \\ 109 & 2.78 \end{bmatrix}$$
???

We've been calling this a **dataframe**.

---

## Matrix Algebra

**Adding** and **subtracting** matrices is straightforward. Just add and subtract elementwise. 

.pull-left[
$A = \begin{bmatrix} 1 & 2 \\ 2 & 3 \\ 4 & 4 \end{bmatrix}$
]

.pull-right[
$B = \begin{bmatrix} 2 & 1 \\ 4 & 4 \\ 8 & 5 \end{bmatrix}$
]

--

$$A + B = \begin{bmatrix} 3 & 3 \\ 6 & 7 \\ 12 & 9 \end{bmatrix}$$
--

**Multiplying** and **dividing** is where it gets tricky.
  - You can only multiply *some* matrices together (they must be **conformable**)
  - And matrix division isn't really a thing. Instead, we multiply by the matrix's **inverse**.


---

## Matrix Multiplication

--

First, let me introduce the **dot product** of two vectors.

If $A = [3,1,2]$ and $B = [1,2,3]$, then:

$$A \cdot B = 3 \times 1 + 1 \times 2 + 2 \times 3 = 11$$
--

```{r dot product}
A <- c(3,1,2)
B <- c(1,2,3)

# dot product
sum(A*B)
```

---

## Matrix Multiplication

When you multiply two matrices, you take a series of dot products.

.pull-left[
$A = \begin{bmatrix} 1 & 2 \\ 2 & 3 \end{bmatrix}$
]

.pull-right[
$B = \begin{bmatrix} 2 & 1 \\ 4 & 4 \end{bmatrix}$
]

--

<br>

$$AB = \begin{bmatrix} 1 \times 2 + 2 \times 4 & 1 \times 1 + 2 \times 4 \\ 2 \times 2 + 3 \times 4 & 2 \times 1 + 3 \times 4 \end{bmatrix} = \begin{bmatrix} 10 & 9 \\ 16 & 14 \end{bmatrix}$$

--

<br>

This is all very strange and confusing to get used to at first, but we'll soon see that it makes representing our multivariable linear regression problem a whole lot easier.

---

## Matrix Multiplication

You can multiply matrices in `R` with the `%*%` command.

```{r matrix multiplication}
A <- cbind(c(1,2), c(2,3))
A

B <- cbind(c(2,4), c(1,4))
B
```

--

```{r matrix multiplication 2}
A %*% B
```

---

## To Be Continued...

```{r regression, echo = FALSE, eval = FALSE}
data <- tibble(c = 1,
            x1 = rnorm(100, 0, 1),
            x2 = rnorm(100, 0, 1),
            y = 2*x1 + 3*x2 + rnorm(100, 0, 1))

linear_model <- lm(y ~ x1 + x2, data = data)

summary(linear_model)

# X matrix
X <- data %>% 
  select(c, x1, x2) %>% 
  as.matrix

# y vector
y <- data %>% 
  pull(y)

# beta hat = (X'X)^-1 X'y
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y

beta_hat
linear_model
```

???

## Outline

1. Covariance and correlation; play Guess the Correlation

2. Linear Regression (Line of Best Fit)

  - The Linear Model; slope/intercept parameters; yhat; estimation; Least squares; residuals
  - `geom_smooth(method = 'lm')`
  - `lm()`
  - Now we want to be able to do that for MULTIPLE explanatory variables, and show that the solution is unique. For that we'll need matrix algebra.

3. Matrices

  - Matrices: multidimensional vectors; we've been calling them `data frames`. Show a data frame. It's a matrix. Same deal. 
  - Adding and subtracting matrices is just like you would expect (elementwise), but multiplying and dividing matrices is the tricky part.
  - Scalar multiplication vs. matrix multiplication; matrices must be *conformable* in order to multiply them; can't just multiply any two things together like you can with scalars; matrix transposes
  - Multivariable regression as a matrix multiplication problem. yhat = Xb; just a compact way of representing a big system of equations
  - Show that you can do those matrix multiplications in `R`; you need Wolfram Alpha to cheat on calculus, but `R` is actually designed for matrix algebra.
  - So you need to find the vector b that minimizes (y - Xb)(y-Xb)'
  - Turns out vector calculus is just like the calculus we saw before, except instead of *dividing* matrices, you take their inverse;  function
  - Quick primer on matrix inverses; in scalar algebra, b/b = 1, in matrix algebra BB^(-1) = I; once again the great thing about taking a political methodology course in 2020 is that I won't teach you how to solve for a matrix inverse by hand (like I was made to do in undergrad). There is literally a function called `solve()` in `R` which inputs a matrix and returns its inverse. 
  - yy' - Xby' - yX'b + XbX'b (minimize wrt b)
  - -X'y - X'y + 2XbX' = 0
  - b = (X'X)^-1(X'y)