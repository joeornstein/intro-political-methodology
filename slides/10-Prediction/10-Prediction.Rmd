---
title: ""
# author: "Joseph T. Ornstein"
# date: "June 12, 2020"
output:
  xaringan::moon_reader:
    nature:
      highlightStyle: github
      countIncrementalSlides: false
# subtitle: 'POLS 7012: Introduction to Political Methodology'
# institute: University of Georgia
---

<style>

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

</style>

```{r Setup, include=FALSE}
library(tidyverse)
theme_set(theme_minimal(base_size = 16))
knitr::opts_chunk$set(comment=NA, fig.width=7, fig.height=5, 
                      fig.align = 'center', out.width = 600,
                      message=FALSE, warning=FALSE, echo=FALSE)
set.seed(42)
```

## Outline

What are models?

1. Abstracted mathematical representations of the dataset.

2. If done right, the relationships expressed in a model separate the "signal" (an interesting phenomenon or a reliable pattern that predicts your data) from the "noise" (randomness from sampling, measurement error, or a bunch of other things that are (hopefully) orthogonal to the topic you care about).

The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.



Observed data & parameters. We fit the parameters using an *estimator*, just like we did with the linear model.



What are they good for?

1. Summarizing patterns and relationships in data (what we've been doing so far). Allows us to quantify the size and uncertainty of estimates.

2. Make predictions about unseen data.

3. Distinguish cause and effect; make **counterfactual** predictions.

Today #2, then #3. 

- Making predictions from an `lm()` model.
  - Prediction error
  - But it's not really a...prediction, is it?
- Split sample with CCES
  - `lm()` the feeling thermometer, maybe?
  
---

## Predicting Things!

```{r load the good old ANES pilot survey}
load('data/anes_pilot_2019.RData')

data <- data %>% 
  mutate(age = 2019 - birthyr,
         female = as.numeric(gender == 2),
         race = case_when(race == 1 ~ 'White',
                          race == 2 ~ 'Black',
                          race == 3 ~ 'Hispanic',
                          TRUE ~ 'Other'),
         educ = factor(educ))

ggplot(data) +
  geom_histogram(aes(x=ftjournal),color = 'black', binwidth = 7)
```

```{r separate into test and training set}

# take 70% of the data as the "training set"
train <- data %>% 
  sample_frac(0.7)

# the test set is whatever is left over (use anti_join)
test <- data %>% 
  anti_join(train, by = 'caseid')
```

---

Now, we only train our models on the `train` set. Never on the `test` set! That's only for validation.

```{r train}
lm1 <- lm(ftjournal ~ age + female + educ + race, data = train)

summary(lm1)
```
---

# Out-Of-Sample Prediction

```{r out-of-sample prediction}
test <- test %>% 
  mutate(ftjournal_hat = predict(lm1, test))

ggplot(test) + 
  geom_point(aes(x=ftjournal_hat,y=ftjournal)) +
  labs(x='Predicted Feeling Thermometer (Journalists)', 
       y='Actual Feeling Thermometer (Journalists)') +
  geom_abline(intercept = 0, slope = 1, linetype = 'dotted')
```

---

Prediction error metrics

How would you compute mean squared error?

--

```{r mean squared error}
mse <- mean((test$ftjournal - test$ftjournal_hat)^2)
```