---
title: "Prediction"
# author: "Joseph T. Ornstein"
# date: "June 12, 2020"
output:
  xaringan::moon_reader:
    nature:
      highlightStyle: github
      countIncrementalSlides: false
# subtitle: 'POLS 7012: Introduction to Political Methodology'
# institute: University of Georgia
---

<style>

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

</style>

```{r Setup, include=FALSE}
library(tidyverse)
theme_set(theme_minimal(base_size = 16))
knitr::opts_chunk$set(comment=NA, fig.width=7, fig.height=5, 
                      fig.align = 'center', out.width = 600,
                      message=FALSE, warning=FALSE, echo=TRUE)
set.seed(42)
```

## What Are Models?

A model is "low-dimensional" mathematical representation of the world. It takes all the messy, noisy complexity of a dataset and simplifies it. 

--

```{r echo = FALSE, out.width='50%'}
tibble(x = rnorm(100,0,1),
       y = 2*x + rnorm(100,0,2)) %>% 
  ggplot() +
  geom_point(aes(x=x,y=y)) +
  geom_smooth(aes(x=x,y=y), method = 'lm', se = TRUE) +
  theme_bw()
```

--

One way to think about statistical models is that they partition the data into **signal** and **noise**.

$$Y = \underbrace{X\beta}_\text{signal} + \underbrace{\varepsilon}_\text{noise}$$

???

If I've done my job right, then the noise is unrelated to the signal, the systematic relationships that I care about.


## Outline

What are models?

1. Abstracted mathematical representations of the dataset.

2. If done right, the relationships expressed in a model separate the "signal" (an interesting phenomenon or a reliable pattern that predicts your data) from the "noise" (randomness from sampling, measurement error, or a bunch of other things that are (hopefully) orthogonal to the topic you care about).

The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.



Observed data & parameters. We fit the parameters using an *estimator*, just like we did with the linear model.



What are they good for?

1. Summarizing patterns and relationships in data (what we've been doing so far). Allows us to quantify the size and uncertainty of estimates.

2. Make predictions about unseen data.

3. Distinguish cause and effect; make **counterfactual** predictions.

Today #2, then #3. 

- Making predictions from an `lm()` model.
  - Prediction error
  - But it's not really a...prediction, is it?
- Split sample with CCES
  - `lm()` the feeling thermometer, maybe?
  
  
  
---

## Why Do We Model Data?

Broadly speaking, there are three reasons social scientists build models:

--

1. To **summarize** patterns and relationships, and to quantify our uncertainty.

--

  - This is what we've been doing so far.
  
--

2. To make **predictions** about unobserved data.

--

3. To distinguish between cause and effect; to make **counterfactual** predictions.

--

Today we discuss prediction. Next week, causal inference.

---

## Today's Objectives

By the end of this module, you will be able to...

- Make predictions using model objects in `R`

- Evaluate a model's predictive accuracy

- Cross-validate and regularize models to improve out-of-sample prediction

- Fit your very first **machine learning** model

---

## Our Challenge Today

I want to try to **predict** how ANES survey respondents feel about journalists, using observable covariates. 

```{r load the good old ANES pilot survey, out.width = '50%'}
# import and tidy the ANES pilot study
# source() function calls another R script
source('tidy-anes.R')

ggplot(data) +
  geom_histogram(aes(x=ftjournal),color = 'black', binwidth = 7)
```

---

## Exercise 1

Estimate a linear model of `ftjournal` using observed characteristics of the respondents. What patterns do you observe?

--

```{r lm1}
linear_model1 <- lm(ftjournal ~ age + female + ftbiden,
                    data = data)
summary(linear_model1)
```

---

## Making Predictions

To make **predictions** using your model, use the `predict()` function!

  - The first input is a model object.
  - The second input is a dataframe that contains all the explantaory variables in your model.

```{r predict}
# create a new variable called ftjournal_prediction
data <- data %>% 
  mutate(ftjournal_prediction = predict(linear_model1, data))
```

---

## Making Predictions

```{r look at predictions}
coef(linear_model1)

# look at some predictions
data %>% 
  select(age, female, ftbiden, ftjournal_prediction) %>% 
  head(5)
```

---

## Making Predictions

```{r visualize predictions, out.width = '60%'}
ggplot(data) +
  geom_point(mapping = aes(x=ftjournal_prediction, y=ftjournal),
             alpha = 0.2) +
  labs(x = 'Predicted Feeling Thermometer (Journalists)',
       y = 'Actual Feeling Thermometer (Journalists)') +
  theme_bw() +
  geom_abline(intercept=0, slope = 1, linetype = 'dashed', size = 1, color = 'red')
```

---

## Quick Clarification 

Why did we recode all of those categorical variables? Why not just leave them as numbers? 

`lm()` needs to know whether to treat a variable as continuous or categorical. 
 - If the variable is numeric, it will treat it as continuous 
 - If it's a character, it will treat it as categorical
 
--

If we leave race coded as numeric, `lm()` will do this:

```{r old coding, echo = FALSE, out.width = '50%'}
proper_data <- data

load('data/anes_pilot_2019.RData')

ggplot(data %>% filter(race != 8)) +
  geom_boxplot(aes(x=factor(race),y=ftjournal), notch = TRUE) +
  geom_smooth(aes(x=race,y=ftjournal), method = 'lm', se = FALSE) +
  labs(x='Race',y='Feeling Thermometer (Journalists)')

# revert to the proper coding
data <- proper_data
rm(proper_data)
```

---

class: center, middle

## Model Selection


---



## Model Selection

How do you decide which variables to include in your model? 

--

Why not **ALL THE VARIABLES**?

```{r kitchen sink, out.width = '75%', echo = FALSE}
knitr::include_graphics('img/piled-up-dishes-in-kitchen-sink.jpg')
```


---

## Model Selection

What constitutes the "best" model depends on what you're trying to do.

- If you're building a model to make **predictions**, then we'll evaluate it on the basis of its predictions.

- If you're building a model for **causal inference**, then we'll need a different set of tools (next week).

--

Regardless of your objective, the correct answer is almost *never* the "kitchen sink model".


---


## Prediction Error



--

A common error metric is the **root mean squared error** (RMSE).

$$RMSE = \sqrt{\frac{\sum(\hat{Y_i} - Y_i)^2}{n}}$$

--

**Exercise:** Compute the RMSE of your predictions in `R`.

---

## Prediction Error

**Exercise:** Compute the RMSE of your predictions in `R`.

--

```{r sse}
data <- data %>% 
  mutate(error = ftjournal_prediction - ftjournal,
         squared_error = error ^ 2)

data %>% 
  select(ftjournal, ftjournal_prediction, error, squared_error) %>% 
  head(4)

data$squared_error %>% mean %>% sqrt
```


---

## Prediction Error

Let's create a function to perform that computation whenever we want:

```{r rmse function}
rmse <- function(truth, prediction){
  (prediction - truth)^2 %>% 
    mean %>% 
    sqrt
}

rmse(truth = data$ftjournal,
     prediction = data$ftjournal_prediction)
```

---

class: center, middle

# Except...

---

## What We Just Did Was Cheating

--

```{r pumpkin, echo = FALSE}
knitr::include_graphics('img/cheater.gif')
```

---

## What We Just Did Was Cheating

Those weren't really *predictions*, were they?

--

Our model knew all of the outcomes ahead of time, and it found the parameters that minimized the sum of squared errors. 

--

$$\hat{\beta} = (X'X)^{-1}X'Y$$

Remember how we derived the OLS estimator? It's *literally* the $\beta$ that minimizes squared error.

---

## What We Just Did Was Cheating

```{r straw man 1, echo = FALSE}
straw_data <- tibble(X = 1:5,
                     Y = 1:5)

p <- ggplot(straw_data) +
  geom_point(aes(x=X,y=Y), size = 4)

p
```

???

It's like if I gave you these five data points and you drew a line through them and said "look at me I perfectly predicted all the data!" No you didn't. You knew the answer ahead of time.

---

## What We Just Did Was Cheating

```{r straw man 2, echo = FALSE}
straw_data <- tibble(X = 1:5,
                     Y = 1:5)

p +
  geom_smooth(aes(x=X,y=Y), method = 'lm', se = FALSE)
```

--

This is called **in-sample** prediction. The true test of a model is **out-of-sample** prediction.

---

## Best Practice: Hide some of your data from your model

Partition the data into a **training** set and a **test** set.

```{r separate into test and training set}
# keep 70% of the data as the "training set"
train <- data %>% 
  sample_frac(0.7)

# hold out the remaining 30% to test your predictions
test <- data %>% 
  anti_join(train, by = 'caseid')
```

--

Now, we only train our models on the `train` set. Never on the `test` set! That's just for validation.

---

## Exercise 2: Out-of-sample Prediction

Re-estimate your linear model on the `train` data and make out-of-sample predictions on the `test` data. Is the RMSE lower or higher than the in-sample predictions?

--

```{r out-of-sample prediction}
# train the model
lm1 <- lm(ftjournal ~ age + female + ftbiden, data = train)

# make out-of-sample predictions
test <- test %>% 
  mutate(ftjournal_prediction = predict(lm1, test))

# evaluate model fit
rmse(truth = test$ftjournal,
     prediction = test$ftjournal_prediction)
```

---

## Back To Model Selection

Let's evaluate two models, a "kitchen sink" model and a simpler model.

--

```{r in-sample lm}
kitchen_sink_lm <- lm(ftjournal ~ age + race + female + educ + liveurban + ftbiden + fttrump +
                        marstat + child18 + pew_religimp + vote20jb + ideo5 + bmi +
                        facebook_user + twitter_user + instagram_user + reddit_user + 
                        youtube_user + snapchat_user + tiktok_user, data = train)

simple_lm <- lm(ftjournal ~ female + ftbiden + fttrump + ideo5 + twitter_user, data = train)
```
--

```{r compare rmse}
# kitchen sink in-sample RMSE
rmse(truth = train$ftjournal,
     prediction = predict(kitchen_sink_lm, train))

# simple model in-sample RMSE
rmse(truth = train$ftjournal,
     prediction = predict(simple_lm, train))
```

???

Now we can return to our original question. What variables should we include? What's the "best" model?

---

## Back To Model Selection

But when you check out-of-sample RMSE...

```{r compare out-of-sample rmse}
# kitchen sink out-of-sample RMSE
rmse(truth = test$ftjournal,
     prediction = predict(kitchen_sink_lm, test))

# simple model out-of-sample RMSE
rmse(truth = test$ftjournal,
     prediction = predict(simple_lm, test))
```

---

## Overfitting

<br>

```{r overfitting, echo = FALSE}
knitr::include_graphics('img/overfitting.jpg')
```


---

class: center, middle

## Cross-Validation and Regularization

---

## Cross-Validation and Regularization

Two techniques for improving the out-of-sample predictive accuracy of models.

--

- **Cross-validation** checks out-of-sample predictive accuracy for every observation in the dataset.

--

- **Regularization** decreases the complexity of a model to improve out-of-sample fit.

---

## Cross-Validation

Repeat the partitions between `train` and `test` until every observation has been included in the `test` set.

--

![](img/cross-validation.png)
--

If you repeat the validation process $k$ times, it is called $k$-fold cross-validation. If you repeat it $n-1$ times, it is called "Leave Out One Cross Validation" (LOOCV).

---

## Cross-Validation

We can automate this process with the `caret` package.

```{r cross-validation with caret}
library(caret)

# 10-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)

# Train the kitchen sink model
kitchen_sink_model <- train(ftjournal ~ age + race + female + educ + liveurban + ftbiden + fttrump +
                              marstat + child18 + pew_religimp + vote20jb + ideo5 + bmi +
                              facebook_user + twitter_user + instagram_user + reddit_user + 
                              youtube_user + snapchat_user + tiktok_user, 
                            data = data, method = "lm",
                            trControl = train.control)

# Train the simple model
simple_model <- train(ftjournal ~ female + ftbiden + fttrump + ideo5 + twitter_user,
                      data = data, method = 'lm',
                      trControl = train.control)
```

---

## Cross-Validation

```{r summarize cv results}
# Summarize the results
print(kitchen_sink_model)
```

---

## Cross-Validation

```{r summarize cv results 2}
# Summarize the results
print(simple_model)
```

---

## Regularization

```{r overfitting 2, out.width = '80%', echo = FALSE}
knitr::include_graphics('img/overfitting.jpg')
```

When you **regularize** a model, you try to find that sweet spot in the middle.
 - Not so simple that it *underfits* the training data.
 - Not so complex that it *overfits* the training data.

???

There are some automated regularization techniques (e.g. LASSO), that you might find interesting! If you do, take Jason Anastasopoulos's machine learning class!

---

## Exercise 3

1. Compute the cross-validated prediction error for your model using the `caret` package. 

2. Then try to find the sweet spot. Add/remove predictor variables until you get the best-predicting model you can find.

---

class: center, middle

## A Taste of Machine Learning

---

## A Taste of Machine Learning

So far we've been fitting only linear models. Can we do better with some other type of model?

--

```{r kknn}
library(kknn)
```

Let's introduce a model called **k-nearest neighbors** (KNN). To make a prediction using KNN, follow

--

1. Find the $k$ observations that are "closest" to the one you want to predict.

--

2. Take the average outcome from those $k$ observations.

--

That's it!

---

## The Linear Model Vs. K-Nearest Neighbors

.pull-left[
```{r knn example, echo = FALSE}
p <- ggplot(mtcars) +
  geom_point(aes(x=hp, y = mpg))

p + geom_smooth(aes(x=hp,y=mpg), 
                method = 'lm', se = FALSE) +
  labs(title = 'Linear Model')
```
]

--

.pull-right[
```{r knn example 2, echo = FALSE}
k_choice <- 10

knn_fit <- tibble(hp = seq(min(mtcars$hp),
                           max(mtcars$hp),
                           1))

knn_model <- kknn(mpg ~ hp, train = mtcars, test = knn_fit,
                  k = k_choice)

knn_fit$knn_prediction <- knn_model$fitted.values

p + geom_line(data = knn_fit,
                aes(x=hp,y=knn_prediction),
              color = 'blue') +
  labs(title = paste0('KNN Model (K = ', k_choice, ')'))
```
]

--

KNN captures non-linearity a bit better. Fuel efficiency levels off around 15 mpg.

---

## Estimating KNN

To make predictions with KNN, use the `kknn` package.

```{r knn estimation}
# first, estimate the model object
knn_model <- kknn(ftjournal ~ fttrump + ftbiden + age,
                  train = train,
                  test = test, 
                  k = 50)

```

--

Compare the RMSE:

```{r rmse knn}
# linear model
rmse(truth = test$ftjournal,
     prediction = predict(simple_lm, test))

# knn
rmse(truth = test$ftjournal,
     prediction = knn_model$fitted.values)
```

---

## Cross-Validation and Regularization (KNN)

We can use cross-validation and regularization to find the best KNN model too. There's a built-in function that performs LOOCV and finds the best value of $k$.

```{r knn cv}
knn_train <- train.kknn(ftjournal ~ fttrump + ftbiden + age + female + ideo5,
                        data = data, kmax = 100)

# What's the best k?
best_k <- knn_train$best.parameters$k
best_k

# cross-validated RMSE
knn_train$MEAN.SQU[best_k] %>% sqrt
```

---

class: center, middle

## Binary Outcomes

---

## Binary Outcomes

What if the thing you're predicting is categorical instead of continuous? For example, let's try to predict whether a respondent approves of the Dream Act.

```{r pro_dream}
table(data$pro_dream)
```

--

Can we still use linear models?

--

Sure. A linear model with a binary outcome is called the **linear probability model**. It's fine, but it has some weird features. Observe.

--

```{r linear probability model}
lpm <- lm(pro_dream ~ ftbiden, data = data)
coef(lpm)
```

---

## Binary Outcomes

According to this model, about 50% of respondents that rate Biden at 0 approve of the Dream Act, and that percentage increases by `r round(coef(lpm)['ftbiden'], 4)` per thermometer unit.

```{r ggplot lpm, out.width = '65%'}
ggplot(data) +
  geom_point(aes(x=ftbiden, y=pro_dream)) +
  geom_smooth(aes(x=ftbiden,y=pro_dream), method = 'lm', se = FALSE)
```

---

class: center, middle

## A Taste of Logistic Regression

---

## A Taste of Logistic Regression

We want a model that only predicts probabilities between zero and one.

--

$$P(Y = 1) = \frac{1}{1 + e^{-X\beta}}$$

This sigmoid function "squishes" $X\beta$ so the resulting probability can't be greater than 1 or less than zero.

--

```{r plot sigmoid, out.width = '55%', echo = FALSE}
sigmoid_data <- tibble(x_beta = seq(-10,10,0.1),
               prob = 1 / (1 + exp(-x_beta)))

ggplot(sigmoid_data) + 
  geom_line(aes(x=x_beta, y = prob)) +
  labs(x = 'X * Beta', y = 'P(Y=1)')
```

---

## A Taste of Logistic Regression

To estimate a logistic regression in `R`, use the `glm()` function (GLM = Generalized Linear Models).

```{r estimate glm}
logit_model <- glm(pro_dream ~ ftbiden + fttrump + age, data = train)
summary(logit_model)
```

---

## A Taste of Logistic Regression

I won't teach you how to interpret those coefficients here. Suffice to say, once you squish everything, the interpretation is not as straightforward.

--

But we can still make predictions just like we did before:

```{r glm predicts}
test <- test %>% 
  mutate(pro_dream_probability = predict(logit_model, test, type = 'response'))

test %>% select(ftbiden, pro_dream_probability, pro_dream) %>% 
  head(8)
```

---

## Prediction Error Metrics for Binary Outcomes

Often, we use **classification accuracy** to evaluate these sorts of model predictions.

What percent of observations were correctly classified?

```{r accuracy function}
classification_accuracy <- function(truth, predicted){
  predicted <- ifelse(predicted > 0.5, 1, 0)
  sum(truth == predicted) / length(truth) * 100
}

classification_accuracy(truth = test$pro_dream,
                        predicted = test$pro_dream_probability)
```

---

## KNN With Binary Outcomes

The syntax for KNN doesn't even change. The model still finds the $k$ closest neighbors, and computes the fraction of those neighbors that are pro-Dream Act.

```{r knn with binary outcomes}
knn_model <- kknn(pro_dream ~ ftbiden + fttrump + age,
                  train = train,
                  test = test,
                  k = 51)

test <- test %>% 
  mutate(pro_dream_probability_knn = knn_model$fitted.values)
```

---

## KNN With Binary Outcomes

Compare those predictions with the ones we got from `glm()`.

```{r compare predictions}
test %>% select(ftbiden, pro_dream_probability, pro_dream_probability_knn, pro_dream) %>% 
  head(10)
```

---

## Cross-Validation and Regularization

We can use the same cross-validation and regularization procedure to get the best predictive models here.

```{r glm caret}
# caret yells at you if the outcome is numeric
data <- data %>% 
  mutate(pro_dream = factor(pro_dream))

glm_model <- train(pro_dream ~ ftbiden + fttrump + age,
                   data = data, method = 'glm',
                   trControl =  train.control)

knn_model <- train.kknn(pro_dream ~ ftbiden + fttrump + age,
                        data = data, kmax = 100)
```

---

# Problem Set 8

Build a model to predict which CCES survey respondents voted for Trump in 2016 and which voted for Clinton in 2016.

Best model wins!

--

**Note:** When I gave this challenge to my undergraduates at the beginning of the semester, their best model correctly predicted 69.6% of respondents.

--

So...that's your benchmark.

---