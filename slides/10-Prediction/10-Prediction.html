<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Prediction</title>
    <meta charset="utf-8" />
    <link href="10-Prediction_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="10-Prediction_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Prediction

---


&lt;style&gt;

.center-middle {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}

&lt;/style&gt;



## What Are Models?

A model is "low-dimensional" mathematical representation of the world. It takes all the messy, noisy complexity of a dataset and simplifies it. 

--

&lt;img src="10-Prediction_files/figure-html/unnamed-chunk-1-1.png" width="50%" style="display: block; margin: auto;" /&gt;

--

One way to think about statistical models is that they partition the data into **signal** and **noise**.

`$$Y = \underbrace{X\beta}_\text{signal} + \underbrace{\varepsilon}_\text{noise}$$`

???

If I've done my job right, then the noise is unrelated to the signal, the systematic relationships that I care about.


## Outline

What are models?

1. Abstracted mathematical representations of the dataset.

2. If done right, the relationships expressed in a model separate the "signal" (an interesting phenomenon or a reliable pattern that predicts your data) from the "noise" (randomness from sampling, measurement error, or a bunch of other things that are (hopefully) orthogonal to the topic you care about).

The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.



Observed data &amp; parameters. We fit the parameters using an *estimator*, just like we did with the linear model.



What are they good for?

1. Summarizing patterns and relationships in data (what we've been doing so far). Allows us to quantify the size and uncertainty of estimates.

2. Make predictions about unseen data.

3. Distinguish cause and effect; make **counterfactual** predictions.

Today #2, then #3. 

- Making predictions from an `lm()` model.
  - Prediction error
  - But it's not really a...prediction, is it?
- Split sample with CCES
  - `lm()` the feeling thermometer, maybe?
  
  
  
---

## Why Do We Model Data?

Broadly speaking, there are three reasons social scientists build models:

--

1. To **summarize** patterns and relationships, and to quantify our uncertainty.

--

  - This is what we've been doing so far.
  
--

2. To make **predictions** about unobserved data.

--

3. To distinguish between cause and effect; to make **counterfactual** predictions.

--

Today we discuss prediction. Next week, causal inference.

---

## Today's Objectives

By the end of this module, you will be able to...

- Make predictions using model objects in `R`

- Evaluate a model's predictive accuracy

- Cross-validate and regularize models to improve out-of-sample prediction

- Fit your very first **machine learning** model

---

## Our Challenge Today

I want to try to **predict** how ANES survey respondents feel about journalists, using observable covariates. 


```r
# import and tidy the ANES pilot study
# source() function calls another R script
source('tidy-anes.R')

ggplot(data) +
  geom_histogram(aes(x=ftjournal),color = 'black', binwidth = 7)
```

&lt;img src="10-Prediction_files/figure-html/load the good old ANES pilot survey-1.png" width="50%" style="display: block; margin: auto;" /&gt;

---

## Exercise 1

Estimate a linear model of `ftjournal` using observed characteristics of the respondents. What patterns do you observe?

--


```r
linear_model1 &lt;- lm(ftjournal ~ age + female + ftbiden,
                    data = data)
summary(linear_model1)
```

```

Call:
lm(formula = ftjournal ~ age + female + ftbiden, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-86.800 -18.063  -1.232  16.427  81.313 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 31.05893    1.56955  19.788  &lt; 2e-16 ***
age         -0.16278    0.02522  -6.454 1.26e-10 ***
female       2.61640    0.86155   3.037  0.00241 ** 
ftbiden      0.61437    0.01290  47.639  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 23.66 on 3045 degrees of freedom
Multiple R-squared:  0.4425,	Adjusted R-squared:  0.4419 
F-statistic: 805.6 on 3 and 3045 DF,  p-value: &lt; 2.2e-16
```

---

## Making Predictions

To make **predictions** using your model, use the `predict()` function!

  - The first input is a model object.
  - The second input is a dataframe that contains all the explantaory variables in your model.


```r
# create a new variable called ftjournal_prediction
data &lt;- data %&gt;% 
  mutate(ftjournal_prediction = predict(linear_model1, data))
```

---

## Making Predictions


```r
coef(linear_model1)
```

```
(Intercept)         age      female     ftbiden 
 31.0589333  -0.1627835   2.6163974   0.6143689 
```

```r
# look at some predictions
data %&gt;% 
  select(age, female, ftbiden, ftjournal_prediction) %&gt;% 
  head(5)
```

```
# A tibble: 5 x 4
    age female ftbiden ftjournal_prediction
  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;                &lt;dbl&gt;
1    50      0      52                 54.9
2    77      0      41                 43.7
3    65      1      88                 77.2
4    40      0       0                 24.5
5    79      1      25                 36.2
```

---

## Making Predictions


```r
ggplot(data) +
  geom_point(mapping = aes(x=ftjournal_prediction, y=ftjournal),
             alpha = 0.2) +
  labs(x = 'Predicted Feeling Thermometer (Journalists)',
       y = 'Actual Feeling Thermometer (Journalists)') +
  theme_bw() +
  geom_abline(intercept=0, slope = 1, linetype = 'dashed', size = 1, color = 'red')
```

&lt;img src="10-Prediction_files/figure-html/visualize predictions-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

## Quick Clarification 

Why did we recode all of those categorical variables? Why not just leave them as numbers? 

`lm()` needs to know whether to treat a variable as continuous or categorical. 
 - If the variable is numeric, it will treat it as continuous 
 - If it's a character, it will treat it as categorical
 
--

If we leave race coded as numeric, `lm()` will do this:

&lt;img src="10-Prediction_files/figure-html/old coding-1.png" width="50%" style="display: block; margin: auto;" /&gt;

---

class: center, middle

## Model Selection


---



## Model Selection

How do you decide which variables to include in your model? 

--

Why not **ALL THE VARIABLES**?

&lt;img src="img/piled-up-dishes-in-kitchen-sink.jpg" width="75%" style="display: block; margin: auto;" /&gt;


---

## Model Selection

What constitutes the "best" model depends on what you're trying to do.

- If you're building a model to make **predictions**, then we'll evaluate it on the basis of its predictions.

- If you're building a model for **causal inference**, then we'll need a different set of tools (next week).

--

Regardless of your objective, the correct answer is almost *never* the "kitchen sink model".


---


## Prediction Error



--

A common error metric is the **root mean squared error** (RMSE).

`$$RMSE = \sqrt{\frac{\sum(\hat{Y_i} - Y_i)^2}{n}}$$`

--

**Exercise:** Compute the RMSE of your predictions in `R`.

---

## Prediction Error

**Exercise:** Compute the RMSE of your predictions in `R`.

--


```r
data &lt;- data %&gt;% 
  mutate(error = ftjournal_prediction - ftjournal,
         squared_error = error ^ 2)

data %&gt;% 
  select(ftjournal, ftjournal_prediction, error, squared_error) %&gt;% 
  head(4)
```

```
# A tibble: 4 x 4
  ftjournal ftjournal_prediction   error squared_error
      &lt;dbl&gt;                &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;
1        99                 54.9 -44.1        1948.   
2        67                 43.7 -23.3         542.   
3        63                 77.2  14.2         200.   
4        25                 24.5  -0.452         0.205
```

```r
data$squared_error %&gt;% mean %&gt;% sqrt
```

```
[1] 23.64159
```


---

## Prediction Error

Let's create a function to perform that computation whenever we want:


```r
rmse &lt;- function(truth, prediction){
  (prediction - truth)^2 %&gt;% 
    mean %&gt;% 
    sqrt
}

rmse(truth = data$ftjournal,
     prediction = data$ftjournal_prediction)
```

```
[1] 23.64159
```

---

class: center, middle

# Except...

---

## What We Just Did Was Cheating

--

&lt;img src="img/cheater.gif" width="600" style="display: block; margin: auto;" /&gt;

---

## What We Just Did Was Cheating

Those weren't really *predictions*, were they?

--

Our model knew all of the outcomes ahead of time, and it found the parameters that minimized the sum of squared errors. 

--

`$$\hat{\beta} = (X'X)^{-1}X'Y$$`

Remember how we derived the OLS estimator? It's *literally* the `\(\beta\)` that minimizes squared error.

---

## What We Just Did Was Cheating

&lt;img src="10-Prediction_files/figure-html/straw man 1-1.png" width="600" style="display: block; margin: auto;" /&gt;

???

It's like if I gave you these five data points and you drew a line through them and said "look at me I perfectly predicted all the data!" No you didn't. You knew the answer ahead of time.

---

## What We Just Did Was Cheating

&lt;img src="10-Prediction_files/figure-html/straw man 2-1.png" width="600" style="display: block; margin: auto;" /&gt;

--

This is called **in-sample** prediction. The true test of a model is **out-of-sample** prediction.

---

## Best Practice: Hide some of your data from your model

Partition the data into a **training** set and a **test** set.


```r
# keep 70% of the data as the "training set"
train &lt;- data %&gt;% 
  sample_frac(0.7)

# hold out the remaining 30% to test your predictions
test &lt;- data %&gt;% 
  anti_join(train, by = 'caseid')
```

--

Now, we only train our models on the `train` set. Never on the `test` set! That's just for validation.

---

## Exercise 2: Out-of-sample Prediction

Re-estimate your linear model on the `train` data and make out-of-sample predictions on the `test` data. Is the RMSE lower or higher than the in-sample predictions?

--


```r
# train the model
lm1 &lt;- lm(ftjournal ~ age + female + ftbiden, data = train)

# make out-of-sample predictions
test &lt;- test %&gt;% 
  mutate(ftjournal_prediction = predict(lm1, test))

# evaluate model fit
rmse(truth = test$ftjournal,
     prediction = test$ftjournal_prediction)
```

```
[1] 23.4964
```

---

## Back To Model Selection

Let's evaluate two models, a "kitchen sink" model and a simpler model.

--


```r
kitchen_sink_lm &lt;- lm(ftjournal ~ age + race + female + educ + liveurban + ftbiden + fttrump +
                        marstat + child18 + pew_religimp + vote20jb + ideo5 + bmi +
                        facebook_user + twitter_user + instagram_user + reddit_user + 
                        youtube_user + snapchat_user + tiktok_user, data = train)

simple_lm &lt;- lm(ftjournal ~ female + ftbiden + fttrump + ideo5 + twitter_user, data = train)
```
--


```r
# kitchen sink in-sample RMSE
rmse(truth = train$ftjournal,
     prediction = predict(kitchen_sink_lm, train))
```

```
[1] 21.50646
```

```r
# simple model in-sample RMSE
rmse(truth = train$ftjournal,
     prediction = predict(simple_lm, train))
```

```
[1] 21.73851
```

???

Now we can return to our original question. What variables should we include? What's the "best" model?

---

## Back To Model Selection

But when you check out-of-sample RMSE...


```r
# kitchen sink out-of-sample RMSE
rmse(truth = test$ftjournal,
     prediction = predict(kitchen_sink_lm, test))
```

```
[1] 21.19686
```

```r
# simple model out-of-sample RMSE
rmse(truth = test$ftjournal,
     prediction = predict(simple_lm, test))
```

```
[1] 21.11025
```

---

## Overfitting

&lt;br&gt;

&lt;img src="img/overfitting.jpg" width="600" style="display: block; margin: auto;" /&gt;


---

class: center, middle

## Cross-Validation and Regularization

---

## Cross-Validation and Regularization

Two techniques for improving the out-of-sample predictive accuracy of models.

--

- **Cross-validation** checks out-of-sample predictive accuracy for every observation in the dataset.

--

- **Regularization** decreases the complexity of a model to improve out-of-sample fit.

---

## Cross-Validation

Repeat the partitions between `train` and `test` until every observation has been included in the `test` set.

--

![](img/cross-validation.png)
--

If you repeat the validation process `\(k\)` times, it is called `\(k\)`-fold cross-validation. If you repeat it `\(n-1\)` times, it is called "Leave Out One Cross Validation" (LOOCV).

---

## Cross-Validation

We can automate this process with the `caret` package.


```r
library(caret)

# 10-fold cross-validation
train.control &lt;- trainControl(method = "cv", number = 10)

# Train the kitchen sink model
kitchen_sink_model &lt;- train(ftjournal ~ age + race + female + educ + liveurban + ftbiden + fttrump +
                              marstat + child18 + pew_religimp + vote20jb + ideo5 + bmi +
                              facebook_user + twitter_user + instagram_user + reddit_user + 
                              youtube_user + snapchat_user + tiktok_user, 
                            data = data, method = "lm",
                            trControl = train.control)

# Train the simple model
simple_model &lt;- train(ftjournal ~ female + ftbiden + fttrump + ideo5 + twitter_user,
                      data = data, method = 'lm',
                      trControl = train.control)
```

---

## Cross-Validation


```r
# Summarize the results
print(kitchen_sink_model)
```

```
Linear Regression 

3049 samples
  20 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2743, 2745, 2744, 2744, 2745, 2744, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.64529  0.5344589  17.29991

Tuning parameter 'intercept' was held constant at a value of TRUE
```

---

## Cross-Validation


```r
# Summarize the results
print(simple_model)
```

```
Linear Regression 

3049 samples
   5 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2744, 2744, 2745, 2744, 2743, 2743, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.56434  0.5375821  17.26245

Tuning parameter 'intercept' was held constant at a value of TRUE
```

---

## Regularization

&lt;img src="img/overfitting.jpg" width="80%" style="display: block; margin: auto;" /&gt;

When you **regularize** a model, you try to find that sweet spot in the middle.
 - Not so simple that it *underfits* the training data.
 - Not so complex that it *overfits* the training data.

???

There are some automated regularization techniques (e.g. LASSO), that you might find interesting! If you do, take Jason Anastasopoulos's machine learning class!

---

## Exercise 3

1. Compute the cross-validated prediction error for your model using the `caret` package. 

2. Then try to find the sweet spot. Add/remove predictor variables until you get the best-predicting model you can find.

---

class: center, middle

## A Taste of Machine Learning

---

## A Taste of Machine Learning

So far we've been fitting only linear models. Can we do better with some other type of model?

--


```r
library(kknn)
```

Let's introduce a model called **k-nearest neighbors** (KNN). To make a prediction using KNN, follow

--

1. Find the `\(k\)` observations that are "closest" to the one you want to predict.

--

2. Take the average outcome from those `\(k\)` observations.

--

That's it!

---

## The Linear Model Vs. K-Nearest Neighbors

.pull-left[
&lt;img src="10-Prediction_files/figure-html/knn example-1.png" width="600" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[
&lt;img src="10-Prediction_files/figure-html/knn example 2-1.png" width="600" style="display: block; margin: auto;" /&gt;
]

--

KNN captures non-linearity a bit better. Fuel efficiency levels off around 15 mpg.

---

## Estimating KNN

To make predictions with KNN, use the `kknn` package.


```r
# first, estimate the model object
knn_model &lt;- kknn(ftjournal ~ fttrump + ftbiden + age,
                  train = train,
                  test = test, 
                  k = 50)
```

--

Compare the RMSE:


```r
# linear model
rmse(truth = test$ftjournal,
     prediction = predict(simple_lm, test))
```

```
[1] 21.11025
```

```r
# knn
rmse(truth = test$ftjournal,
     prediction = knn_model$fitted.values)
```

```
[1] 21.20456
```

---

## Cross-Validation and Regularization (KNN)

We can use cross-validation and regularization to find the best KNN model too. There's a built-in function that performs LOOCV and finds the best value of `\(k\)`.


```r
knn_train &lt;- train.kknn(ftjournal ~ fttrump + ftbiden + age + female + ideo5,
                        data = data, kmax = 100)

# What's the best k?
best_k &lt;- knn_train$best.parameters$k
best_k
```

```
[1] 73
```

```r
# cross-validated RMSE
knn_train$MEAN.SQU[best_k] %&gt;% sqrt
```

```
[1] 21.16585
```

---

class: center, middle

## Binary Outcomes

---

## Binary Outcomes

What if the thing you're predicting is categorical instead of continuous? For example, let's try to predict whether a respondent approves of the Dream Act.


```r
table(data$pro_dream)
```

```

   0    1 
 841 2208 
```

--

Can we still use linear models?

--

Sure. A linear model with a binary outcome is called the **linear probability model**. It's fine, but it has some weird features. Observe.

--


```r
lpm &lt;- lm(pro_dream ~ ftbiden, data = data)
coef(lpm)
```

```
(Intercept)     ftbiden 
0.508936269 0.005115031 
```

---

## Binary Outcomes

According to this model, about 50% of respondents that rate Biden at 0 approve of the Dream Act, and that percentage increases by 0.0051 per thermometer unit.


```r
ggplot(data) +
  geom_point(aes(x=ftbiden, y=pro_dream)) +
  geom_smooth(aes(x=ftbiden,y=pro_dream), method = 'lm', se = FALSE)
```

&lt;img src="10-Prediction_files/figure-html/ggplot lpm-1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

class: center, middle

## A Taste of Logistic Regression

---

## A Taste of Logistic Regression

We want a model that only predicts probabilities between zero and one.

--

`$$P(Y = 1) = \frac{1}{1 + e^{-X\beta}}$$`

This sigmoid function "squishes" `\(X\beta\)` so the resulting probability can't be greater than 1 or less than zero.

--

&lt;img src="10-Prediction_files/figure-html/plot sigmoid-1.png" width="55%" style="display: block; margin: auto;" /&gt;

---

## A Taste of Logistic Regression

To estimate a logistic regression in `R`, use the `glm()` function (GLM = Generalized Linear Models).


```r
logit_model &lt;- glm(pro_dream ~ ftbiden + fttrump + age, data = train)
summary(logit_model)
```

```

Call:
glm(formula = pro_dream ~ ftbiden + fttrump + age, data = train)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.01792  -0.39377   0.05708   0.24987   0.62994  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.8096456  0.0334033  24.239  &lt; 2e-16 ***
ftbiden      0.0013749  0.0003335   4.122 3.89e-05 ***
fttrump     -0.0047945  0.0002696 -17.786  &lt; 2e-16 ***
age          0.0011319  0.0005068   2.234   0.0256 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 0.1505925)

    Null deviance: 433.05  on 2133  degrees of freedom
Residual deviance: 320.76  on 2130  degrees of freedom
AIC: 2022

Number of Fisher Scoring iterations: 2
```

---

## A Taste of Logistic Regression

I won't teach you how to interpret those coefficients here. Suffice to say, once you squish everything, the interpretation is not as straightforward.

--

But we can still make predictions just like we did before:


```r
test &lt;- test %&gt;% 
  mutate(pro_dream_probability = predict(logit_model, test, type = 'response'))

test %&gt;% select(ftbiden, pro_dream_probability, pro_dream) %&gt;% 
  head(8)
```

```
# A tibble: 8 x 3
  ftbiden pro_dream_probability pro_dream
    &lt;dbl&gt;                 &lt;dbl&gt;     &lt;dbl&gt;
1      25                 0.483         1
2      28                 0.578         0
3       6                 0.425         0
4      49                 0.952         1
5      10                 0.447         1
6      20                 0.451         1
7       2                 0.455         1
8      61                 0.917         1
```

---

## Prediction Error Metrics for Binary Outcomes

Often, we use **classification accuracy** to evaluate these sorts of model predictions.

What percent of observations were correctly classified?


```r
classification_accuracy &lt;- function(truth, predicted){
  predicted &lt;- ifelse(predicted &gt; 0.5, 1, 0)
  sum(truth == predicted) / length(truth) * 100
}

classification_accuracy(truth = test$pro_dream,
                        predicted = test$pro_dream_probability)
```

```
[1] 76.06557
```

---

## KNN With Binary Outcomes

The syntax for KNN doesn't even change. The model still finds the `\(k\)` closest neighbors, and computes the fraction of those neighbors that are pro-Dream Act.


```r
knn_model &lt;- kknn(pro_dream ~ ftbiden + fttrump + age,
                  train = train,
                  test = test,
                  k = 51)

test &lt;- test %&gt;% 
  mutate(pro_dream_probability_knn = knn_model$fitted.values)
```

---

## KNN With Binary Outcomes

Compare those predictions with the ones we got from `glm()`.


```r
test %&gt;% select(ftbiden, pro_dream_probability, pro_dream_probability_knn, pro_dream) %&gt;% 
  head(10)
```

```
# A tibble: 10 x 4
   ftbiden pro_dream_probability pro_dream_probability_knn pro_dream
     &lt;dbl&gt;                 &lt;dbl&gt;                     &lt;dbl&gt;     &lt;dbl&gt;
 1      25                 0.483                     0.580         1
 2      28                 0.578                     0.753         0
 3       6                 0.425                     0.489         0
 4      49                 0.952                     0.975         1
 5      10                 0.447                     0.432         1
 6      20                 0.451                     0.568         1
 7       2                 0.455                     0.451         1
 8      61                 0.917                     0.994         1
 9      71                 0.981                     1             1
10      94                 0.979                     0.945         1
```

---

## Cross-Validation and Regularization

We can use the same cross-validation and regularization procedure to get the best predictive models here.


```r
# caret yells at you if the outcome is numeric
data &lt;- data %&gt;% 
  mutate(pro_dream = factor(pro_dream))

glm_model &lt;- train(pro_dream ~ ftbiden + fttrump + age,
                   data = data, method = 'glm',
                   trControl =  train.control)

knn_model &lt;- train.kknn(pro_dream ~ ftbiden + fttrump + age,
                        data = data, kmax = 100)
```

---

# Problem Set 8

Build a model to predict which CCES survey respondents voted for Trump in 2016 and which voted for Clinton in 2016.

Best model wins!

--

**Note:** When I gave this challenge to my undergraduates at the beginning of the semester, their best model correctly predicted 69.6% of respondents.

--

So...that's your benchmark.

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
