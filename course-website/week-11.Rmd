---
title: "Week 11"
description: |
  Sampling Distributions
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
set.seed(42)
```

Okay. So we have an estimate we computed from our data. Maybe we've even done enough theoretical legwork to believe that our estimate reflects a causal relationship. We're feeling pretty good about ourselves.

But we're not done yet, because that estimate we calculated is based on limited information about the world. Maybe our data only represents a sample from a much larger population. There's no guarantee that the estimate we get from that sample is that same one we would get if we could observe everyone in the population. And even if we've collected data from the entire population, don't forget about the [Fundamental Problem of Causal Inference](week-08.html) -- we can never know with certainty what would have happened under some counterfactual treatment regime.[^1] So we need a way to express our uncertainty about the estimates we produced. How likely is it that our result was just caused by a weird sample?

[^1]: For a thorough review of the distinction between sampling-based and design-based uncertainty, see @abadie2020.

This week, we discuss the fundamental concept underlying all of statistical inference[^2] -- the **sampling distribution**.

[^2]: Well, frequentist statistical inference. Bayesians think about uncertainty in a different way [@mcelreath2020].

## Reading Assignments

-   [Teacup Giraffes: Module 6](https://tinystats.github.io/teacups-giraffes-and-statistics/06_standardError.html)

-   [The Joy of X](https://ebookcentral.proquest.com/lib/ugalib/reader.action?docID=3304559), Chapters 22 and 23

-   [Seeing Theory](https://seeing-theory.brown.edu/), some nice interactive explanations of concepts from probability theory

For an (optional) deeper dive into the mathematics of probability theory:

-   [Moore & Siegel](https://ebookcentral.proquest.com/lib/ugalib/reader.action?docID=1205618), Chapters 9-11

## Class Notes

```{r}
# load the CES dataset
load('data/ces-2020/cleaned-CES.RData')

# recode TV news viewership as a 0-1 variable
ces <- mutate(ces, tv_news_24h = as.numeric(tv_news_24h == 'Yes'))


```

What percent of this population (CES respondents) watched TV news in the past 24 hours?

```{r}
mean(ces$tv_news_24h)
```

But suppose we just had a *sample* from this population, and we were trying to estimate that population mean?

```{r}
# pull a sample
sample_data <- slice_sample(ces, n = 100)

# what percent of the sample watched TV news recently?
mean(sample_data$tv_news_24h)
```

Your result may differ! (It's a random sample, so there's some...randomness involved.) But the estimate I got was 54%. Not terrible, but not terribly accurate either.

Here's the question that motivates basically all of statistics. How *weird* can our sample estimates get? On average, do they tend to be close to the truth, or can they be pretty far off the mark? To answer this question, let's introduce the **sampling distribution** -- the distribution of outcomes we would expect to observe if we repeatedly drew samples and computed the estimate.[^3]

[^3]: In the real world, this is a pure thought experiment. We can't actually do this (unless we had a *lot* of money for repeated surveys).

```{r sampling-distribution, echo=TRUE, eval=FALSE}

# we'll use the replicate() function to repeat that sampling process a large number of times
sampling_distribution <- replicate(25000,
                                   ces %>% 
                                     # draw a sample
                                     slice_sample(n = 100) %>% 
                                     # pull the tv news variable
                                     pull(tv_news_24h) %>% 
                                     # take the mean
                                     mean)
```

```{r speed-up-site-building, echo=FALSE, eval=TRUE}
# note to the curious inspecting the raw Rmd file. I saved all the sampling distribution I created above to a file I'm loading them here so that R doesn't repeat those 25k draws whenever I build my website. That would be annoying.
load('data/ces-2020/sampling_distribution.RData')
```

The object `sampling_distribution` is a vector with 25,000 sample means. Each one was computed from an independent random sample from the CES dataset. The first sample estimate I got was 54%, but what does this set of 25,000 estimates look like?

```{r}

head(sampling_distribution)

ggplot(mapping = aes(x = sampling_distribution)) +
  geom_histogram(color = 'black', binwidth = 0.01) +
  theme_minimal() +
  labs(x = 'Estimate', y = 'Number of Samples')
```

## Problem Set

1.  
